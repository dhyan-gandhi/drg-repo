{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations=glob.glob('data/OPP-115/annotations/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_list=[]\n",
    "for file in annotations:\n",
    "    annotations_list.append(pd.read_csv(file, header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot=pd.concat(annotations_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot.columns = ['annotation_ID', 'batch_ID', 'annotator_ID', 'policy_ID', 'segment_ID','category_name',\n",
    "            'attribute_value_pairs','date','policy_URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot['dict'] = annot.attribute_value_pairs.apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_ID</th>\n",
       "      <th>batch_ID</th>\n",
       "      <th>annotator_ID</th>\n",
       "      <th>policy_ID</th>\n",
       "      <th>segment_ID</th>\n",
       "      <th>category_name</th>\n",
       "      <th>attribute_value_pairs</th>\n",
       "      <th>date</th>\n",
       "      <th>policy_URL</th>\n",
       "      <th>dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13160</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>{\"Other Type\": {\"endIndexInSegment\": 575, \"sta...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Other Type': {'endIndexInSegment': 575, 'sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13161</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13162</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13163</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13164</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   annotation_ID                                       batch_ID  annotator_ID  \\\n",
       "0          13160  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "1          13161  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "2          13162  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "3          13163  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "4          13164  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "\n",
       "   policy_ID  segment_ID               category_name  \\\n",
       "0       3828           0                       Other   \n",
       "1       3828           1  First Party Collection/Use   \n",
       "2       3828           1  First Party Collection/Use   \n",
       "3       3828           1  First Party Collection/Use   \n",
       "4       3828           1  First Party Collection/Use   \n",
       "\n",
       "                               attribute_value_pairs    date  \\\n",
       "0  {\"Other Type\": {\"endIndexInSegment\": 575, \"sta...  5/7/15   \n",
       "1  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "2  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "3  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "4  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "\n",
       "                                          policy_URL  \\\n",
       "0  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "1  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "2  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "3  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "4  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "\n",
       "                                                dict  \n",
       "0  {'Other Type': {'endIndexInSegment': 575, 'sta...  \n",
       "1  {'Collection Mode': {'endIndexInSegment': -1, ...  \n",
       "2  {'Collection Mode': {'endIndexInSegment': -1, ...  \n",
       "3  {'Collection Mode': {'endIndexInSegment': -1, ...  \n",
       "4  {'Collection Mode': {'endIndexInSegment': -1, ...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in annot['dict']:\n",
    "    for key, value in x.items():\n",
    "        value.pop('endIndexInSegment', None)\n",
    "        value.pop('startIndexInSegment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[]\n",
    "category=[]\n",
    "subcat=[]\n",
    "label=[]\n",
    "counter=0\n",
    "for i,x in enumerate(annot['dict']):\n",
    "    for key, value in x.items():\n",
    "        subcat.append(key)\n",
    "        if value.get('selectedText')==None:\n",
    "            text.append('noSelectedText')\n",
    "        else:\n",
    "            text.append(value.get('selectedText'))  \n",
    "        category.append(annot['category_name'][i])\n",
    "        for k, v in value.items():\n",
    "            if k=='value':\n",
    "                label.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=list(zip(text, category, subcat, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(d, columns=['text', 'category', 'subcategory', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Effective Date: May 7, 2015 Kraft Site Privacy...</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other Type</td>\n",
       "      <td>Introductory/Generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>noSelectedText</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Collection Mode</td>\n",
       "      <td>not-selected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>collec</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Choice Scope</td>\n",
       "      <td>Collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>register on our website or participate in our ...</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Action First-Party</td>\n",
       "      <td>Collect on website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>personally-identifiable information, such as</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Personal Information Type</td>\n",
       "      <td>Generic personal information</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Effective Date: May 7, 2015 Kraft Site Privacy...   \n",
       "1                                     noSelectedText   \n",
       "2                                             collec   \n",
       "3  register on our website or participate in our ...   \n",
       "4       personally-identifiable information, such as   \n",
       "\n",
       "                     category                subcategory  \\\n",
       "0                       Other                 Other Type   \n",
       "1  First Party Collection/Use            Collection Mode   \n",
       "2  First Party Collection/Use               Choice Scope   \n",
       "3  First Party Collection/Use         Action First-Party   \n",
       "4  First Party Collection/Use  Personal Information Type   \n",
       "\n",
       "                          label  \n",
       "0          Introductory/Generic  \n",
       "1                  not-selected  \n",
       "2                    Collection  \n",
       "3            Collect on website  \n",
       "4  Generic personal information  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cat_sub'] = data['category'] +'-'+ data['subcategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('cat_sub').count().to_csv(\"subcategory_models.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           36\n",
       "category       36\n",
       "subcategory    36\n",
       "label          36\n",
       "cat_sub        36\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['category','subcategory']).nunique().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cat_sub_lab'] = data['category'] +'-'+ data['subcategory'] +'-'+ data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "data['text'] = data['text'].apply(lambda x : remove_punct(x.lower()))\n",
    "\n",
    "data = data[data['text']!='null']\n",
    "data = data[data['text']!='noselectedtext']\n",
    "data = data[data['text']!='']\n",
    "data = data[data['text']!=' ']\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "ds = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36717, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "      <th>cat_sub</th>\n",
       "      <th>cat_sub_lab</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Data Retention</th>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data Security</th>\n",
       "      <td>808</td>\n",
       "      <td>808</td>\n",
       "      <td>808</td>\n",
       "      <td>808</td>\n",
       "      <td>808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Do Not Track</th>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First Party Collection/Use</th>\n",
       "      <td>15346</td>\n",
       "      <td>15346</td>\n",
       "      <td>15346</td>\n",
       "      <td>15346</td>\n",
       "      <td>15346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>International and Specific Audiences</th>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>2759</td>\n",
       "      <td>2759</td>\n",
       "      <td>2759</td>\n",
       "      <td>2759</td>\n",
       "      <td>2759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Policy Change</th>\n",
       "      <td>932</td>\n",
       "      <td>932</td>\n",
       "      <td>932</td>\n",
       "      <td>932</td>\n",
       "      <td>932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Third Party Sharing/Collection</th>\n",
       "      <td>10655</td>\n",
       "      <td>10655</td>\n",
       "      <td>10655</td>\n",
       "      <td>10655</td>\n",
       "      <td>10655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Access, Edit and Deletion</th>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Choice/Control</th>\n",
       "      <td>3798</td>\n",
       "      <td>3798</td>\n",
       "      <td>3798</td>\n",
       "      <td>3798</td>\n",
       "      <td>3798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text  subcategory  label  cat_sub  \\\n",
       "category                                                                   \n",
       "Data Retention                          676          676    676      676   \n",
       "Data Security                           808          808    808      808   \n",
       "Do Not Track                             67           67     67       67   \n",
       "First Party Collection/Use            15346        15346  15346    15346   \n",
       "International and Specific Audiences    582          582    582      582   \n",
       "Other                                  2759         2759   2759     2759   \n",
       "Policy Change                           932          932    932      932   \n",
       "Third Party Sharing/Collection        10655        10655  10655    10655   \n",
       "User Access, Edit and Deletion         1094         1094   1094     1094   \n",
       "User Choice/Control                    3798         3798   3798     3798   \n",
       "\n",
       "                                      cat_sub_lab  \n",
       "category                                           \n",
       "Data Retention                                676  \n",
       "Data Security                                 808  \n",
       "Do Not Track                                   67  \n",
       "First Party Collection/Use                  15346  \n",
       "International and Specific Audiences          582  \n",
       "Other                                        2759  \n",
       "Policy Change                                 932  \n",
       "Third Party Sharing/Collection              10655  \n",
       "User Access, Edit and Deletion               1094  \n",
       "User Choice/Control                          3798  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.groupby('category').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.groupby(['category','subcategory']).count().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(259, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.groupby(['category','subcategory','label']).count().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "ds['tokenized'] = ds['text'].apply(lambda x : re.split(' ', x))\n",
    "# ds['tokenized_text']=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    " \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-19 21:37:46,412 : INFO : loading Word2Vec object from word2vec.model\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-09-19 21:37:46,460 : INFO : loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "2019-09-19 21:37:46,462 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-09-19 21:37:46,464 : INFO : loading vocabulary recursively from word2vec.model.vocabulary.* with mmap=None\n",
      "2019-09-19 21:37:46,465 : INFO : loading trainables recursively from word2vec.model.trainables.* with mmap=None\n",
      "2019-09-19 21:37:46,469 : INFO : setting ignored attribute cum_table to None\n",
      "2019-09-19 21:37:46,470 : INFO : loaded word2vec.model\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encode = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {k: v.index for k, v in word_vectors.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_word(new_word, new_vector, new_index, embedding_matrix, word2id):\n",
    "    embedding_matrix = np.insert(embedding_matrix, [new_index], [new_vector], axis=0)\n",
    "    \n",
    "    word2id = {word: (index+1) if index >= new_index else index for word, index in word2id.items()}\n",
    "    word2id[new_word] = new_index\n",
    "    return embedding_matrix, word2id\n",
    "\n",
    "UNK_INDEX = 0\n",
    "UNK_TOKEN = 'UNK'\n",
    "\n",
    "embedding_matrix = word_vectors.vectors\n",
    "unk_vector = embedding_matrix.mean(0)\n",
    "embedding_matrix, word2id = add_new_word(UNK_TOKEN, unk_vector, UNK_INDEX, embedding_matrix, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3933"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3933, 100)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.insert(embedding_matrix, len(embedding_matrix), [np.zeros((100,))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3934"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[3933].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created. Percentage of unknown words: 7006.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36717,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_int_data(token_text, word2id):\n",
    "    x = []\n",
    "    unk_count = 0\n",
    "    for item in token_text:\n",
    "        temp=[]\n",
    "        x.append(temp)\n",
    "        for word in item:\n",
    "            if word in word2id:\n",
    "                temp.append(word2id.get(word))\n",
    "            else:\n",
    "                temp.append(UNK_INDEX)\n",
    "                unk_count += 1\n",
    "    print('Data created. Percentage of unknown words: %.3f' % (unk_count))\n",
    "    return np.array(x)\n",
    "\n",
    "x=get_int_data(ds.tokenized, word2id)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "ds['enumerated_text']=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "data_retention = ds[ds.category=='Data Retention']\n",
    "data_retention['Personal_Information_Type'] = data_retention['subcategory'].apply(lambda x: 1 if x=='Personal Information Type' else 0)\n",
    "data_retention['Retention_Period'] = data_retention['subcategory'].apply(lambda x: 1 if x=='Retention Period' else 0)\n",
    "data_retention['Retention_Purpose'] = data_retention['subcategory'].apply(lambda x: 1 if x=='Retention Purpose' else 0)\n",
    "\n",
    "first_party_collection_use = ds[ds.category=='First Party Collection/Use']\n",
    "first_party_collection_use['Action_First_Party'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Action First-Party' else 0)\n",
    "first_party_collection_use['Choice_Scope'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Choice Scope' else 0)\n",
    "first_party_collection_use['Choice_Type'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Choice Type' else 0)\n",
    "first_party_collection_use['Collection_Mode'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Collection Mode' else 0)\n",
    "first_party_collection_use['Does_Does_Not'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Does/Does Not' else 0)\n",
    "first_party_collection_use['Identifiability'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Identifiability' else 0)\n",
    "first_party_collection_use['Personal_Information_Type'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Personal Information Type' else 0)\n",
    "first_party_collection_use['Purpose'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Purpose' else 0)\n",
    "first_party_collection_use['User_Type'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='User Type' else 0)\n",
    "\n",
    "policy_change = ds[ds.category=='Policy Change']\n",
    "policy_change['Change_Type'] = policy_change['subcategory'].apply(lambda x: 1 if x=='Change Type' else 0)\n",
    "policy_change['Notification_Type'] = policy_change['subcategory'].apply(lambda x: 1 if x=='Notification Type' else 0)\n",
    "policy_change['User_Choice'] = policy_change['subcategory'].apply(lambda x: 1 if x=='User Choice' else 0)\n",
    "\n",
    "third_party_sharing_collection = ds[ds.category=='Third Party Sharing/Collection']\n",
    "third_party_sharing_collection['Action_Third_Party'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Action Third Party' else 0)\n",
    "third_party_sharing_collection['Choice_Scope'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Choice Scope' else 0)\n",
    "third_party_sharing_collection['Choice_Type'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Choice Type' else 0)\n",
    "third_party_sharing_collection['Does_Does_Not'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Does/Does Not' else 0)\n",
    "third_party_sharing_collection['Identifiability'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Identifiability' else 0)\n",
    "third_party_sharing_collection['Personal_Information_Type'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Personal Information Type' else 0)\n",
    "third_party_sharing_collection['Purpose'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Purpose' else 0)\n",
    "third_party_sharing_collection['Third_Party_Entity'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Third Party Entity' else 0)\n",
    "third_party_sharing_collection['User_Type'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='User Type' else 0)\n",
    "\n",
    "user_access_edit_deletion = ds[ds.category=='User Access, Edit and Deletion']\n",
    "user_access_edit_deletion['Access_Scope'] = user_access_edit_deletion['subcategory'].apply(lambda x: 1 if x=='Access Scope' else 0)\n",
    "user_access_edit_deletion['Access_Type'] = user_access_edit_deletion['subcategory'].apply(lambda x: 1 if x=='Access Type' else 0)\n",
    "user_access_edit_deletion['User_Type'] = user_access_edit_deletion['subcategory'].apply(lambda x: 1 if x=='User Type' else 0)\n",
    "\n",
    "user_choice_control = ds[ds.category=='User Choice/Control']\n",
    "user_choice_control['Choice_Scope'] = user_choice_control['subcategory'].apply(lambda x: 1 if x=='Choice Scope' else 0)\n",
    "user_choice_control['Choice_Type'] = user_choice_control['subcategory'].apply(lambda x: 1 if x=='Choice Type' else 0)\n",
    "user_choice_control['Personal_Information_Type'] = user_choice_control['subcategory'].apply(lambda x: 1 if x=='Personal Information Type' else 0)\n",
    "user_choice_control['Purpose'] = user_choice_control['subcategory'].apply(lambda x: 1 if x=='Purpose' else 0)\n",
    "user_choice_control['User_Type'] = user_choice_control['subcategory'].apply(lambda x: 1 if x=='User Type' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_party_collection_use.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Activation, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_length = max(ds.enumerated_text.apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA RETENTION MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 473 samples, validate on 203 samples\n",
      "Epoch 1/5\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 0.8819 - acc: 0.6512 - val_loss: 0.4620 - val_acc: 0.7537\n",
      "Epoch 2/5\n",
      "473/473 [==============================] - 0s 644us/step - loss: 0.4072 - acc: 0.8689 - val_loss: 0.3287 - val_acc: 0.8374\n",
      "Epoch 3/5\n",
      "473/473 [==============================] - 0s 676us/step - loss: 0.2757 - acc: 0.9281 - val_loss: 0.3415 - val_acc: 0.8325\n",
      "Epoch 4/5\n",
      "473/473 [==============================] - 0s 654us/step - loss: 0.2096 - acc: 0.9577 - val_loss: 0.3402 - val_acc: 0.8374\n",
      "Epoch 5/5\n",
      "473/473 [==============================] - 0s 672us/step - loss: 0.1744 - acc: 0.9662 - val_loss: 0.3393 - val_acc: 0.8621\n",
      "[[333  10]\n",
      " [  6 124]]\n",
      "0.9393939393939394\n",
      "0.9538461538461539\n",
      "[[128  17]\n",
      " [ 11  47]]\n",
      "0.7704918032786885\n",
      "0.8103448275862069\n"
     ]
    }
   ],
   "source": [
    "# Personal Information Type Model (Data Retention)\n",
    "\n",
    "padded_docs = pad_sequences(data_retention.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, data_retention.Personal_Information_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 3.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('data_retention_personal_information_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 473 samples, validate on 203 samples\n",
      "Epoch 1/5\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 0.8654 - acc: 0.6575 - val_loss: 0.6800 - val_acc: 0.7291\n",
      "Epoch 2/5\n",
      "473/473 [==============================] - 0s 673us/step - loss: 0.5727 - acc: 0.8034 - val_loss: 0.6251 - val_acc: 0.7291\n",
      "Epoch 3/5\n",
      "473/473 [==============================] - 0s 644us/step - loss: 0.4399 - acc: 0.8710 - val_loss: 0.6615 - val_acc: 0.7340\n",
      "Epoch 4/5\n",
      "473/473 [==============================] - 0s 604us/step - loss: 0.3468 - acc: 0.9027 - val_loss: 0.7058 - val_acc: 0.7488\n",
      "Epoch 5/5\n",
      "473/473 [==============================] - 0s 695us/step - loss: 0.2873 - acc: 0.9070 - val_loss: 0.7626 - val_acc: 0.7586\n",
      "[[280  31]\n",
      " [  7 155]]\n",
      "0.8908045977011495\n",
      "0.9567901234567902\n",
      "[[103  28]\n",
      " [ 21  51]]\n",
      "0.6754966887417219\n",
      "0.7083333333333334\n"
     ]
    }
   ],
   "source": [
    "# Retention Period Model (Data Retention)\n",
    "\n",
    "padded_docs = pad_sequences(data_retention.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, data_retention.Retention_Period, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 2.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('data_retention_rentention_period.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 473 samples, validate on 203 samples\n",
      "Epoch 1/5\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 0.9025 - acc: 0.6025 - val_loss: 0.6885 - val_acc: 0.6650\n",
      "Epoch 2/5\n",
      "473/473 [==============================] - 0s 626us/step - loss: 0.5029 - acc: 0.8436 - val_loss: 0.7028 - val_acc: 0.7537\n",
      "Epoch 3/5\n",
      "473/473 [==============================] - 0s 641us/step - loss: 0.3654 - acc: 0.8901 - val_loss: 0.8175 - val_acc: 0.7340\n",
      "Epoch 4/5\n",
      "473/473 [==============================] - 0s 644us/step - loss: 0.2844 - acc: 0.9091 - val_loss: 0.8280 - val_acc: 0.7635\n",
      "Epoch 5/5\n",
      "473/473 [==============================] - 0s 621us/step - loss: 0.2336 - acc: 0.9345 - val_loss: 0.8439 - val_acc: 0.7340\n",
      "[[271  21]\n",
      " [  6 175]]\n",
      "0.9283819628647214\n",
      "0.9668508287292817\n",
      "[[101  29]\n",
      " [ 25  48]]\n",
      "0.64\n",
      "0.6575342465753424\n"
     ]
    }
   ],
   "source": [
    "# Retention Purpose Model (Data Retention)\n",
    "\n",
    "padded_docs = pad_sequences(data_retention.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, data_retention.Retention_Purpose, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 2.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('data_retention_retention_purpose.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIRST PARTY COLLECTION/USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 7s 650us/step - loss: 1.0256 - acc: 0.6457 - val_loss: 0.5653 - val_acc: 0.7094\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 6s 594us/step - loss: 0.8222 - acc: 0.7594 - val_loss: 0.5620 - val_acc: 0.7233\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 6s 592us/step - loss: 0.6994 - acc: 0.8000 - val_loss: 0.5616 - val_acc: 0.7374\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 6s 598us/step - loss: 0.5912 - acc: 0.8334 - val_loss: 0.6251 - val_acc: 0.7096\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 6s 586us/step - loss: 0.5275 - acc: 0.8529 - val_loss: 0.5692 - val_acc: 0.7483\n",
      "[[7483 1318]\n",
      " [  87 1854]]\n",
      "0.725210248386466\n",
      "0.955177743431221\n",
      "[[2967  845]\n",
      " [ 314  478]]\n",
      "0.45200945626477534\n",
      "0.6035353535353535\n"
     ]
    }
   ],
   "source": [
    "# Action First-Party Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Action_First_Party, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('first_party_collection_use_action_first_party.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 7s 657us/step - loss: 0.8632 - acc: 0.8686 - val_loss: 0.4103 - val_acc: 0.7974\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 6s 598us/step - loss: 0.6459 - acc: 0.8586 - val_loss: 0.3393 - val_acc: 0.8454\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 6s 592us/step - loss: 0.5269 - acc: 0.8886 - val_loss: 0.3600 - val_acc: 0.8273\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 6s 602us/step - loss: 0.4202 - acc: 0.9048 - val_loss: 0.2887 - val_acc: 0.8827\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 6s 584us/step - loss: 0.3489 - acc: 0.9232 - val_loss: 0.3028 - val_acc: 0.8747\n",
      "[[9447  946]\n",
      " [  19  330]]\n",
      "0.4061538461538462\n",
      "0.9455587392550143\n",
      "[[3954  471]\n",
      " [ 106   73]]\n",
      "0.20193637621023514\n",
      "0.40782122905027934\n"
     ]
    }
   ],
   "source": [
    "# Choice Scope Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Choice_Scope, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 15.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('first_party_collection_use_choice_scope.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 7s 646us/step - loss: 0.5110 - acc: 0.8996 - val_loss: 0.2588 - val_acc: 0.9118\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 6s 594us/step - loss: 0.3325 - acc: 0.9337 - val_loss: 0.2409 - val_acc: 0.9112\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 6s 598us/step - loss: 0.2703 - acc: 0.9423 - val_loss: 0.2236 - val_acc: 0.9209\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 6s 605us/step - loss: 0.2238 - acc: 0.9472 - val_loss: 0.2279 - val_acc: 0.9177\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 6s 584us/step - loss: 0.1927 - acc: 0.9528 - val_loss: 0.2553 - val_acc: 0.9129\n",
      "[[9656  457]\n",
      " [  40  589]]\n",
      "0.7032835820895523\n",
      "0.9364069952305246\n",
      "[[4028  268]\n",
      " [ 133  175]]\n",
      "0.4660452729693742\n",
      "0.5681818181818182\n"
     ]
    }
   ],
   "source": [
    "# Choice Type Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Choice_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('first_party_collection_use_choice_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 9s 793us/step - loss: 0.8486 - acc: 0.8133 - val_loss: 0.4942 - val_acc: 0.8056\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 7s 683us/step - loss: 0.7026 - acc: 0.8473 - val_loss: 0.4133 - val_acc: 0.8267\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 7s 648us/step - loss: 0.6173 - acc: 0.8553 - val_loss: 0.4526 - val_acc: 0.7991\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 7s 641us/step - loss: 0.5452 - acc: 0.8648 - val_loss: 0.4661 - val_acc: 0.7902\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 7s 654us/step - loss: 0.4726 - acc: 0.8796 - val_loss: 0.4847 - val_acc: 0.7921\n",
      "[[8499 1200]\n",
      " [  88  955]]\n",
      "0.5972482801751096\n",
      "0.9156279961649089\n",
      "[[3428  691]\n",
      " [ 266  219]]\n",
      "0.31397849462365596\n",
      "0.4515463917525773\n"
     ]
    }
   ],
   "source": [
    "# Collection Mode Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Collection_Mode, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('first_party_collection_collection_mode.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 7s 683us/step - loss: 0.7260 - acc: 0.8265 - val_loss: 0.4293 - val_acc: 0.8223\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 6s 596us/step - loss: 0.5767 - acc: 0.8505 - val_loss: 0.3556 - val_acc: 0.8377\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 6s 591us/step - loss: 0.4916 - acc: 0.8744 - val_loss: 0.3670 - val_acc: 0.8354\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 6s 596us/step - loss: 0.4270 - acc: 0.8919 - val_loss: 0.3757 - val_acc: 0.8393\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 6s 588us/step - loss: 0.3780 - acc: 0.9007 - val_loss: 0.4148 - val_acc: 0.8288\n",
      "[[8894  927]\n",
      " [  67  854]]\n",
      "0.6321243523316062\n",
      "0.9272529858849077\n",
      "[[3629  588]\n",
      " [ 200  187]]\n",
      "0.3218588640275387\n",
      "0.48320413436692505\n"
     ]
    }
   ],
   "source": [
    "# Does/Does Not Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Does_Does_Not, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('first_party_collection_use_does_does_not.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 7s 660us/step - loss: 0.4570 - acc: 0.9385 - val_loss: 0.1740 - val_acc: 0.9553\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 6s 603us/step - loss: 0.3161 - acc: 0.9433 - val_loss: 0.1883 - val_acc: 0.9429\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 6s 596us/step - loss: 0.2433 - acc: 0.9546 - val_loss: 0.1849 - val_acc: 0.9407\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 6s 598us/step - loss: 0.1907 - acc: 0.9609 - val_loss: 0.1965 - val_acc: 0.9340\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 6s 597us/step - loss: 0.1567 - acc: 0.9653 - val_loss: 0.1752 - val_acc: 0.9513\n",
      "[[10098   189]\n",
      " [   68   387]]\n",
      "0.7507274490785645\n",
      "0.8505494505494505\n",
      "[[4301  122]\n",
      " [ 102   79]]\n",
      "0.41361256544502617\n",
      "0.43646408839779005\n"
     ]
    }
   ],
   "source": [
    "# Identifiability Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Identifiability, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('first_party_collection_use_identifiability.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 7s 659us/step - loss: 0.8838 - acc: 0.7410 - val_loss: 0.4987 - val_acc: 0.7778\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 6s 597us/step - loss: 0.6448 - acc: 0.8237 - val_loss: 0.4066 - val_acc: 0.8308\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 7s 676us/step - loss: 0.5228 - acc: 0.8613 - val_loss: 0.4564 - val_acc: 0.8141\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 10s 944us/step - loss: 0.4278 - acc: 0.8896 - val_loss: 0.4471 - val_acc: 0.8230\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 7s 626us/step - loss: 0.3684 - acc: 0.9049 - val_loss: 0.4214 - val_acc: 0.8486\n",
      "[[7727  788]\n",
      " [  52 2175]]\n",
      "0.838150289017341\n",
      "0.9766502020655591\n",
      "[[3195  517]\n",
      " [ 180  712]]\n",
      "0.6713814238566713\n",
      "0.7982062780269058\n"
     ]
    }
   ],
   "source": [
    "# Personal Information Type Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Personal_Information_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('first_party_collection_use_personal_information_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 8s 699us/step - loss: 0.8864 - acc: 0.7306 - val_loss: 0.4752 - val_acc: 0.7887\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 6s 601us/step - loss: 0.6514 - acc: 0.8264 - val_loss: 0.5201 - val_acc: 0.7639\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 6s 595us/step - loss: 0.5296 - acc: 0.8673 - val_loss: 0.4734 - val_acc: 0.8093\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 6s 605us/step - loss: 0.4346 - acc: 0.8953 - val_loss: 0.4235 - val_acc: 0.8380\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 6s 592us/step - loss: 0.3607 - acc: 0.9172 - val_loss: 0.4223 - val_acc: 0.8506\n",
      "[[7177  627]\n",
      " [  65 2873]]\n",
      "0.8925132028580304\n",
      "0.9778761061946902\n",
      "[[2867  461]\n",
      " [ 227 1049]]\n",
      "0.7530509691313712\n",
      "0.8221003134796239\n"
     ]
    }
   ],
   "source": [
    "# Purpose Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Purpose, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('first_party_collection_use_purpose.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 7s 690us/step - loss: 0.2934 - acc: 0.9696 - val_loss: 0.1913 - val_acc: 0.9290\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 8s 765us/step - loss: 0.1809 - acc: 0.9714 - val_loss: 0.1189 - val_acc: 0.9639\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 8s 740us/step - loss: 0.1362 - acc: 0.9758 - val_loss: 0.0921 - val_acc: 0.9718\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 7s 611us/step - loss: 0.1104 - acc: 0.9791 - val_loss: 0.1157 - val_acc: 0.9611\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 6s 604us/step - loss: 0.0857 - acc: 0.9837 - val_loss: 0.1061 - val_acc: 0.9687\n",
      "[[10393   110]\n",
      " [   21   218]]\n",
      "0.7689594356261024\n",
      "0.9121338912133892\n",
      "[[4411   89]\n",
      " [  55   49]]\n",
      "0.4049586776859504\n",
      "0.47115384615384615\n"
     ]
    }
   ],
   "source": [
    "# User Type Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.User_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('first_party_collection_use_user_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POLICY CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 652 samples, validate on 280 samples\n",
      "Epoch 1/5\n",
      "652/652 [==============================] - 1s 2ms/step - loss: 1.4465 - acc: 0.4294 - val_loss: 0.7795 - val_acc: 0.5929\n",
      "Epoch 2/5\n",
      "652/652 [==============================] - 0s 615us/step - loss: 0.9534 - acc: 0.6994 - val_loss: 0.8716 - val_acc: 0.6214\n",
      "Epoch 3/5\n",
      "652/652 [==============================] - 0s 690us/step - loss: 0.7319 - acc: 0.7423 - val_loss: 0.7400 - val_acc: 0.6786\n",
      "Epoch 4/5\n",
      "652/652 [==============================] - 0s 656us/step - loss: 0.6026 - acc: 0.8206 - val_loss: 0.8499 - val_acc: 0.6536\n",
      "Epoch 5/5\n",
      "652/652 [==============================] - 0s 664us/step - loss: 0.5276 - acc: 0.8252 - val_loss: 0.8164 - val_acc: 0.6893\n",
      "[[317  91]\n",
      " [  3 241]]\n",
      "0.8368055555555556\n",
      "0.9877049180327869\n",
      "[[121  62]\n",
      " [ 25  72]]\n",
      "0.6233766233766234\n",
      "0.7422680412371134\n"
     ]
    }
   ],
   "source": [
    "# Change Type Model (Policy Change)\n",
    "\n",
    "padded_docs = pad_sequences(policy_change.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, policy_change.Change_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('policy_change_change_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 652 samples, validate on 280 samples\n",
      "Epoch 1/5\n",
      "652/652 [==============================] - 1s 2ms/step - loss: 1.4417 - acc: 0.5015 - val_loss: 0.9353 - val_acc: 0.5536\n",
      "Epoch 2/5\n",
      "652/652 [==============================] - 1s 971us/step - loss: 0.9332 - acc: 0.6672 - val_loss: 0.7854 - val_acc: 0.6107\n",
      "Epoch 3/5\n",
      "652/652 [==============================] - 1s 792us/step - loss: 0.7301 - acc: 0.7193 - val_loss: 0.8323 - val_acc: 0.6143\n",
      "Epoch 4/5\n",
      "652/652 [==============================] - 1s 778us/step - loss: 0.6457 - acc: 0.7684 - val_loss: 0.7942 - val_acc: 0.6821\n",
      "Epoch 5/5\n",
      "652/652 [==============================] - 1s 774us/step - loss: 0.5942 - acc: 0.7945 - val_loss: 0.8661 - val_acc: 0.6857\n",
      "[[272 114]\n",
      " [  3 263]]\n",
      "0.818040435458787\n",
      "0.9887218045112782\n",
      "[[ 77  66]\n",
      " [ 22 115]]\n",
      "0.7232704402515723\n",
      "0.8394160583941606\n"
     ]
    }
   ],
   "source": [
    "# Notification Type Model (Policy Change)\n",
    "\n",
    "padded_docs = pad_sequences(policy_change.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, policy_change.Notification_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('policy_change_notification_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 652 samples, validate on 280 samples\n",
      "Epoch 1/5\n",
      "652/652 [==============================] - 1s 2ms/step - loss: 1.2145 - acc: 0.5107 - val_loss: 0.6357 - val_acc: 0.7286\n",
      "Epoch 2/5\n",
      "652/652 [==============================] - 0s 625us/step - loss: 0.7707 - acc: 0.8528 - val_loss: 0.6976 - val_acc: 0.6536\n",
      "Epoch 3/5\n",
      "652/652 [==============================] - 0s 655us/step - loss: 0.6176 - acc: 0.8236 - val_loss: 0.6321 - val_acc: 0.7286\n",
      "Epoch 4/5\n",
      "652/652 [==============================] - 0s 641us/step - loss: 0.5031 - acc: 0.8666 - val_loss: 0.6787 - val_acc: 0.7179\n",
      "Epoch 5/5\n",
      "652/652 [==============================] - 0s 667us/step - loss: 0.4478 - acc: 0.8773 - val_loss: 0.6887 - val_acc: 0.7214\n",
      "[[439  71]\n",
      " [  4 138]]\n",
      "0.7863247863247862\n",
      "0.971830985915493\n",
      "[[178  56]\n",
      " [ 22  24]]\n",
      "0.38095238095238093\n",
      "0.5217391304347826\n"
     ]
    }
   ],
   "source": [
    "# User Choice Model (Policy Change)\n",
    "\n",
    "padded_docs = pad_sequences(policy_change.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, policy_change.User_Choice, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('policy_change_user_choice.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIRD PARTY SHARING/COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 6s 797us/step - loss: 1.0042 - acc: 0.6881 - val_loss: 0.5508 - val_acc: 0.7366\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 5s 637us/step - loss: 0.7731 - acc: 0.7776 - val_loss: 0.5128 - val_acc: 0.7557\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 6s 822us/step - loss: 0.6564 - acc: 0.8159 - val_loss: 0.5133 - val_acc: 0.7660\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 8s 1ms/step - loss: 0.5600 - acc: 0.8459 - val_loss: 0.5531 - val_acc: 0.7657\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 6s 748us/step - loss: 0.4931 - acc: 0.8615 - val_loss: 0.5451 - val_acc: 0.7710\n",
      "[[5309  841]\n",
      " [  40 1268]]\n",
      "0.7421714954638572\n",
      "0.9694189602446484\n",
      "[[2087  529]\n",
      " [ 203  378]]\n",
      "0.5080645161290323\n",
      "0.6506024096385542\n"
     ]
    }
   ],
   "source": [
    "# Action Third-Party Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Action_Third_Party, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('third_party_sharing_collection_action_third_party.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 5s 731us/step - loss: 0.4768 - acc: 0.9553 - val_loss: 0.2159 - val_acc: 0.9687\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 4s 599us/step - loss: 0.3349 - acc: 0.9623 - val_loss: 0.1469 - val_acc: 0.9653\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 4s 592us/step - loss: 0.2742 - acc: 0.9627 - val_loss: 0.1723 - val_acc: 0.9556\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 4s 595us/step - loss: 0.2335 - acc: 0.9603 - val_loss: 0.1735 - val_acc: 0.9503\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 4s 595us/step - loss: 0.1968 - acc: 0.9646 - val_loss: 0.1743 - val_acc: 0.9446\n",
      "[[7032  183]\n",
      " [  58  185]]\n",
      "0.6055646481178396\n",
      "0.7613168724279835\n",
      "[[2994  104]\n",
      " [  73   26]]\n",
      "0.22707423580786026\n",
      "0.26262626262626265\n"
     ]
    }
   ],
   "source": [
    "# Choice Scope Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Choice_Scope, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('third_party_sharing_collection_choice_scope.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 6s 743us/step - loss: 0.4627 - acc: 0.9128 - val_loss: 0.2735 - val_acc: 0.8993\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 5s 605us/step - loss: 0.2513 - acc: 0.9501 - val_loss: 0.1800 - val_acc: 0.9396\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 5s 616us/step - loss: 0.1863 - acc: 0.9602 - val_loss: 0.1816 - val_acc: 0.9396\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 5s 609us/step - loss: 0.1469 - acc: 0.9702 - val_loss: 0.1574 - val_acc: 0.9506\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 4s 590us/step - loss: 0.1169 - acc: 0.9772 - val_loss: 0.1697 - val_acc: 0.9468\n",
      "[[6864  125]\n",
      " [  11  458]]\n",
      "0.870722433460076\n",
      "0.976545842217484\n",
      "[[2883  102]\n",
      " [  68  144]]\n",
      "0.62882096069869\n",
      "0.6792452830188679\n"
     ]
    }
   ],
   "source": [
    "# Choice Type Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Choice_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('third_party_sharing_collection_choice_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 6s 780us/step - loss: 0.7579 - acc: 0.8097 - val_loss: 0.3520 - val_acc: 0.8233\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 4s 602us/step - loss: 0.5698 - acc: 0.8459 - val_loss: 0.4073 - val_acc: 0.8123\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 5s 607us/step - loss: 0.5071 - acc: 0.8591 - val_loss: 0.4047 - val_acc: 0.8086\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 4s 595us/step - loss: 0.4625 - acc: 0.8718 - val_loss: 0.4175 - val_acc: 0.8151\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 4s 600us/step - loss: 0.4200 - acc: 0.8837 - val_loss: 0.4346 - val_acc: 0.8076\n",
      "[[6023  776]\n",
      " [  51  608]]\n",
      "0.5952031326480667\n",
      "0.9226100151745068\n",
      "[[2467  452]\n",
      " [ 163  115]]\n",
      "0.2721893491124261\n",
      "0.4136690647482014\n"
     ]
    }
   ],
   "source": [
    "# Does/Does Not Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Does_Does_Not, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('third_party_sharing_collection_does_does_not.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 6s 786us/step - loss: 0.4198 - acc: 0.9358 - val_loss: 0.2085 - val_acc: 0.9309\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 5s 630us/step - loss: 0.2539 - acc: 0.9520 - val_loss: 0.1564 - val_acc: 0.9431\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 5s 736us/step - loss: 0.1968 - acc: 0.9566 - val_loss: 0.1496 - val_acc: 0.9478\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 7s 888us/step - loss: 0.1675 - acc: 0.9615 - val_loss: 0.1528 - val_acc: 0.9443\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 6s 853us/step - loss: 0.1416 - acc: 0.9643 - val_loss: 0.1480 - val_acc: 0.9475\n",
      "[[6990  194]\n",
      " [  34  240]]\n",
      "0.6779661016949152\n",
      "0.8759124087591241\n",
      "[[2975  110]\n",
      " [  58   54]]\n",
      "0.391304347826087\n",
      "0.48214285714285715\n"
     ]
    }
   ],
   "source": [
    "# Identifiability Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Identifiability, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('third_party_sharing_collection_identifiability.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 5s 711us/step - loss: 0.6842 - acc: 0.8384 - val_loss: 0.2926 - val_acc: 0.8883\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 6s 793us/step - loss: 0.4653 - acc: 0.8914 - val_loss: 0.2960 - val_acc: 0.8858\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 6s 843us/step - loss: 0.3854 - acc: 0.9091 - val_loss: 0.2731 - val_acc: 0.9015\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 6s 795us/step - loss: 0.3212 - acc: 0.9265 - val_loss: 0.3020 - val_acc: 0.8861\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 5s 722us/step - loss: 0.2754 - acc: 0.9370 - val_loss: 0.2843 - val_acc: 0.8965\n",
      "[[6003  425]\n",
      " [  26 1004]]\n",
      "0.8165921106140707\n",
      "0.974757281553398\n",
      "[[2504  260]\n",
      " [  71  362]]\n",
      "0.6862559241706161\n",
      "0.836027713625866\n"
     ]
    }
   ],
   "source": [
    "# Personal Information Type Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Personal_Information_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('third_party_sharing_collection_personal_information_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 7s 905us/step - loss: 0.9501 - acc: 0.6984 - val_loss: 0.5439 - val_acc: 0.7554\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 7s 899us/step - loss: 0.6611 - acc: 0.8222 - val_loss: 0.5203 - val_acc: 0.7764\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 5s 680us/step - loss: 0.5516 - acc: 0.8584 - val_loss: 0.5019 - val_acc: 0.7967\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 4s 499us/step - loss: 0.4731 - acc: 0.8851 - val_loss: 0.4654 - val_acc: 0.8308\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 4s 505us/step - loss: 0.3970 - acc: 0.9091 - val_loss: 0.5056 - val_acc: 0.8214\n",
      "[[5017  521]\n",
      " [  51 1869]]\n",
      "0.8672853828306264\n",
      "0.9734375\n",
      "[[1969  421]\n",
      " [ 150  657]]\n",
      "0.6970822281167108\n",
      "0.8141263940520446\n"
     ]
    }
   ],
   "source": [
    "# Purpose Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Purpose, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('third_party_sharing_collection_purpose.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 8s 1ms/step - loss: 0.9144 - acc: 0.7247 - val_loss: 0.4618 - val_acc: 0.7839\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 4s 544us/step - loss: 0.6725 - acc: 0.8215 - val_loss: 0.4633 - val_acc: 0.8048\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 4s 528us/step - loss: 0.5583 - acc: 0.8663 - val_loss: 0.4409 - val_acc: 0.8133\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 7s 881us/step - loss: 0.4595 - acc: 0.8903 - val_loss: 0.4500 - val_acc: 0.8273\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 5s 680us/step - loss: 0.4044 - acc: 0.9084 - val_loss: 0.4633 - val_acc: 0.8286\n",
      "[[5451  553]\n",
      " [  42 1412]]\n",
      "0.8259725065808715\n",
      "0.9711141678129298\n",
      "[[2181  376]\n",
      " [ 172  468]]\n",
      "0.6307277628032345\n",
      "0.73125\n"
     ]
    }
   ],
   "source": [
    "# Third Part Entity Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Third_Party_Entity, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('third_party_sharing_collection_third_party_entity.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 8s 1ms/step - loss: 0.2296 - acc: 0.9796 - val_loss: 0.0982 - val_acc: 0.9875\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 6s 829us/step - loss: 0.1206 - acc: 0.9851 - val_loss: 0.0553 - val_acc: 0.9881\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 5s 649us/step - loss: 0.0823 - acc: 0.9890 - val_loss: 0.0573 - val_acc: 0.9847\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 5s 644us/step - loss: 0.0603 - acc: 0.9905 - val_loss: 0.0570 - val_acc: 0.9853\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 5s 611us/step - loss: 0.0463 - acc: 0.9928 - val_loss: 0.0532 - val_acc: 0.9869\n",
      "[[7313   44]\n",
      " [  10   91]]\n",
      "0.7711864406779662\n",
      "0.900990099009901\n",
      "[[3133   29]\n",
      " [  13   22]]\n",
      "0.5116279069767442\n",
      "0.6285714285714286\n"
     ]
    }
   ],
   "source": [
    "# User Type Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.User_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('third_party_sharing_collection_user_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER ACCESS, EDIT, AND DELETION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 765 samples, validate on 329 samples\n",
      "Epoch 1/5\n",
      "765/765 [==============================] - 2s 2ms/step - loss: 1.4039 - acc: 0.4183 - val_loss: 0.7786 - val_acc: 0.4711\n",
      "Epoch 2/5\n",
      "765/765 [==============================] - 0s 618us/step - loss: 0.9382 - acc: 0.6261 - val_loss: 0.7972 - val_acc: 0.5836\n",
      "Epoch 3/5\n",
      "765/765 [==============================] - 0s 622us/step - loss: 0.7607 - acc: 0.7712 - val_loss: 0.7491 - val_acc: 0.6930\n",
      "Epoch 4/5\n",
      "765/765 [==============================] - 0s 625us/step - loss: 0.6562 - acc: 0.8078 - val_loss: 0.8302 - val_acc: 0.6565\n",
      "Epoch 5/5\n",
      "765/765 [==============================] - 0s 627us/step - loss: 0.5547 - acc: 0.8588 - val_loss: 0.8303 - val_acc: 0.6960\n",
      "[[373  98]\n",
      " [  6 288]]\n",
      "0.8470588235294118\n",
      "0.9795918367346939\n",
      "[[134  68]\n",
      " [ 32  95]]\n",
      "0.6551724137931034\n",
      "0.7480314960629921\n"
     ]
    }
   ],
   "source": [
    "# Access Scope Model (User Access, Edit and Deletion)\n",
    "\n",
    "padded_docs = pad_sequences(user_access_edit_deletion.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_access_edit_deletion.Access_Scope, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('user_access_edit_deletion_access_scope.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 765 samples, validate on 329 samples\n",
      "Epoch 1/5\n",
      "765/765 [==============================] - 2s 2ms/step - loss: 1.5148 - acc: 0.4850 - val_loss: 0.8844 - val_acc: 0.5076\n",
      "Epoch 2/5\n",
      "765/765 [==============================] - 0s 629us/step - loss: 0.9756 - acc: 0.6183 - val_loss: 1.0319 - val_acc: 0.5562\n",
      "Epoch 3/5\n",
      "765/765 [==============================] - 1s 656us/step - loss: 0.7977 - acc: 0.7294 - val_loss: 0.8235 - val_acc: 0.6991\n",
      "Epoch 4/5\n",
      "765/765 [==============================] - 0s 619us/step - loss: 0.6379 - acc: 0.7791 - val_loss: 0.8267 - val_acc: 0.7082\n",
      "Epoch 5/5\n",
      "765/765 [==============================] - 0s 615us/step - loss: 0.5530 - acc: 0.8405 - val_loss: 0.9402 - val_acc: 0.6960\n",
      "[[273 129]\n",
      " [  3 360]]\n",
      "0.8450704225352113\n",
      "0.9917355371900827\n",
      "[[100  78]\n",
      " [ 22 129]]\n",
      "0.7206703910614525\n",
      "0.8543046357615894\n"
     ]
    }
   ],
   "source": [
    "# Access Type Model (User Access, Edit and Deletion)\n",
    "\n",
    "padded_docs = pad_sequences(user_access_edit_deletion.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_access_edit_deletion.Access_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('user_access_edit_deletion_access_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 765 samples, validate on 329 samples\n",
      "Epoch 1/5\n",
      "765/765 [==============================] - 2s 2ms/step - loss: 0.9084 - acc: 0.6732 - val_loss: 0.4542 - val_acc: 0.8450\n",
      "Epoch 2/5\n",
      "765/765 [==============================] - 0s 621us/step - loss: 0.5366 - acc: 0.8706 - val_loss: 0.4711 - val_acc: 0.8419\n",
      "Epoch 3/5\n",
      "765/765 [==============================] - 0s 629us/step - loss: 0.4201 - acc: 0.8627 - val_loss: 0.4509 - val_acc: 0.8663\n",
      "Epoch 4/5\n",
      "765/765 [==============================] - 0s 622us/step - loss: 0.3544 - acc: 0.9124 - val_loss: 0.4830 - val_acc: 0.8663\n",
      "Epoch 5/5\n",
      "765/765 [==============================] - 0s 653us/step - loss: 0.3071 - acc: 0.8967 - val_loss: 0.4772 - val_acc: 0.8845\n",
      "[[612  45]\n",
      " [  7 101]]\n",
      "0.795275590551181\n",
      "0.9351851851851852\n",
      "[[252  26]\n",
      " [ 12  39]]\n",
      "0.6724137931034482\n",
      "0.7647058823529411\n"
     ]
    }
   ],
   "source": [
    "# User Type Model (User Access, Edit and Deletion)\n",
    "\n",
    "padded_docs = pad_sequences(user_access_edit_deletion.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_access_edit_deletion.User_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('user_access_edit_deletion_user_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER CHOICE/CONTROL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2658 samples, validate on 1140 samples\n",
      "Epoch 1/5\n",
      "2658/2658 [==============================] - 3s 1ms/step - loss: 1.3325 - acc: 0.4108 - val_loss: 0.8313 - val_acc: 0.4684\n",
      "Epoch 2/5\n",
      "2658/2658 [==============================] - 2s 654us/step - loss: 1.0718 - acc: 0.6065 - val_loss: 0.8020 - val_acc: 0.5035\n",
      "Epoch 3/5\n",
      "2658/2658 [==============================] - 2s 627us/step - loss: 0.9292 - acc: 0.6697 - val_loss: 0.8092 - val_acc: 0.5596\n",
      "Epoch 4/5\n",
      "2658/2658 [==============================] - 2s 630us/step - loss: 0.8159 - acc: 0.7378 - val_loss: 0.7550 - val_acc: 0.6228\n",
      "Epoch 5/5\n",
      "2658/2658 [==============================] - 2s 698us/step - loss: 0.7079 - acc: 0.7904 - val_loss: 0.7698 - val_acc: 0.6368\n",
      "[[1535  368]\n",
      " [  39  716]]\n",
      "0.77868406742795\n",
      "0.9483443708609272\n",
      "[[504 292]\n",
      " [122 222]]\n",
      "0.5174825174825175\n",
      "0.6453488372093024\n"
     ]
    }
   ],
   "source": [
    "# Choice Scope Model (User Choice/Control)\n",
    "\n",
    "padded_docs = pad_sequences(user_choice_control.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_choice_control.Choice_Scope, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('user_choice_control_choice_scope.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2658 samples, validate on 1140 samples\n",
      "Epoch 1/5\n",
      "2658/2658 [==============================] - 4s 2ms/step - loss: 1.1361 - acc: 0.6166 - val_loss: 0.6711 - val_acc: 0.6956\n",
      "Epoch 2/5\n",
      "2658/2658 [==============================] - 2s 791us/step - loss: 0.7587 - acc: 0.7784 - val_loss: 0.6881 - val_acc: 0.7096\n",
      "Epoch 3/5\n",
      "2658/2658 [==============================] - 2s 643us/step - loss: 0.6166 - acc: 0.8198 - val_loss: 0.6226 - val_acc: 0.7482\n",
      "Epoch 4/5\n",
      "2658/2658 [==============================] - 2s 634us/step - loss: 0.5241 - acc: 0.8563 - val_loss: 0.6342 - val_acc: 0.7518\n",
      "Epoch 5/5\n",
      "2658/2658 [==============================] - 2s 628us/step - loss: 0.4542 - acc: 0.8864 - val_loss: 0.6448 - val_acc: 0.7693\n",
      "[[1536  227]\n",
      " [  17  878]]\n",
      "0.8779999999999999\n",
      "0.9810055865921787\n",
      "[[580 193]\n",
      " [ 70 297]]\n",
      "0.6931155192532089\n",
      "0.8092643051771117\n"
     ]
    }
   ],
   "source": [
    "# Choice Type Model (User Choice/Control)\n",
    "\n",
    "padded_docs = pad_sequences(user_choice_control.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_choice_control.Choice_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('user_choice_control_choice_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2658 samples, validate on 1140 samples\n",
      "Epoch 1/5\n",
      "2658/2658 [==============================] - 3s 1ms/step - loss: 0.8621 - acc: 0.7761 - val_loss: 0.4588 - val_acc: 0.8070\n",
      "Epoch 2/5\n",
      "2658/2658 [==============================] - 2s 660us/step - loss: 0.5762 - acc: 0.8683 - val_loss: 0.4348 - val_acc: 0.8114\n",
      "Epoch 3/5\n",
      "2658/2658 [==============================] - 2s 668us/step - loss: 0.4708 - acc: 0.8837 - val_loss: 0.4027 - val_acc: 0.8377\n",
      "Epoch 4/5\n",
      "2658/2658 [==============================] - 2s 675us/step - loss: 0.4043 - acc: 0.8984 - val_loss: 0.4109 - val_acc: 0.8386\n",
      "Epoch 5/5\n",
      "2658/2658 [==============================] - 2s 693us/step - loss: 0.3537 - acc: 0.9108 - val_loss: 0.4272 - val_acc: 0.8404\n",
      "[[2115  201]\n",
      " [  26  316]]\n",
      "0.7357392316647263\n",
      "0.9239766081871345\n",
      "[[872 118]\n",
      " [ 64  86]]\n",
      "0.48587570621468934\n",
      "0.5733333333333334\n"
     ]
    }
   ],
   "source": [
    "# Personal Information Type Model (User Choice/Control)\n",
    "\n",
    "padded_docs = pad_sequences(user_choice_control.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_choice_control.Personal_Information_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('user_choice_control_personal_information_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2658 samples, validate on 1140 samples\n",
      "Epoch 1/5\n",
      "2658/2658 [==============================] - 3s 1ms/step - loss: 1.1136 - acc: 0.6193 - val_loss: 0.6622 - val_acc: 0.6632\n",
      "Epoch 2/5\n",
      "2658/2658 [==============================] - 2s 615us/step - loss: 0.7654 - acc: 0.7863 - val_loss: 0.5824 - val_acc: 0.7526\n",
      "Epoch 3/5\n",
      "2658/2658 [==============================] - 2s 628us/step - loss: 0.6194 - acc: 0.8232 - val_loss: 0.6671 - val_acc: 0.7211\n",
      "Epoch 4/5\n",
      "2658/2658 [==============================] - 2s 630us/step - loss: 0.5373 - acc: 0.8582 - val_loss: 0.6327 - val_acc: 0.7526\n",
      "Epoch 5/5\n",
      "2658/2658 [==============================] - 2s 639us/step - loss: 0.4614 - acc: 0.8751 - val_loss: 0.6795 - val_acc: 0.7456\n",
      "[[1791  292]\n",
      " [  18  557]]\n",
      "0.7823033707865168\n",
      "0.9686956521739131\n",
      "[[694 219]\n",
      " [ 71 156]]\n",
      "0.5182724252491695\n",
      "0.6872246696035242\n"
     ]
    }
   ],
   "source": [
    "# Purpose Model (User Choice/Control)\n",
    "\n",
    "padded_docs = pad_sequences(user_choice_control.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_choice_control.Purpose, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('user_choice_control_purpose.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2658 samples, validate on 1140 samples\n",
      "Epoch 1/5\n",
      "2658/2658 [==============================] - 3s 1ms/step - loss: 0.4448 - acc: 0.9379 - val_loss: 0.2118 - val_acc: 0.9351\n",
      "Epoch 2/5\n",
      "2658/2658 [==============================] - 2s 586us/step - loss: 0.2340 - acc: 0.9579 - val_loss: 0.1836 - val_acc: 0.9351\n",
      "Epoch 3/5\n",
      "2658/2658 [==============================] - 2s 614us/step - loss: 0.1699 - acc: 0.9676 - val_loss: 0.1578 - val_acc: 0.9500\n",
      "Epoch 4/5\n",
      "2658/2658 [==============================] - 2s 596us/step - loss: 0.1346 - acc: 0.9763 - val_loss: 0.1479 - val_acc: 0.9640\n",
      "Epoch 5/5\n",
      "2658/2658 [==============================] - 2s 627us/step - loss: 0.1168 - acc: 0.9804 - val_loss: 0.1554 - val_acc: 0.9535\n",
      "[[2533   34]\n",
      " [  13   78]]\n",
      "0.768472906403941\n",
      "0.8571428571428571\n",
      "[[1060   28]\n",
      " [  25   27]]\n",
      "0.5046728971962617\n",
      "0.5192307692307693\n"
     ]
    }
   ],
   "source": [
    "# User Type Model (User Choice/Control)\n",
    "\n",
    "padded_docs = pad_sequences(user_choice_control.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_choice_control.User_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "model.save('user_choice_control_user_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
