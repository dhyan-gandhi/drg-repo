{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations=glob.glob('data/OPP-115/annotations/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_list=[]\n",
    "for file in annotations:\n",
    "    annotations_list.append(pd.read_csv(file, header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot=pd.concat(annotations_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot.columns = ['annotation_ID', 'batch_ID', 'annotator_ID', 'policy_ID', 'segment_ID','category_name',\n",
    "            'attribute_value_pairs','date','policy_URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot['dict'] = annot.attribute_value_pairs.apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_ID</th>\n",
       "      <th>batch_ID</th>\n",
       "      <th>annotator_ID</th>\n",
       "      <th>policy_ID</th>\n",
       "      <th>segment_ID</th>\n",
       "      <th>category_name</th>\n",
       "      <th>attribute_value_pairs</th>\n",
       "      <th>date</th>\n",
       "      <th>policy_URL</th>\n",
       "      <th>dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13160</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>{\"Other Type\": {\"endIndexInSegment\": 575, \"sta...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Other Type': {'endIndexInSegment': 575, 'sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13161</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13162</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13163</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13164</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   annotation_ID                                       batch_ID  annotator_ID  \\\n",
       "0          13160  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "1          13161  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "2          13162  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "3          13163  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "4          13164  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "\n",
       "   policy_ID  segment_ID               category_name  \\\n",
       "0       3828           0                       Other   \n",
       "1       3828           1  First Party Collection/Use   \n",
       "2       3828           1  First Party Collection/Use   \n",
       "3       3828           1  First Party Collection/Use   \n",
       "4       3828           1  First Party Collection/Use   \n",
       "\n",
       "                               attribute_value_pairs    date  \\\n",
       "0  {\"Other Type\": {\"endIndexInSegment\": 575, \"sta...  5/7/15   \n",
       "1  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "2  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "3  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "4  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "\n",
       "                                          policy_URL  \\\n",
       "0  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "1  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "2  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "3  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "4  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "\n",
       "                                                dict  \n",
       "0  {'Other Type': {'endIndexInSegment': 575, 'sta...  \n",
       "1  {'Collection Mode': {'endIndexInSegment': -1, ...  \n",
       "2  {'Collection Mode': {'endIndexInSegment': -1, ...  \n",
       "3  {'Collection Mode': {'endIndexInSegment': -1, ...  \n",
       "4  {'Collection Mode': {'endIndexInSegment': -1, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in annot['dict']:\n",
    "    for key, value in x.items():\n",
    "        value.pop('endIndexInSegment', None)\n",
    "        value.pop('startIndexInSegment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[]\n",
    "category=[]\n",
    "subcat=[]\n",
    "label=[]\n",
    "counter=0\n",
    "for i,x in enumerate(annot['dict']):\n",
    "    for key, value in x.items():\n",
    "        subcat.append(key)\n",
    "        if value.get('selectedText')==None:\n",
    "            text.append('noSelectedText')\n",
    "        else:\n",
    "            text.append(value.get('selectedText'))  \n",
    "        category.append(annot['category_name'][i])\n",
    "        for k, v in value.items():\n",
    "            if k=='value':\n",
    "                label.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=list(zip(text, category, subcat, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(d, columns=['text', 'category', 'subcategory', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Effective Date: May 7, 2015 Kraft Site Privacy...</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other Type</td>\n",
       "      <td>Introductory/Generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>noSelectedText</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Collection Mode</td>\n",
       "      <td>not-selected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>collec</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Choice Scope</td>\n",
       "      <td>Collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>register on our website or participate in our ...</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Action First-Party</td>\n",
       "      <td>Collect on website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>personally-identifiable information, such as</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Personal Information Type</td>\n",
       "      <td>Generic personal information</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Effective Date: May 7, 2015 Kraft Site Privacy...   \n",
       "1                                     noSelectedText   \n",
       "2                                             collec   \n",
       "3  register on our website or participate in our ...   \n",
       "4       personally-identifiable information, such as   \n",
       "\n",
       "                     category                subcategory  \\\n",
       "0                       Other                 Other Type   \n",
       "1  First Party Collection/Use            Collection Mode   \n",
       "2  First Party Collection/Use               Choice Scope   \n",
       "3  First Party Collection/Use         Action First-Party   \n",
       "4  First Party Collection/Use  Personal Information Type   \n",
       "\n",
       "                          label  \n",
       "0          Introductory/Generic  \n",
       "1                  not-selected  \n",
       "2                    Collection  \n",
       "3            Collect on website  \n",
       "4  Generic personal information  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "data['text'] = data['text'].apply(lambda x : remove_punct(x.lower()))\n",
    "\n",
    "data = data[data['text']!='null']\n",
    "data = data[data['text']!='noselectedtext']\n",
    "data = data[data['text']!='']\n",
    "data = data[data['text']!=' ']\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "ds = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>effective date may 7 2015 kraft site privacy n...</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other Type</td>\n",
       "      <td>Introductory/Generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>collec</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Choice Scope</td>\n",
       "      <td>Collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>register on our website or participate in our ...</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Action First-Party</td>\n",
       "      <td>Collect on website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>personallyidentifiable information such as</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Personal Information Type</td>\n",
       "      <td>Generic personal information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if you choose to register on our website or pa...</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Choice Type</td>\n",
       "      <td>Opt-in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  effective date may 7 2015 kraft site privacy n...   \n",
       "1                                             collec   \n",
       "2  register on our website or participate in our ...   \n",
       "3         personallyidentifiable information such as   \n",
       "4  if you choose to register on our website or pa...   \n",
       "\n",
       "                     category                subcategory  \\\n",
       "0                       Other                 Other Type   \n",
       "1  First Party Collection/Use               Choice Scope   \n",
       "2  First Party Collection/Use         Action First-Party   \n",
       "3  First Party Collection/Use  Personal Information Type   \n",
       "4  First Party Collection/Use                Choice Type   \n",
       "\n",
       "                          label  \n",
       "0          Introductory/Generic  \n",
       "1                    Collection  \n",
       "2            Collect on website  \n",
       "3  Generic personal information  \n",
       "4                        Opt-in  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "ds['tokenized'] = ds['text'].apply(lambda x : re.split(' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    " \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-19 21:59:46,671 : INFO : loading Word2Vec object from word2vec.model\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-09-19 21:59:46,714 : INFO : loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "2019-09-19 21:59:46,715 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-09-19 21:59:46,717 : INFO : loading vocabulary recursively from word2vec.model.vocabulary.* with mmap=None\n",
      "2019-09-19 21:59:46,719 : INFO : loading trainables recursively from word2vec.model.trainables.* with mmap=None\n",
      "2019-09-19 21:59:46,721 : INFO : setting ignored attribute cum_table to None\n",
      "2019-09-19 21:59:46,722 : INFO : loaded word2vec.model\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {k: v.index for k, v in word_vectors.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_word(new_word, new_vector, new_index, embedding_matrix, word2id):\n",
    "    embedding_matrix = np.insert(embedding_matrix, [new_index], [new_vector], axis=0)\n",
    "    \n",
    "    word2id = {word: (index+1) if index >= new_index else index for word, index in word2id.items()}\n",
    "    word2id[new_word] = new_index\n",
    "    return embedding_matrix, word2id\n",
    "\n",
    "UNK_INDEX = 0\n",
    "UNK_TOKEN = 'UNK'\n",
    "\n",
    "embedding_matrix = word_vectors.vectors\n",
    "unk_vector = embedding_matrix.mean(0)\n",
    "embedding_matrix, word2id = add_new_word(UNK_TOKEN, unk_vector, UNK_INDEX, embedding_matrix, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.insert(embedding_matrix, len(embedding_matrix), [np.zeros((100,))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created. Percentage of unknown words: 7006.000\n"
     ]
    }
   ],
   "source": [
    "def get_int_data(token_text, word2id):\n",
    "    x = []\n",
    "    unk_count = 0\n",
    "    for item in token_text:\n",
    "        temp=[]\n",
    "        x.append(temp)\n",
    "        for word in item:\n",
    "            if word in word2id:\n",
    "                temp.append(word2id.get(word))\n",
    "            else:\n",
    "                temp.append(UNK_INDEX)\n",
    "                unk_count += 1\n",
    "    print('Data created. Percentage of unknown words: %.3f' % (unk_count))\n",
    "    return np.array(x)\n",
    "\n",
    "x=get_int_data(ds.tokenized, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "ds['enumerated_text']=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>enumerated_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Data Retention</th>\n",
       "      <th>Personal Information Type</th>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retention Period</th>\n",
       "      <td>234</td>\n",
       "      <td>234</td>\n",
       "      <td>234</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retention Purpose</th>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data Security</th>\n",
       "      <th>Security Measure</th>\n",
       "      <td>808</td>\n",
       "      <td>808</td>\n",
       "      <td>808</td>\n",
       "      <td>808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Do Not Track</th>\n",
       "      <th>Do Not Track policy</th>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">First Party Collection/Use</th>\n",
       "      <th>Action First-Party</th>\n",
       "      <td>2733</td>\n",
       "      <td>2733</td>\n",
       "      <td>2733</td>\n",
       "      <td>2733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Choice Scope</th>\n",
       "      <td>528</td>\n",
       "      <td>528</td>\n",
       "      <td>528</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Choice Type</th>\n",
       "      <td>937</td>\n",
       "      <td>937</td>\n",
       "      <td>937</td>\n",
       "      <td>937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Collection Mode</th>\n",
       "      <td>1528</td>\n",
       "      <td>1528</td>\n",
       "      <td>1528</td>\n",
       "      <td>1528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Does/Does Not</th>\n",
       "      <td>1308</td>\n",
       "      <td>1308</td>\n",
       "      <td>1308</td>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifiability</th>\n",
       "      <td>636</td>\n",
       "      <td>636</td>\n",
       "      <td>636</td>\n",
       "      <td>636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personal Information Type</th>\n",
       "      <td>3119</td>\n",
       "      <td>3119</td>\n",
       "      <td>3119</td>\n",
       "      <td>3119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Purpose</th>\n",
       "      <td>4214</td>\n",
       "      <td>4214</td>\n",
       "      <td>4214</td>\n",
       "      <td>4214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Type</th>\n",
       "      <td>343</td>\n",
       "      <td>343</td>\n",
       "      <td>343</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>International and Specific Audiences</th>\n",
       "      <th>Audience Type</th>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <th>Other Type</th>\n",
       "      <td>2759</td>\n",
       "      <td>2759</td>\n",
       "      <td>2759</td>\n",
       "      <td>2759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Policy Change</th>\n",
       "      <th>Change Type</th>\n",
       "      <td>341</td>\n",
       "      <td>341</td>\n",
       "      <td>341</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Notification Type</th>\n",
       "      <td>403</td>\n",
       "      <td>403</td>\n",
       "      <td>403</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Choice</th>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">Third Party Sharing/Collection</th>\n",
       "      <th>Action Third Party</th>\n",
       "      <td>1889</td>\n",
       "      <td>1889</td>\n",
       "      <td>1889</td>\n",
       "      <td>1889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Choice Scope</th>\n",
       "      <td>342</td>\n",
       "      <td>342</td>\n",
       "      <td>342</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Choice Type</th>\n",
       "      <td>681</td>\n",
       "      <td>681</td>\n",
       "      <td>681</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Does/Does Not</th>\n",
       "      <td>937</td>\n",
       "      <td>937</td>\n",
       "      <td>937</td>\n",
       "      <td>937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifiability</th>\n",
       "      <td>386</td>\n",
       "      <td>386</td>\n",
       "      <td>386</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personal Information Type</th>\n",
       "      <td>1463</td>\n",
       "      <td>1463</td>\n",
       "      <td>1463</td>\n",
       "      <td>1463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Purpose</th>\n",
       "      <td>2727</td>\n",
       "      <td>2727</td>\n",
       "      <td>2727</td>\n",
       "      <td>2727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Third Party Entity</th>\n",
       "      <td>2094</td>\n",
       "      <td>2094</td>\n",
       "      <td>2094</td>\n",
       "      <td>2094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Type</th>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">User Access, Edit and Deletion</th>\n",
       "      <th>Access Scope</th>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Access Type</th>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Type</th>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">User Choice/Control</th>\n",
       "      <th>Choice Scope</th>\n",
       "      <td>1099</td>\n",
       "      <td>1099</td>\n",
       "      <td>1099</td>\n",
       "      <td>1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Choice Type</th>\n",
       "      <td>1262</td>\n",
       "      <td>1262</td>\n",
       "      <td>1262</td>\n",
       "      <td>1262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personal Information Type</th>\n",
       "      <td>492</td>\n",
       "      <td>492</td>\n",
       "      <td>492</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Purpose</th>\n",
       "      <td>802</td>\n",
       "      <td>802</td>\n",
       "      <td>802</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Type</th>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                text  label  \\\n",
       "category                             subcategory                              \n",
       "Data Retention                       Personal Information Type   188    188   \n",
       "                                     Retention Period            234    234   \n",
       "                                     Retention Purpose           254    254   \n",
       "Data Security                        Security Measure            808    808   \n",
       "Do Not Track                         Do Not Track policy          67     67   \n",
       "First Party Collection/Use           Action First-Party         2733   2733   \n",
       "                                     Choice Scope                528    528   \n",
       "                                     Choice Type                 937    937   \n",
       "                                     Collection Mode            1528   1528   \n",
       "                                     Does/Does Not              1308   1308   \n",
       "                                     Identifiability             636    636   \n",
       "                                     Personal Information Type  3119   3119   \n",
       "                                     Purpose                    4214   4214   \n",
       "                                     User Type                   343    343   \n",
       "International and Specific Audiences Audience Type               582    582   \n",
       "Other                                Other Type                 2759   2759   \n",
       "Policy Change                        Change Type                 341    341   \n",
       "                                     Notification Type           403    403   \n",
       "                                     User Choice                 188    188   \n",
       "Third Party Sharing/Collection       Action Third Party         1889   1889   \n",
       "                                     Choice Scope                342    342   \n",
       "                                     Choice Type                 681    681   \n",
       "                                     Does/Does Not               937    937   \n",
       "                                     Identifiability             386    386   \n",
       "                                     Personal Information Type  1463   1463   \n",
       "                                     Purpose                    2727   2727   \n",
       "                                     Third Party Entity         2094   2094   \n",
       "                                     User Type                   136    136   \n",
       "User Access, Edit and Deletion       Access Scope                421    421   \n",
       "                                     Access Type                 514    514   \n",
       "                                     User Type                   159    159   \n",
       "User Choice/Control                  Choice Scope               1099   1099   \n",
       "                                     Choice Type                1262   1262   \n",
       "                                     Personal Information Type   492    492   \n",
       "                                     Purpose                     802    802   \n",
       "                                     User Type                   143    143   \n",
       "\n",
       "                                                                tokenized  \\\n",
       "category                             subcategory                            \n",
       "Data Retention                       Personal Information Type        188   \n",
       "                                     Retention Period                 234   \n",
       "                                     Retention Purpose                254   \n",
       "Data Security                        Security Measure                 808   \n",
       "Do Not Track                         Do Not Track policy               67   \n",
       "First Party Collection/Use           Action First-Party              2733   \n",
       "                                     Choice Scope                     528   \n",
       "                                     Choice Type                      937   \n",
       "                                     Collection Mode                 1528   \n",
       "                                     Does/Does Not                   1308   \n",
       "                                     Identifiability                  636   \n",
       "                                     Personal Information Type       3119   \n",
       "                                     Purpose                         4214   \n",
       "                                     User Type                        343   \n",
       "International and Specific Audiences Audience Type                    582   \n",
       "Other                                Other Type                      2759   \n",
       "Policy Change                        Change Type                      341   \n",
       "                                     Notification Type                403   \n",
       "                                     User Choice                      188   \n",
       "Third Party Sharing/Collection       Action Third Party              1889   \n",
       "                                     Choice Scope                     342   \n",
       "                                     Choice Type                      681   \n",
       "                                     Does/Does Not                    937   \n",
       "                                     Identifiability                  386   \n",
       "                                     Personal Information Type       1463   \n",
       "                                     Purpose                         2727   \n",
       "                                     Third Party Entity              2094   \n",
       "                                     User Type                        136   \n",
       "User Access, Edit and Deletion       Access Scope                     421   \n",
       "                                     Access Type                      514   \n",
       "                                     User Type                        159   \n",
       "User Choice/Control                  Choice Scope                    1099   \n",
       "                                     Choice Type                     1262   \n",
       "                                     Personal Information Type        492   \n",
       "                                     Purpose                          802   \n",
       "                                     User Type                        143   \n",
       "\n",
       "                                                                enumerated_text  \n",
       "category                             subcategory                                 \n",
       "Data Retention                       Personal Information Type              188  \n",
       "                                     Retention Period                       234  \n",
       "                                     Retention Purpose                      254  \n",
       "Data Security                        Security Measure                       808  \n",
       "Do Not Track                         Do Not Track policy                     67  \n",
       "First Party Collection/Use           Action First-Party                    2733  \n",
       "                                     Choice Scope                           528  \n",
       "                                     Choice Type                            937  \n",
       "                                     Collection Mode                       1528  \n",
       "                                     Does/Does Not                         1308  \n",
       "                                     Identifiability                        636  \n",
       "                                     Personal Information Type             3119  \n",
       "                                     Purpose                               4214  \n",
       "                                     User Type                              343  \n",
       "International and Specific Audiences Audience Type                          582  \n",
       "Other                                Other Type                            2759  \n",
       "Policy Change                        Change Type                            341  \n",
       "                                     Notification Type                      403  \n",
       "                                     User Choice                            188  \n",
       "Third Party Sharing/Collection       Action Third Party                    1889  \n",
       "                                     Choice Scope                           342  \n",
       "                                     Choice Type                            681  \n",
       "                                     Does/Does Not                          937  \n",
       "                                     Identifiability                        386  \n",
       "                                     Personal Information Type             1463  \n",
       "                                     Purpose                               2727  \n",
       "                                     Third Party Entity                    2094  \n",
       "                                     User Type                              136  \n",
       "User Access, Edit and Deletion       Access Scope                           421  \n",
       "                                     Access Type                            514  \n",
       "                                     User Type                              159  \n",
       "User Choice/Control                  Choice Scope                          1099  \n",
       "                                     Choice Type                           1262  \n",
       "                                     Personal Information Type              492  \n",
       "                                     Purpose                                802  \n",
       "                                     User Type                              143  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.groupby(['category','subcategory']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>enumerated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>effective date may 7 2015 kraft site privacy n...</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other Type</td>\n",
       "      <td>Introductory/Generic</td>\n",
       "      <td>[effective, date, may, 7, 2015, kraft, site, p...</td>\n",
       "      <td>[553, 244, 11, 2150, 839, 1332, 35, 26, 139, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>collec</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Choice Scope</td>\n",
       "      <td>Collection</td>\n",
       "      <td>[collec]</td>\n",
       "      <td>[2151]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>register on our website or participate in our ...</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Action First-Party</td>\n",
       "      <td>Collect on website</td>\n",
       "      <td>[register, on, our, website, or, participate, ...</td>\n",
       "      <td>[204, 19, 10, 52, 6, 212, 13, 10, 168, 3, 363]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>personallyidentifiable information such as</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Personal Information Type</td>\n",
       "      <td>Generic personal information</td>\n",
       "      <td>[personallyidentifiable, information, such, as]</td>\n",
       "      <td>[630, 5, 31, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if you choose to register on our website or pa...</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Choice Type</td>\n",
       "      <td>Opt-in</td>\n",
       "      <td>[if, you, choose, to, register, on, our, websi...</td>\n",
       "      <td>[32, 4, 105, 1, 204, 19, 10, 52, 6, 212, 13, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  effective date may 7 2015 kraft site privacy n...   \n",
       "1                                             collec   \n",
       "2  register on our website or participate in our ...   \n",
       "3         personallyidentifiable information such as   \n",
       "4  if you choose to register on our website or pa...   \n",
       "\n",
       "                     category                subcategory  \\\n",
       "0                       Other                 Other Type   \n",
       "1  First Party Collection/Use               Choice Scope   \n",
       "2  First Party Collection/Use         Action First-Party   \n",
       "3  First Party Collection/Use  Personal Information Type   \n",
       "4  First Party Collection/Use                Choice Type   \n",
       "\n",
       "                          label  \\\n",
       "0          Introductory/Generic   \n",
       "1                    Collection   \n",
       "2            Collect on website   \n",
       "3  Generic personal information   \n",
       "4                        Opt-in   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [effective, date, may, 7, 2015, kraft, site, p...   \n",
       "1                                           [collec]   \n",
       "2  [register, on, our, website, or, participate, ...   \n",
       "3    [personallyidentifiable, information, such, as]   \n",
       "4  [if, you, choose, to, register, on, our, websi...   \n",
       "\n",
       "                                     enumerated_text  \n",
       "0  [553, 244, 11, 2150, 839, 1332, 35, 26, 139, 2...  \n",
       "1                                             [2151]  \n",
       "2     [204, 19, 10, 52, 6, 212, 13, 10, 168, 3, 363]  \n",
       "3                                   [630, 5, 31, 23]  \n",
       "4  [32, 4, 105, 1, 204, 19, 10, 52, 6, 212, 13, 1...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Retention_Personal_Information_Type = ds[(ds['category']=='Data Retention') & (ds['subcategory']=='Personal Information Type')]\n",
    "Data_Retention_Retention_Period = ds[(ds['category']=='Data Retention') & (ds['subcategory']=='Retention Period')]\n",
    "Data_Retention_Retention_Purpose = ds[(ds['category']=='Data Retention') & (ds['subcategory']=='Retention Purpose')]\n",
    "\n",
    "Data_Security_Security_Measure = ds[(ds['category']=='Data Security') & (ds['subcategory']=='Security Measure')]\n",
    "\n",
    "Do_Not_Track_Do_Not_Track_Policy = ds[(ds['category']=='Do Not Track') & (ds['subcategory']=='Do Not Track policy')]\n",
    "\n",
    "First_Party_Collection_Use_Action_First_Party = ds[(ds['category']=='First Party Collection/Use') & (ds['subcategory']=='Action First-Party')]\n",
    "First_Party_Collection_Use_Choice_Scope = ds[(ds['category']=='First Party Collection/Use') & (ds['subcategory']=='Choice Scope')]\n",
    "First_Party_Collection_Use_Choice_Type = ds[(ds['category']=='First Party Collection/Use') & (ds['subcategory']=='Choice Type')]\n",
    "First_Party_Collection_Use_Collection_Mode = ds[(ds['category']=='First Party Collection/Use') & (ds['subcategory']=='Collection Mode')]\n",
    "First_Party_Collection_Use_Does_Does_Not = ds[(ds['category']=='First Party Collection/Use') & (ds['subcategory']=='Does/Does Not')]\n",
    "First_Party_Collection_Use_Identifiability = ds[(ds['category']=='First Party Collection/Use') & (ds['subcategory']=='Identifiability')]\n",
    "First_Party_Collection_Use_Personal_Information_Type = ds[(ds['category']=='First Party Collection/Use') & (ds['subcategory']=='Personal Information Type')]\n",
    "First_Party_Collection_Use_Purpose = ds[(ds['category']=='First Party Collection/Use') & (ds['subcategory']=='Purpose')]\n",
    "First_Party_Collection_Use_User_Type = ds[(ds['category']=='First Party Collection/Use') & (ds['subcategory']=='User Type')]\n",
    "\n",
    "International_And_Specific_Audiences_Audience_Type = ds[(ds['category']=='International and Specific Audiences') & (ds['subcategory']=='Audience Type')]\n",
    "\n",
    "Other_Other_Type = ds[(ds['category']=='Other') & (ds['subcategory']=='Other Type')]\n",
    "\n",
    "Policy_Change_Change_Type = ds[(ds['category']=='Policy Change') & (ds['subcategory']=='Change Type')]\n",
    "Policy_Change_Notification_Type = ds[(ds['category']=='Policy Change') & (ds['subcategory']=='Notification Type')]\n",
    "Policy_Change_User_Choice = ds[(ds['category']=='Policy Change') & (ds['subcategory']=='User Choice')]\n",
    "\n",
    "Third_Party_Sharing_Collection_Action_Third_Party = ds[(ds['category']=='Third Party Sharing/Collection') & (ds['subcategory']=='Action Third Party')]\n",
    "Third_Party_Sharing_Collection_Choice_Scope = ds[(ds['category']=='Third Party Sharing/Collection') & (ds['subcategory']=='Choice Scope')]\n",
    "Third_Party_Sharing_Collection_Choice_Type = ds[(ds['category']=='Third Party Sharing/Collection') & (ds['subcategory']=='Choice Type')]\n",
    "Third_Party_Sharing_Collection_Does_Does_Not = ds[(ds['category']=='Third Party Sharing/Collection') & (ds['subcategory']=='Does/Does Not')]\n",
    "Third_Party_Sharing_Collection_Identifiability = ds[(ds['category']=='Third Party Sharing/Collection') & (ds['subcategory']=='Identifiability')]\n",
    "Third_Party_Sharing_Collection_Personal_Information_Type = ds[(ds['category']=='Third Party Sharing/Collection') & (ds['subcategory']=='Personal Information Type')]\n",
    "Third_Party_Sharing_Collection_Purpose = ds[(ds['category']=='Third Party Sharing/Collection') & (ds['subcategory']=='Purpose')]\n",
    "Third_Party_Sharing_Collection_Third_Party_Entity = ds[(ds['category']=='Third Party Sharing/Collection') & (ds['subcategory']=='Third Party Entity')]\n",
    "Third_Party_Sharing_Collection_User_Type = ds[(ds['category']=='Third Party Sharing/Collection') & (ds['subcategory']=='User Type')]\n",
    "\n",
    "User_Access_Edit_And_Deletion_Access_Scope = ds[(ds['category']=='User Access, Edit and Deletion') & (ds['subcategory']=='Access Scope')]\n",
    "User_Access_Edit_And_Deletion_Access_Type = ds[(ds['category']=='User Access, Edit and Deletion') & (ds['subcategory']=='Access Type')]\n",
    "User_Access_Edit_And_Deletion_User_Type = ds[(ds['category']=='User Access, Edit and Deletion') & (ds['subcategory']=='User Type')]\n",
    "\n",
    "User_Choice_Control_Choice_Scope = ds[(ds['category']=='User Choice/Control') & (ds['subcategory']=='Choice Scope')]\n",
    "User_Choice_Control_Choice_Type = ds[(ds['category']=='User Choice/Control') & (ds['subcategory']=='Choice Type')]\n",
    "User_Choice_Control_Personal_Information_Type = ds[(ds['category']=='User Choice/Control') & (ds['subcategory']=='Personal Information Type')]\n",
    "User_Choice_Control_Purpose = ds[(ds['category']=='User Choice/Control') & (ds['subcategory']=='Purpose')]\n",
    "User_Choice_Control_User_Type = ds[(ds['category']=='User Choice/Control') & (ds['subcategory']=='User Type')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Retention_Personal_Information_Type.reset_index(inplace=True, drop=True)\n",
    "Data_Retention_Retention_Period.reset_index(inplace=True, drop=True)\n",
    "Data_Retention_Retention_Purpose.reset_index(inplace=True, drop=True)\n",
    "\n",
    "Data_Security_Security_Measure.reset_index(inplace=True, drop=True)\n",
    "\n",
    "Do_Not_Track_Do_Not_Track_Policy.reset_index(inplace=True, drop=True)\n",
    "\n",
    "First_Party_Collection_Use_Action_First_Party.reset_index(inplace=True, drop=True)\n",
    "First_Party_Collection_Use_Choice_Scope.reset_index(inplace=True, drop=True)\n",
    "First_Party_Collection_Use_Choice_Type.reset_index(inplace=True, drop=True)\n",
    "First_Party_Collection_Use_Collection_Mode.reset_index(inplace=True, drop=True)\n",
    "First_Party_Collection_Use_Does_Does_Not.reset_index(inplace=True, drop=True)\n",
    "First_Party_Collection_Use_Identifiability.reset_index(inplace=True, drop=True)\n",
    "First_Party_Collection_Use_Personal_Information_Type.reset_index(inplace=True, drop=True)\n",
    "First_Party_Collection_Use_Purpose.reset_index(inplace=True, drop=True)\n",
    "First_Party_Collection_Use_User_Type.reset_index(inplace=True, drop=True)\n",
    "\n",
    "International_And_Specific_Audiences_Audience_Type.reset_index(inplace=True, drop=True)\n",
    "\n",
    "Other_Other_Type.reset_index(inplace=True, drop=True)\n",
    "\n",
    "Policy_Change_Change_Type.reset_index(inplace=True, drop=True)\n",
    "Policy_Change_Notification_Type.reset_index(inplace=True, drop=True)\n",
    "Policy_Change_User_Choice.reset_index(inplace=True, drop=True)\n",
    "\n",
    "Third_Party_Sharing_Collection_Third_Party_Entity.reset_index(inplace=True, drop=True)\n",
    "Third_Party_Sharing_Collection_Action_Third_Party.reset_index(inplace=True, drop=True)\n",
    "Third_Party_Sharing_Collection_Choice_Scope.reset_index(inplace=True, drop=True)\n",
    "Third_Party_Sharing_Collection_Choice_Type.reset_index(inplace=True, drop=True)\n",
    "Third_Party_Sharing_Collection_Does_Does_Not.reset_index(inplace=True, drop=True)\n",
    "Third_Party_Sharing_Collection_Identifiability.reset_index(inplace=True, drop=True)\n",
    "Third_Party_Sharing_Collection_Personal_Information_Type.reset_index(inplace=True, drop=True)\n",
    "Third_Party_Sharing_Collection_Purpose.reset_index(inplace=True, drop=True)\n",
    "Third_Party_Sharing_Collection_User_Type.reset_index(inplace=True, drop=True)\n",
    "\n",
    "User_Access_Edit_And_Deletion_Access_Scope.reset_index(inplace=True, drop=True)\n",
    "User_Access_Edit_And_Deletion_Access_Type.reset_index(inplace=True, drop=True)\n",
    "User_Access_Edit_And_Deletion_User_Type.reset_index(inplace=True, drop=True)\n",
    "\n",
    "User_Choice_Control_Choice_Scope.reset_index(inplace=True, drop=True)\n",
    "User_Choice_Control_Choice_Type.reset_index(inplace=True, drop=True)\n",
    "User_Choice_Control_Personal_Information_Type.reset_index(inplace=True, drop=True)\n",
    "User_Choice_Control_Purpose.reset_index(inplace=True, drop=True)\n",
    "User_Choice_Control_User_Type.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(ds.enumerated_text.apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encode = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 131 samples, validate on 57 samples\n",
      "Epoch 1/10\n",
      "131/131 [==============================] - 0s 4ms/step - loss: 2.7671 - acc: 0.0916 - val_loss: 2.6258 - val_acc: 0.2807\n",
      "Epoch 2/10\n",
      "131/131 [==============================] - 0s 1ms/step - loss: 2.3163 - acc: 0.4962 - val_loss: 2.4909 - val_acc: 0.2982\n",
      "Epoch 3/10\n",
      "131/131 [==============================] - 0s 813us/step - loss: 2.0080 - acc: 0.5038 - val_loss: 2.3979 - val_acc: 0.2807\n",
      "Epoch 4/10\n",
      "131/131 [==============================] - 0s 830us/step - loss: 1.7601 - acc: 0.5573 - val_loss: 2.2937 - val_acc: 0.3158\n",
      "Epoch 5/10\n",
      "131/131 [==============================] - 0s 857us/step - loss: 1.5605 - acc: 0.6641 - val_loss: 2.2080 - val_acc: 0.3333\n",
      "Epoch 6/10\n",
      "131/131 [==============================] - 0s 830us/step - loss: 1.3914 - acc: 0.6870 - val_loss: 2.1487 - val_acc: 0.3509\n",
      "Epoch 7/10\n",
      "131/131 [==============================] - 0s 807us/step - loss: 1.2625 - acc: 0.7176 - val_loss: 2.1050 - val_acc: 0.3509\n",
      "Epoch 8/10\n",
      "131/131 [==============================] - 0s 802us/step - loss: 1.1386 - acc: 0.7634 - val_loss: 2.0722 - val_acc: 0.3860\n",
      "Epoch 9/10\n",
      "131/131 [==============================] - 0s 888us/step - loss: 1.0430 - acc: 0.7863 - val_loss: 2.0439 - val_acc: 0.4386\n",
      "Epoch 10/10\n",
      "131/131 [==============================] - 0s 920us/step - loss: 0.9664 - acc: 0.8168 - val_loss: 2.0139 - val_acc: 0.4035\n",
      "[[ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 15  0  0  0  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  0  1  0  0  1  1  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 1  1  0  0  0  0  0  0  0 13  0  0  2  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  2  0  0  0  2  0  0 17  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1 14  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  0  2  4]]\n",
      "131/131 [==============================] - 0s 218us/step\n",
      "[0.9037989428025166, 0.8244274809160306]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 5 0 0 0 2 0 1 0 0 0 0 0 0]\n",
      " [0 0 3 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 2 1 0]\n",
      " [0 0 0 0 1 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 2 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 2 0 0 2 0 0 0 3 4 0]\n",
      " [0 0 0 0 1 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 7 0 0]\n",
      " [1 0 0 0 0 0 0 1 0 0 0 1 2 0]\n",
      " [0 0 0 0 1 0 0 2 0 0 0 1 2 0]]\n",
      "57/57 [==============================] - 0s 225us/step\n",
      "[2.0138703438273646, 0.403508768792738]\n"
     ]
    }
   ],
   "source": [
    "#Data_Retention_Personal_Information_Type\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Activation, Flatten\n",
    "\n",
    "\n",
    "padded_docs = pad_sequences(Data_Retention_Personal_Information_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Data_Retention_Personal_Information_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Data_Retention_Personal_Information_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "import pickle\n",
    "\n",
    "filename = 'data_retention_personal_information_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('data_retention_personal_information_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 163 samples, validate on 71 samples\n",
      "Epoch 1/10\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5681 - acc: 0.2945 - val_loss: 1.5744 - val_acc: 0.3521\n",
      "Epoch 2/10\n",
      "163/163 [==============================] - 0s 813us/step - loss: 1.0759 - acc: 0.6564 - val_loss: 1.5755 - val_acc: 0.4225\n",
      "Epoch 3/10\n",
      "163/163 [==============================] - 0s 683us/step - loss: 0.8179 - acc: 0.7423 - val_loss: 1.5661 - val_acc: 0.3662\n",
      "Epoch 4/10\n",
      "163/163 [==============================] - 0s 727us/step - loss: 0.6755 - acc: 0.7791 - val_loss: 1.6299 - val_acc: 0.3944\n",
      "Epoch 5/10\n",
      "163/163 [==============================] - 0s 685us/step - loss: 0.5571 - acc: 0.8282 - val_loss: 1.7580 - val_acc: 0.3803\n",
      "Epoch 6/10\n",
      "163/163 [==============================] - 0s 692us/step - loss: 0.4732 - acc: 0.8712 - val_loss: 1.8526 - val_acc: 0.3662\n",
      "Epoch 7/10\n",
      "163/163 [==============================] - 0s 709us/step - loss: 0.4041 - acc: 0.8589 - val_loss: 1.9402 - val_acc: 0.3380\n",
      "Epoch 8/10\n",
      "163/163 [==============================] - 0s 695us/step - loss: 0.3509 - acc: 0.8773 - val_loss: 2.0485 - val_acc: 0.3662\n",
      "Epoch 9/10\n",
      "163/163 [==============================] - 0s 743us/step - loss: 0.3068 - acc: 0.9080 - val_loss: 2.1339 - val_acc: 0.3239\n",
      "Epoch 10/10\n",
      "163/163 [==============================] - 0s 713us/step - loss: 0.2722 - acc: 0.9202 - val_loss: 2.2255 - val_acc: 0.3099\n",
      "[[11  1  0  0  3]\n",
      " [ 0 56  0  1  2]\n",
      " [ 0  2 11  0  0]\n",
      " [ 0  0  0 17  0]\n",
      " [ 0  0  1  0 58]]\n",
      "163/163 [==============================] - 0s 212us/step\n",
      "[0.2427898492504117, 0.9386503067484663]\n",
      "[[ 0  2  2  1  4]\n",
      " [ 1  6  4  1  1]\n",
      " [ 1  2  0  1  2]\n",
      " [ 1  5  1  2  3]\n",
      " [ 1 14  0  2 14]]\n",
      "71/71 [==============================] - 0s 246us/step\n",
      "[2.2254780480559444, 0.3098591553493285]\n"
     ]
    }
   ],
   "source": [
    "#Data Retention-Retention Period\n",
    "\n",
    "padded_docs = pad_sequences(Data_Retention_Retention_Period.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Data_Retention_Retention_Period.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Data_Retention_Retention_Period.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'data_retention_rentention_period_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('data_retention_rentention_period_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 177 samples, validate on 77 samples\n",
      "Epoch 1/10\n",
      "177/177 [==============================] - 1s 3ms/step - loss: 2.0844 - acc: 0.1751 - val_loss: 1.9212 - val_acc: 0.2468\n",
      "Epoch 2/10\n",
      "177/177 [==============================] - 0s 639us/step - loss: 1.5582 - acc: 0.6384 - val_loss: 1.7684 - val_acc: 0.3766\n",
      "Epoch 3/10\n",
      "177/177 [==============================] - 0s 625us/step - loss: 1.2367 - acc: 0.7006 - val_loss: 1.6461 - val_acc: 0.4416\n",
      "Epoch 4/10\n",
      "177/177 [==============================] - 0s 623us/step - loss: 1.0043 - acc: 0.7684 - val_loss: 1.5910 - val_acc: 0.4416\n",
      "Epoch 5/10\n",
      "177/177 [==============================] - 0s 646us/step - loss: 0.8384 - acc: 0.8079 - val_loss: 1.6264 - val_acc: 0.4805\n",
      "Epoch 6/10\n",
      "177/177 [==============================] - 0s 669us/step - loss: 0.7101 - acc: 0.8644 - val_loss: 1.6664 - val_acc: 0.4545\n",
      "Epoch 7/10\n",
      "177/177 [==============================] - 0s 683us/step - loss: 0.5924 - acc: 0.8701 - val_loss: 1.6555 - val_acc: 0.4545\n",
      "Epoch 8/10\n",
      "177/177 [==============================] - 0s 744us/step - loss: 0.5075 - acc: 0.8870 - val_loss: 1.6462 - val_acc: 0.4416\n",
      "Epoch 9/10\n",
      "177/177 [==============================] - 0s 681us/step - loss: 0.4339 - acc: 0.8983 - val_loss: 1.6605 - val_acc: 0.4675\n",
      "Epoch 10/10\n",
      "177/177 [==============================] - 0s 672us/step - loss: 0.3742 - acc: 0.9266 - val_loss: 1.7014 - val_acc: 0.4545\n",
      "[[ 2  1  0  0  0  0  0  0]\n",
      " [ 0 11  0  0  0  1  1  0]\n",
      " [ 0  0 32  0  0  0  1  0]\n",
      " [ 0  0  0  3  0  2  0  0]\n",
      " [ 0  0  0  0 13  1  1  1]\n",
      " [ 0  1  0  0  0 57  0  0]\n",
      " [ 0  0  0  0  0  1 24  0]\n",
      " [ 0  0  0  0  2  0  0 22]]\n",
      "177/177 [==============================] - 0s 192us/step\n",
      "[0.33349649052498703, 0.9265536723163842]\n",
      "[[ 1  0  0  2  0  1]\n",
      " [ 0  8  0  0  0  1]\n",
      " [ 0  1  0  7  1  1]\n",
      " [ 1  1  0 24  1  1]\n",
      " [ 0  0  1  8  2  1]\n",
      " [ 1  1  1 10  1  0]]\n",
      "77/77 [==============================] - 0s 217us/step\n",
      "[1.7013937683848592, 0.4545454549324977]\n"
     ]
    }
   ],
   "source": [
    "#Data Retention-Retention Purpose\n",
    "\n",
    "padded_docs = pad_sequences(Data_Retention_Retention_Purpose.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Data_Retention_Retention_Purpose.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Data_Retention_Retention_Purpose.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'data_retention_retention_purpose_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('data_retention_retention_purpose_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 565 samples, validate on 243 samples\n",
      "Epoch 1/10\n",
      "565/565 [==============================] - 1s 2ms/step - loss: 2.0194 - acc: 0.3027 - val_loss: 1.7958 - val_acc: 0.3539\n",
      "Epoch 2/10\n",
      "565/565 [==============================] - 0s 589us/step - loss: 1.2821 - acc: 0.5841 - val_loss: 1.6569 - val_acc: 0.4691\n",
      "Epoch 3/10\n",
      "565/565 [==============================] - 0s 671us/step - loss: 0.9151 - acc: 0.7540 - val_loss: 1.6478 - val_acc: 0.4938\n",
      "Epoch 4/10\n",
      "565/565 [==============================] - 0s 613us/step - loss: 0.6852 - acc: 0.8319 - val_loss: 1.6272 - val_acc: 0.5144\n",
      "Epoch 5/10\n",
      "565/565 [==============================] - 0s 674us/step - loss: 0.5179 - acc: 0.8779 - val_loss: 1.6650 - val_acc: 0.5144\n",
      "Epoch 6/10\n",
      "565/565 [==============================] - 0s 627us/step - loss: 0.4050 - acc: 0.8938 - val_loss: 1.6894 - val_acc: 0.5185\n",
      "Epoch 7/10\n",
      "565/565 [==============================] - 0s 662us/step - loss: 0.3263 - acc: 0.9062 - val_loss: 1.7402 - val_acc: 0.5103\n",
      "Epoch 8/10\n",
      "565/565 [==============================] - 0s 656us/step - loss: 0.2746 - acc: 0.9204 - val_loss: 1.7648 - val_acc: 0.5226\n",
      "Epoch 9/10\n",
      "565/565 [==============================] - 0s 623us/step - loss: 0.2364 - acc: 0.9133 - val_loss: 1.8023 - val_acc: 0.5144\n",
      "Epoch 10/10\n",
      "565/565 [==============================] - 0s 620us/step - loss: 0.2094 - acc: 0.9221 - val_loss: 1.8576 - val_acc: 0.5062\n",
      "[[ 68   1   2   0   0   0   1   0   0   0]\n",
      " [  0 205   5   0   0   2   1   0   0   0]\n",
      " [  0   3  70   0   0   1   1   2   0   0]\n",
      " [  0   0   0  21   0   2   0   0   0   0]\n",
      " [  0   1   0   0   4   0   0   0   0   0]\n",
      " [  0   3   0   0   2  44   0   0   0   0]\n",
      " [  1   2   0   0   0   0  22   0   0   0]\n",
      " [  0   0   0   0   0   0   2  64   0   0]\n",
      " [  0   2   0   0   0   0   0   0  24   0]\n",
      " [  0   2   0   0   0   1   0   0   0   6]]\n",
      "565/565 [==============================] - 0s 187us/step\n",
      "[0.19006283320156875, 0.9345132744417781]\n",
      "[[27  7  4  0  0  1  0  1  2  0]\n",
      " [ 3 63  9  0  0  4  1  3  1  0]\n",
      " [ 1 14 12  1  0  2  1  3  1  0]\n",
      " [ 1  2  0  1  0  5  0  0  0  0]\n",
      " [ 0  2  1  0  0  0  0  0  0  0]\n",
      " [ 0  6  2  2  0  0  0  2  0  0]\n",
      " [ 1  6  4  0  0  1  2  4  0  0]\n",
      " [ 1  9  0  0  0  1  3 18  0  0]\n",
      " [ 1  2  2  0  0  0  1  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  1  0  0]]\n",
      "243/243 [==============================] - 0s 222us/step\n",
      "[1.857586793938782, 0.5061728409778925]\n"
     ]
    }
   ],
   "source": [
    "#Data Security-Security Measure\n",
    "\n",
    "padded_docs = pad_sequences(Data_Security_Security_Measure.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Data_Security_Security_Measure.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Data_Security_Security_Measure.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'data_security_security_measure_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('data_security_security_measure_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 46 samples, validate on 21 samples\n",
      "Epoch 1/10\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5775 - acc: 0.1957 - val_loss: 0.8118 - val_acc: 0.8571\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.6867 - acc: 0.8696 - val_loss: 0.6175 - val_acc: 0.8571\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.4460 - acc: 0.9130 - val_loss: 0.5213 - val_acc: 0.8571\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.3136 - acc: 0.9130 - val_loss: 0.4831 - val_acc: 0.8571\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 0s 893us/step - loss: 0.2399 - acc: 0.9130 - val_loss: 0.4624 - val_acc: 0.8571\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 0s 916us/step - loss: 0.1970 - acc: 0.9348 - val_loss: 0.4494 - val_acc: 0.8571\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 0s 951us/step - loss: 0.1684 - acc: 0.9348 - val_loss: 0.4386 - val_acc: 0.9048\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.1490 - acc: 0.9348 - val_loss: 0.4288 - val_acc: 0.9048\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.1337 - acc: 0.9348 - val_loss: 0.4212 - val_acc: 0.9048\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.1206 - acc: 0.9348 - val_loss: 0.4175 - val_acc: 0.9048\n",
      "[[ 2  0  0  0  0]\n",
      " [ 0  3  1  0  0]\n",
      " [ 0  0 36  0  0]\n",
      " [ 0  0  1  0  0]\n",
      " [ 0  0  0  0  3]]\n",
      "46/46 [==============================] - 0s 223us/step\n",
      "[0.10977561642294345, 0.9565217313559159]\n",
      "[[ 1  2]\n",
      " [ 0 18]]\n",
      "21/21 [==============================] - 0s 245us/step\n",
      "[0.41745448112487793, 0.9047619104385376]\n"
     ]
    }
   ],
   "source": [
    "#Do Not Track-Do Not Track policy\n",
    "\n",
    "padded_docs = pad_sequences(Do_Not_Track_Do_Not_Track_Policy.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Do_Not_Track_Do_Not_Track_Policy.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Do_Not_Track_Do_Not_Track_Policy.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'do_not_track_do_not_track_policy_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('do_not_track_do_not_track_policy_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1913 samples, validate on 820 samples\n",
      "Epoch 1/10\n",
      "1913/1913 [==============================] - 2s 952us/step - loss: 1.7450 - acc: 0.4187 - val_loss: 1.5783 - val_acc: 0.4378\n",
      "Epoch 2/10\n",
      "1913/1913 [==============================] - 1s 649us/step - loss: 1.3403 - acc: 0.5149 - val_loss: 1.4934 - val_acc: 0.4537\n",
      "Epoch 3/10\n",
      "1913/1913 [==============================] - 1s 617us/step - loss: 1.1201 - acc: 0.6053 - val_loss: 1.4478 - val_acc: 0.4695\n",
      "Epoch 4/10\n",
      "1913/1913 [==============================] - 1s 606us/step - loss: 0.9478 - acc: 0.6733 - val_loss: 1.4572 - val_acc: 0.4841\n",
      "Epoch 5/10\n",
      "1913/1913 [==============================] - 1s 612us/step - loss: 0.8160 - acc: 0.7167 - val_loss: 1.4967 - val_acc: 0.4659\n",
      "Epoch 6/10\n",
      "1913/1913 [==============================] - 1s 603us/step - loss: 0.7131 - acc: 0.7480 - val_loss: 1.5315 - val_acc: 0.4829\n",
      "Epoch 7/10\n",
      "1913/1913 [==============================] - 1s 629us/step - loss: 0.6213 - acc: 0.7935 - val_loss: 1.5661 - val_acc: 0.4805\n",
      "Epoch 8/10\n",
      "1913/1913 [==============================] - 1s 613us/step - loss: 0.5482 - acc: 0.8233 - val_loss: 1.6176 - val_acc: 0.4915\n",
      "Epoch 9/10\n",
      "1913/1913 [==============================] - 1s 614us/step - loss: 0.4934 - acc: 0.8364 - val_loss: 1.6815 - val_acc: 0.4878\n",
      "Epoch 10/10\n",
      "1913/1913 [==============================] - 1s 597us/step - loss: 0.4383 - acc: 0.8500 - val_loss: 1.7240 - val_acc: 0.4780\n",
      "[[ 27   0   0   3   0   0   1   1   0   1]\n",
      " [  0  70   1   2   3   0   0   0   0   3]\n",
      " [  0   6   6   5   1   0   0   0   0   0]\n",
      " [  0   0   0 794   8   0   0   1   0  38]\n",
      " [  0   0   0  20 238   0   0   1   0  32]\n",
      " [  0   0   0   2   1  13   1   1   0   1]\n",
      " [  0   0   0   4   1   0  73   4   0   6]\n",
      " [  1   0   0   4   3   0   4 129   0   3]\n",
      " [  0   0   0   4   1   0   1   0  38   1]\n",
      " [  0   0   0  55  12   0   1   2   0 285]]\n",
      "1913/1913 [==============================] - 0s 195us/step\n",
      "[0.381569836115376, 0.8745426028982483]\n",
      "[[  1   0   0   4   0   0   2   1   1   1]\n",
      " [  0  24   2  15   7   0   0   0   0   0]\n",
      " [  0   2   1   2   0   0   0   0   0   1]\n",
      " [  0   4   0 251  42   0   3   4   0  56]\n",
      " [  0   2   1  52  16   0   3   4   1  25]\n",
      " [  1   0   0   5   1   0   1   2   0   4]\n",
      " [  1   0   0   9   1   0   9  10   0   2]\n",
      " [  0   1   0   9   7   0   5  34   1   5]\n",
      " [  0   0   0  13   3   0   1   0   1   3]\n",
      " [  1   5   0  73  21   0   5   3   0  55]]\n",
      "820/820 [==============================] - 0s 229us/step\n",
      "[1.7239825958158912, 0.47804878063318207]\n"
     ]
    }
   ],
   "source": [
    "#First Party Collection/Use-Action First-Party\n",
    "\n",
    "padded_docs = pad_sequences(First_Party_Collection_Use_Action_First_Party.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(First_Party_Collection_Use_Action_First_Party.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, First_Party_Collection_Use_Action_First_Party.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "\n",
    "filename = 'first_party_collection_use_action_first_party_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('first_party_collection_use_action_first_party_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 369 samples, validate on 159 samples\n",
      "Epoch 1/10\n",
      "369/369 [==============================] - 1s 3ms/step - loss: 1.2383 - acc: 0.5122 - val_loss: 0.8956 - val_acc: 0.6730\n",
      "Epoch 2/10\n",
      "369/369 [==============================] - 1s 1ms/step - loss: 0.8233 - acc: 0.6558 - val_loss: 0.8293 - val_acc: 0.6667\n",
      "Epoch 3/10\n",
      "369/369 [==============================] - 0s 944us/step - loss: 0.6644 - acc: 0.7615 - val_loss: 0.8427 - val_acc: 0.6918\n",
      "Epoch 4/10\n",
      "369/369 [==============================] - 0s 651us/step - loss: 0.5396 - acc: 0.8374 - val_loss: 0.8417 - val_acc: 0.7107\n",
      "Epoch 5/10\n",
      "369/369 [==============================] - 0s 654us/step - loss: 0.4484 - acc: 0.8482 - val_loss: 0.8613 - val_acc: 0.7296\n",
      "Epoch 6/10\n",
      "369/369 [==============================] - 0s 680us/step - loss: 0.3837 - acc: 0.8591 - val_loss: 0.9076 - val_acc: 0.7107\n",
      "Epoch 7/10\n",
      "369/369 [==============================] - 0s 680us/step - loss: 0.3255 - acc: 0.8916 - val_loss: 0.9345 - val_acc: 0.7044\n",
      "Epoch 8/10\n",
      "369/369 [==============================] - 0s 635us/step - loss: 0.2818 - acc: 0.9051 - val_loss: 0.9662 - val_acc: 0.6918\n",
      "Epoch 9/10\n",
      "369/369 [==============================] - 0s 701us/step - loss: 0.2447 - acc: 0.9214 - val_loss: 0.9830 - val_acc: 0.6918\n",
      "Epoch 10/10\n",
      "369/369 [==============================] - 1s 2ms/step - loss: 0.2138 - acc: 0.9241 - val_loss: 1.0377 - val_acc: 0.6855\n",
      "[[ 57   1   0   2]\n",
      " [  0 202   0   5]\n",
      " [  0   3  11   1]\n",
      " [  2  11   0  74]]\n",
      "369/369 [==============================] - 0s 188us/step\n",
      "[0.1905409711486279, 0.9322493229778155]\n",
      "[[ 7  7  0  1]\n",
      " [ 8 85  0 12]\n",
      " [ 1  4  0  0]\n",
      " [ 7 10  0 17]]\n",
      "159/159 [==============================] - 0s 229us/step\n",
      "[1.0377408003657118, 0.6855345930693284]\n"
     ]
    }
   ],
   "source": [
    "#First Party Collection/Use-Choice Scope\n",
    "\n",
    "padded_docs = pad_sequences(First_Party_Collection_Use_Choice_Scope.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(First_Party_Collection_Use_Choice_Scope.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, First_Party_Collection_Use_Choice_Scope.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'first_party_collection_use_choice_scope_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('first_party_collection_use_choice_scope_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 655 samples, validate on 282 samples\n",
      "Epoch 1/10\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 1.9262 - acc: 0.3435 - val_loss: 1.6911 - val_acc: 0.4326\n",
      "Epoch 2/10\n",
      "655/655 [==============================] - 0s 641us/step - loss: 1.2666 - acc: 0.5634 - val_loss: 1.5663 - val_acc: 0.4787\n",
      "Epoch 3/10\n",
      "655/655 [==============================] - 0s 635us/step - loss: 1.0017 - acc: 0.6870 - val_loss: 1.4669 - val_acc: 0.5284\n",
      "Epoch 4/10\n",
      "655/655 [==============================] - 0s 677us/step - loss: 0.8154 - acc: 0.7603 - val_loss: 1.4394 - val_acc: 0.5284\n",
      "Epoch 5/10\n",
      "655/655 [==============================] - 0s 608us/step - loss: 0.6673 - acc: 0.8015 - val_loss: 1.4397 - val_acc: 0.5177\n",
      "Epoch 6/10\n",
      "655/655 [==============================] - 0s 635us/step - loss: 0.5621 - acc: 0.8321 - val_loss: 1.4553 - val_acc: 0.5142\n",
      "Epoch 7/10\n",
      "655/655 [==============================] - 0s 699us/step - loss: 0.4763 - acc: 0.8504 - val_loss: 1.4727 - val_acc: 0.5319\n",
      "Epoch 8/10\n",
      "655/655 [==============================] - 0s 636us/step - loss: 0.4068 - acc: 0.8794 - val_loss: 1.5269 - val_acc: 0.5177\n",
      "Epoch 9/10\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.3711 - acc: 0.8733 - val_loss: 1.5431 - val_acc: 0.5319\n",
      "Epoch 10/10\n",
      "655/655 [==============================] - 0s 752us/step - loss: 0.3197 - acc: 0.8992 - val_loss: 1.6039 - val_acc: 0.5284\n",
      "[[ 83   0   0   0   0   0   1   0   0]\n",
      " [  0 132   0  19   0   0   1   0   0]\n",
      " [  0   0  16   0   0   0   0   0   0]\n",
      " [  0   7   0 235   0   0   1   0   0]\n",
      " [  0   0   1   2  34   0   1   0   0]\n",
      " [  0   0   0   3   1   9   0   0   0]\n",
      " [  1   4   0   3   0   0  49   0   0]\n",
      " [  1   0   0   0   0   0   0  21   0]\n",
      " [  0   2   1   5   3   0   2   0  17]]\n",
      "655/655 [==============================] - 0s 206us/step\n",
      "[0.28926386528342735, 0.9099236642131369]\n",
      "[[32  1  1  3  0  0  0  0  0]\n",
      " [ 0 22  0 32  0  0  3  0  2]\n",
      " [ 2  2  1  1  1  0  0  1  1]\n",
      " [ 2 17  0 75  0  0  3  0  3]\n",
      " [ 1  0  0  2  9  1  2  0  0]\n",
      " [ 0  0  0  2  1  1  3  0  0]\n",
      " [ 2  4  0  9  1  0  7  0  0]\n",
      " [ 6  3  2  1  2  0  1  1  0]\n",
      " [ 0  3  0  6  1  0  5  0  1]]\n",
      "282/282 [==============================] - 0s 209us/step\n",
      "[1.6039394402334877, 0.5283687939035132]\n"
     ]
    }
   ],
   "source": [
    "#First_Party_Collection_Use_Choice_Type\n",
    "\n",
    "padded_docs = pad_sequences(First_Party_Collection_Use_Choice_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(First_Party_Collection_Use_Choice_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, First_Party_Collection_Use_Choice_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'first_party_collection_use_choice_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('first_party_collection_use_choice_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1069 samples, validate on 459 samples\n",
      "Epoch 1/10\n",
      "1069/1069 [==============================] - 1s 1ms/step - loss: 0.7702 - acc: 0.6848 - val_loss: 0.5659 - val_acc: 0.7865\n",
      "Epoch 2/10\n",
      "1069/1069 [==============================] - 1s 624us/step - loss: 0.4557 - acc: 0.8260 - val_loss: 0.5255 - val_acc: 0.8235\n",
      "Epoch 3/10\n",
      "1069/1069 [==============================] - 1s 614us/step - loss: 0.3515 - acc: 0.8765 - val_loss: 0.5229 - val_acc: 0.8279\n",
      "Epoch 4/10\n",
      "1069/1069 [==============================] - 1s 621us/step - loss: 0.2890 - acc: 0.8943 - val_loss: 0.5432 - val_acc: 0.8214\n",
      "Epoch 5/10\n",
      "1069/1069 [==============================] - 1s 630us/step - loss: 0.2440 - acc: 0.9130 - val_loss: 0.5616 - val_acc: 0.8322\n",
      "Epoch 6/10\n",
      "1069/1069 [==============================] - 1s 623us/step - loss: 0.2105 - acc: 0.9308 - val_loss: 0.5972 - val_acc: 0.8170\n",
      "Epoch 7/10\n",
      "1069/1069 [==============================] - 1s 609us/step - loss: 0.1803 - acc: 0.9336 - val_loss: 0.6469 - val_acc: 0.8039\n",
      "Epoch 8/10\n",
      "1069/1069 [==============================] - 1s 619us/step - loss: 0.1570 - acc: 0.9439 - val_loss: 0.6537 - val_acc: 0.8322\n",
      "Epoch 9/10\n",
      "1069/1069 [==============================] - 1s 609us/step - loss: 0.1394 - acc: 0.9514 - val_loss: 0.6737 - val_acc: 0.8105\n",
      "Epoch 10/10\n",
      "1069/1069 [==============================] - 1s 620us/step - loss: 0.1185 - acc: 0.9616 - val_loss: 0.7409 - val_acc: 0.8061\n",
      "[[487   4   1]\n",
      " [  9 448  12]\n",
      " [  5   3 100]]\n",
      "1069/1069 [==============================] - 0s 193us/step\n",
      "[0.10538413554736674, 0.9681945743685687]\n",
      "[[218  13   8]\n",
      " [ 31 145  15]\n",
      " [ 11  11   7]]\n",
      "459/459 [==============================] - 0s 212us/step\n",
      "[0.7409113641657861, 0.8061002177350661]\n"
     ]
    }
   ],
   "source": [
    "#First Party Collection/Use-Collection Mode\n",
    "\n",
    "padded_docs = pad_sequences(First_Party_Collection_Use_Collection_Mode.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(First_Party_Collection_Use_Collection_Mode.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, First_Party_Collection_Use_Collection_Mode.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'first_party_collection_collection_mode_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('first_party_collection_collection_mode_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 915 samples, validate on 393 samples\n",
      "Epoch 1/10\n",
      "915/915 [==============================] - 1s 1ms/step - loss: 0.3279 - acc: 0.8831 - val_loss: 0.2354 - val_acc: 0.9160\n",
      "Epoch 2/10\n",
      "915/915 [==============================] - 1s 639us/step - loss: 0.1381 - acc: 0.9563 - val_loss: 0.1847 - val_acc: 0.9491\n",
      "Epoch 3/10\n",
      "915/915 [==============================] - 1s 619us/step - loss: 0.0776 - acc: 0.9738 - val_loss: 0.1707 - val_acc: 0.9364\n",
      "Epoch 4/10\n",
      "915/915 [==============================] - 1s 614us/step - loss: 0.0623 - acc: 0.9792 - val_loss: 0.1938 - val_acc: 0.9440\n",
      "Epoch 5/10\n",
      "915/915 [==============================] - 1s 641us/step - loss: 0.0488 - acc: 0.9869 - val_loss: 0.1923 - val_acc: 0.9389\n",
      "Epoch 6/10\n",
      "915/915 [==============================] - 1s 617us/step - loss: 0.0435 - acc: 0.9858 - val_loss: 0.2058 - val_acc: 0.9440\n",
      "Epoch 7/10\n",
      "915/915 [==============================] - 1s 647us/step - loss: 0.0339 - acc: 0.9902 - val_loss: 0.2103 - val_acc: 0.9466\n",
      "Epoch 8/10\n",
      "915/915 [==============================] - 1s 672us/step - loss: 0.0343 - acc: 0.9902 - val_loss: 0.2107 - val_acc: 0.9415\n",
      "Epoch 9/10\n",
      "915/915 [==============================] - 1s 641us/step - loss: 0.0264 - acc: 0.9913 - val_loss: 0.2154 - val_acc: 0.9415\n",
      "Epoch 10/10\n",
      "915/915 [==============================] - 1s 644us/step - loss: 0.0246 - acc: 0.9913 - val_loss: 0.2294 - val_acc: 0.9389\n",
      "[[839   0]\n",
      " [  7  69]]\n",
      "915/915 [==============================] - 0s 208us/step\n",
      "[0.01997773118337484, 0.9923497267759562]\n",
      "[[342   9]\n",
      " [ 15  27]]\n",
      "393/393 [==============================] - 0s 243us/step\n",
      "[0.22944376698005517, 0.9389312977099237]\n"
     ]
    }
   ],
   "source": [
    "#First Party Collection/Use-Does/Does Not\n",
    "\n",
    "padded_docs = pad_sequences(First_Party_Collection_Use_Does_Does_Not.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(First_Party_Collection_Use_Does_Does_Not.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, First_Party_Collection_Use_Does_Does_Not.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'first_party_collection_use_does_does_not_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('first_party_collection_use_does_does_not_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 445 samples, validate on 191 samples\n",
      "Epoch 1/10\n",
      "445/445 [==============================] - 1s 3ms/step - loss: 1.2515 - acc: 0.4022 - val_loss: 1.1483 - val_acc: 0.5812\n",
      "Epoch 2/10\n",
      "445/445 [==============================] - 0s 623us/step - loss: 0.8783 - acc: 0.6787 - val_loss: 1.0599 - val_acc: 0.6073\n",
      "Epoch 3/10\n",
      "445/445 [==============================] - 0s 720us/step - loss: 0.7039 - acc: 0.7596 - val_loss: 1.0262 - val_acc: 0.6126\n",
      "Epoch 4/10\n",
      "445/445 [==============================] - 0s 669us/step - loss: 0.5959 - acc: 0.7888 - val_loss: 1.0512 - val_acc: 0.6335\n",
      "Epoch 5/10\n",
      "445/445 [==============================] - 0s 650us/step - loss: 0.5210 - acc: 0.8112 - val_loss: 1.0652 - val_acc: 0.6440\n",
      "Epoch 6/10\n",
      "445/445 [==============================] - 0s 652us/step - loss: 0.4617 - acc: 0.8337 - val_loss: 1.0882 - val_acc: 0.6649\n",
      "Epoch 7/10\n",
      "445/445 [==============================] - 0s 659us/step - loss: 0.4197 - acc: 0.8607 - val_loss: 1.1423 - val_acc: 0.6492\n",
      "Epoch 8/10\n",
      "445/445 [==============================] - 0s 660us/step - loss: 0.3763 - acc: 0.8674 - val_loss: 1.1919 - val_acc: 0.6230\n",
      "Epoch 9/10\n",
      "445/445 [==============================] - 0s 676us/step - loss: 0.3468 - acc: 0.8787 - val_loss: 1.1879 - val_acc: 0.6335\n",
      "Epoch 10/10\n",
      "445/445 [==============================] - 0s 661us/step - loss: 0.3245 - acc: 0.8899 - val_loss: 1.2169 - val_acc: 0.6387\n",
      "[[136   3   3   0]\n",
      " [  4 202   0   2]\n",
      " [  9   4  25   2]\n",
      " [  7  10   1  37]]\n",
      "445/445 [==============================] - 0s 223us/step\n",
      "[0.29842585298452484, 0.8988764059677553]\n",
      "[[46 11  4  0]\n",
      " [ 7 69  4  2]\n",
      " [10 11  1  0]\n",
      " [ 4 16  0  6]]\n",
      "191/191 [==============================] - 0s 212us/step\n",
      "[1.2168732407205392, 0.6387434576818456]\n"
     ]
    }
   ],
   "source": [
    "#First Party Collection/Use-Identifiability\n",
    "\n",
    "padded_docs = pad_sequences(First_Party_Collection_Use_Identifiability.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(First_Party_Collection_Use_Identifiability.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, First_Party_Collection_Use_Identifiability.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'first_party_collection_use_identifiability_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('first_party_collection_use_identifiability_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2183 samples, validate on 936 samples\n",
      "Epoch 1/10\n",
      "2183/2183 [==============================] - 2s 1ms/step - loss: 2.1265 - acc: 0.3940 - val_loss: 1.7757 - val_acc: 0.4872\n",
      "Epoch 2/10\n",
      "2183/2183 [==============================] - 1s 601us/step - loss: 1.3693 - acc: 0.6207 - val_loss: 1.5067 - val_acc: 0.5556\n",
      "Epoch 3/10\n",
      "2183/2183 [==============================] - 1s 613us/step - loss: 1.0579 - acc: 0.7155 - val_loss: 1.4338 - val_acc: 0.5812\n",
      "Epoch 4/10\n",
      "2183/2183 [==============================] - 1s 604us/step - loss: 0.8737 - acc: 0.7696 - val_loss: 1.3964 - val_acc: 0.5897\n",
      "Epoch 5/10\n",
      "2183/2183 [==============================] - 2s 835us/step - loss: 0.7461 - acc: 0.8131 - val_loss: 1.3866 - val_acc: 0.5876\n",
      "Epoch 6/10\n",
      "2183/2183 [==============================] - 2s 1ms/step - loss: 0.6543 - acc: 0.8291 - val_loss: 1.3991 - val_acc: 0.5940\n",
      "Epoch 7/10\n",
      "2183/2183 [==============================] - 3s 1ms/step - loss: 0.5777 - acc: 0.8484 - val_loss: 1.3947 - val_acc: 0.5919\n",
      "Epoch 8/10\n",
      "2183/2183 [==============================] - 1s 639us/step - loss: 0.5161 - acc: 0.8694 - val_loss: 1.4177 - val_acc: 0.5887\n",
      "Epoch 9/10\n",
      "2183/2183 [==============================] - 1s 626us/step - loss: 0.4654 - acc: 0.8777 - val_loss: 1.4331 - val_acc: 0.5897\n",
      "Epoch 10/10\n",
      "2183/2183 [==============================] - 2s 708us/step - loss: 0.4228 - acc: 0.8873 - val_loss: 1.4575 - val_acc: 0.6015\n",
      "[[173   0   1   0   0   2   0   2   0   2   0   0   0   2   1   0]\n",
      " [  0 321   0   0   0   1   0   0   0   0   0   0   0   1   0   1]\n",
      " [  0   0 233   0   0   2   0   2   0   0   0   0   0   2   0   0]\n",
      " [  0   0   1  79   0   1   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   1  83   1   0   0   0   2   0   0   0   4   1   0]\n",
      " [  0   5   0   1   0  90   0   0   0   1   0   0   0  15   2   0]\n",
      " [  0   0   1   0   1   2   5   0   2   0   0   0   0   3   1   0]\n",
      " [  1   0   2   0   0   2   0 135   0   1   0   0   0   0   0   0]\n",
      " [  0   3   1   1   0   0   0   0  76   0   0   0   0   1   0   0]\n",
      " [  1   8   4   1   3   3   0   0   1  82   4   0   0  29  10   5]\n",
      " [  0   1   0   4   0   3   0   1   0   2  23   0   0   1   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  38   0   0   0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  25   1   2   1]\n",
      " [  0   0   1   0   2  13   0   0   0   1   0   0   1 167   1   1]\n",
      " [  1   1   1   1   0   2   0   0   0   0   0   0   0   7 361   0]\n",
      " [  0   3   1   0   0   2   0   0   0   5   0   0   0   3   1  79]]\n",
      "2183/2183 [==============================] - 0s 217us/step\n",
      "[0.38272265833258573, 0.902427851580394]\n",
      "[[ 59   0   2   0   0   0   0   6   3   0   0   0   0   0  11   0]\n",
      " [  0 114   0   0   3   1   0   1   0   0   0   0   0   4   1   2]\n",
      " [  3   0  84   0   0   1   0   4   0   1   1   0   0   0   3   0]\n",
      " [  1   1   2  22   3   1   0   0   2   3   0   0   0   4   2   1]\n",
      " [  0   1   0   2  37   1   0   0   1   2   0   1   0   2   0   2]\n",
      " [  0   2   1   0   0  20   0   0   0   1   1   0   0  17   1   1]\n",
      " [  0   0   1   0   0   1   0   0   0   0   0   0   0   2   0   0]\n",
      " [  8   0   3   0   0   0   0  47   0   0   0   0   0   1   3   0]\n",
      " [  4   8   2   2   0   1   0   3  19   1   0   0   0   7   3   0]\n",
      " [  2   9   4   6   1   8   0   2   2   6   2   0   1  15  10   4]\n",
      " [  0   1   1   0   0   2   0   0   0   1   3   0   0   0   1   0]\n",
      " [  1   0   0   0   0   1   0   0   0   1   1   4   0   3   9   1]\n",
      " [  0   0   1   0   0   0   0   0   0   0   0   0   4   2   4   2]\n",
      " [  1   2   6   1   3  18   0   0   0   4   1   2   2  37   9   1]\n",
      " [  8   3   6   1   0   1   0   2   0   7   0   1   0   6  95   6]\n",
      " [  0   2   1   1   2   5   0   0   3   4   0   1   0   3   9  12]]\n",
      "936/936 [==============================] - 0s 281us/step\n",
      "[1.4575089369064722, 0.6014957264957265]\n"
     ]
    }
   ],
   "source": [
    "#First Party Collection/Use-Personal Information Type\n",
    "\n",
    "padded_docs = pad_sequences(First_Party_Collection_Use_Personal_Information_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(First_Party_Collection_Use_Personal_Information_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, First_Party_Collection_Use_Personal_Information_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'first_party_collection_use_personal_information_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('first_party_collection_use_personal_information_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2949 samples, validate on 1265 samples\n",
      "Epoch 1/10\n",
      "2949/2949 [==============================] - 3s 1ms/step - loss: 1.8390 - acc: 0.3645 - val_loss: 1.6069 - val_acc: 0.4379\n",
      "Epoch 2/10\n",
      "2949/2949 [==============================] - 2s 620us/step - loss: 1.2093 - acc: 0.6260 - val_loss: 1.4659 - val_acc: 0.5036\n",
      "Epoch 3/10\n",
      "2949/2949 [==============================] - 2s 607us/step - loss: 0.9417 - acc: 0.7043 - val_loss: 1.4685 - val_acc: 0.5059\n",
      "Epoch 4/10\n",
      "2949/2949 [==============================] - 2s 627us/step - loss: 0.7696 - acc: 0.7586 - val_loss: 1.4934 - val_acc: 0.5154\n",
      "Epoch 5/10\n",
      "2949/2949 [==============================] - 2s 626us/step - loss: 0.6386 - acc: 0.8159 - val_loss: 1.5416 - val_acc: 0.4980\n",
      "Epoch 6/10\n",
      "2949/2949 [==============================] - 2s 632us/step - loss: 0.5399 - acc: 0.8366 - val_loss: 1.5799 - val_acc: 0.5091\n",
      "Epoch 7/10\n",
      "2949/2949 [==============================] - 2s 622us/step - loss: 0.4612 - acc: 0.8623 - val_loss: 1.6546 - val_acc: 0.5004\n",
      "Epoch 8/10\n",
      "2949/2949 [==============================] - 2s 627us/step - loss: 0.3920 - acc: 0.8884 - val_loss: 1.6813 - val_acc: 0.5115\n",
      "Epoch 9/10\n",
      "2949/2949 [==============================] - 2s 614us/step - loss: 0.3468 - acc: 0.8969 - val_loss: 1.7606 - val_acc: 0.4996\n",
      "Epoch 10/10\n",
      "2949/2949 [==============================] - 2s 635us/step - loss: 0.3032 - acc: 0.9105 - val_loss: 1.7947 - val_acc: 0.4972\n",
      "[[392   0   2  21   0   4   0   1   5   2   1]\n",
      " [  4 202   3   0   0   2   0   0   2   0   0]\n",
      " [  1   0 476   3   0   2   0   2   4   1   2]\n",
      " [  8   0   1 522   0   7   0   3   2   5   2]\n",
      " [  0   0   0   0  45   0   0   0   0   4   0]\n",
      " [  7   0   1   1   0 286   0   0   0   1   2]\n",
      " [  0   0   0   2   0   0   4   0   0   0   0]\n",
      " [  3   0   4   2   0   4   0 111   2   3  10]\n",
      " [  3   2   3   5   0   0   0   0 278   0   0]\n",
      " [  2   0   1  15   0   0   0   2   1 247   2]\n",
      " [  1   1   4   5   2   1   0   3   0   1 196]]\n",
      "2949/2949 [==============================] - 1s 302us/step\n",
      "[0.2354943929950679, 0.9355713801288572]\n",
      "[[ 81   0   4  47   0  25   0   6   6   5   5]\n",
      " [  7  66   7   0   0   7   0   1   3   1   2]\n",
      " [  7   7 135  13   0   5   0   2  16  19   5]\n",
      " [ 56   2  11 108   0  14   0  11  16  13  10]\n",
      " [  2   0   0   1  14   1   0   1   0   5   1]\n",
      " [ 21   0   9  16   0  63   0   1   2   0   1]\n",
      " [  1   0   0   0   0   0   0   1   0   0   0]\n",
      " [  2   6   5   9   1   3   0   6   7   5  13]\n",
      " [ 14   5  20  19   0   2   0   3  76   9   0]\n",
      " [  8   1  14  20   2   1   0   3   3  46   4]\n",
      " [  4   2  10  20   1   1   0  12   4   7  34]]\n",
      "1265/1265 [==============================] - 0s 243us/step\n",
      "[1.7946625965857224, 0.49723320186373743]\n"
     ]
    }
   ],
   "source": [
    "#First Party Collection/Use-Purpose\n",
    "\n",
    "padded_docs = pad_sequences(First_Party_Collection_Use_Purpose.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(First_Party_Collection_Use_Purpose.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, First_Party_Collection_Use_Purpose.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'first_party_collection_use_purpose_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('first_party_collection_use_purpose.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 103 samples\n",
      "Epoch 1/10\n",
      "240/240 [==============================] - 1s 5ms/step - loss: 1.3000 - acc: 0.4708 - val_loss: 1.1053 - val_acc: 0.5631\n",
      "Epoch 2/10\n",
      "240/240 [==============================] - 0s 658us/step - loss: 0.9551 - acc: 0.5917 - val_loss: 1.0049 - val_acc: 0.6117\n",
      "Epoch 3/10\n",
      "240/240 [==============================] - 0s 789us/step - loss: 0.7379 - acc: 0.7583 - val_loss: 1.0158 - val_acc: 0.6117\n",
      "Epoch 4/10\n",
      "240/240 [==============================] - 0s 770us/step - loss: 0.6446 - acc: 0.7917 - val_loss: 1.0119 - val_acc: 0.6311\n",
      "Epoch 5/10\n",
      "240/240 [==============================] - 0s 797us/step - loss: 0.5493 - acc: 0.8125 - val_loss: 1.0418 - val_acc: 0.6408\n",
      "Epoch 6/10\n",
      "240/240 [==============================] - 0s 780us/step - loss: 0.4904 - acc: 0.8208 - val_loss: 1.0747 - val_acc: 0.6408\n",
      "Epoch 7/10\n",
      "240/240 [==============================] - 0s 794us/step - loss: 0.4312 - acc: 0.8625 - val_loss: 1.1045 - val_acc: 0.6117\n",
      "Epoch 8/10\n",
      "240/240 [==============================] - 0s 682us/step - loss: 0.3863 - acc: 0.8792 - val_loss: 1.1336 - val_acc: 0.6214\n",
      "Epoch 9/10\n",
      "240/240 [==============================] - 0s 724us/step - loss: 0.3495 - acc: 0.8917 - val_loss: 1.1659 - val_acc: 0.6117\n",
      "Epoch 10/10\n",
      "240/240 [==============================] - 0s 721us/step - loss: 0.3186 - acc: 0.9042 - val_loss: 1.1930 - val_acc: 0.6214\n",
      "[[ 42   0   2   4]\n",
      " [  2  31   2   3]\n",
      " [  0   2 120   0]\n",
      " [  0   2   6  24]]\n",
      "240/240 [==============================] - 0s 212us/step\n",
      "[0.29294341802597046, 0.9041666666666667]\n",
      "[[13  3 11  2]\n",
      " [ 1  5  4  1]\n",
      " [ 3  2 46  1]\n",
      " [ 4  2  5  0]]\n",
      "103/103 [==============================] - 0s 219us/step\n",
      "[1.193022608757019, 0.6213592235903138]\n"
     ]
    }
   ],
   "source": [
    "#First Party Collection/Use-User Type\n",
    "\n",
    "padded_docs = pad_sequences(First_Party_Collection_Use_User_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(First_Party_Collection_Use_User_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, First_Party_Collection_Use_User_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'first_party_collection_use_user_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('first_party_collection_use_user_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 407 samples, validate on 175 samples\n",
      "Epoch 1/10\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 1.2669 - acc: 0.4816 - val_loss: 0.9065 - val_acc: 0.6914\n",
      "Epoch 2/10\n",
      "407/407 [==============================] - 0s 693us/step - loss: 0.5599 - acc: 0.8280 - val_loss: 0.8256 - val_acc: 0.7143\n",
      "Epoch 3/10\n",
      "407/407 [==============================] - 0s 658us/step - loss: 0.3919 - acc: 0.8821 - val_loss: 0.7980 - val_acc: 0.7314\n",
      "Epoch 4/10\n",
      "407/407 [==============================] - 0s 652us/step - loss: 0.2980 - acc: 0.9091 - val_loss: 0.7933 - val_acc: 0.7486\n",
      "Epoch 5/10\n",
      "407/407 [==============================] - 0s 710us/step - loss: 0.2348 - acc: 0.9165 - val_loss: 0.8032 - val_acc: 0.7486\n",
      "Epoch 6/10\n",
      "407/407 [==============================] - 0s 699us/step - loss: 0.2010 - acc: 0.9312 - val_loss: 0.8307 - val_acc: 0.7600\n",
      "Epoch 7/10\n",
      "407/407 [==============================] - 0s 685us/step - loss: 0.1782 - acc: 0.9459 - val_loss: 0.8602 - val_acc: 0.7429\n",
      "Epoch 8/10\n",
      "407/407 [==============================] - 0s 652us/step - loss: 0.1532 - acc: 0.9607 - val_loss: 0.8994 - val_acc: 0.7486\n",
      "Epoch 9/10\n",
      "407/407 [==============================] - 0s 691us/step - loss: 0.1395 - acc: 0.9459 - val_loss: 0.9061 - val_acc: 0.7600\n",
      "Epoch 10/10\n",
      "407/407 [==============================] - 0s 679us/step - loss: 0.1288 - acc: 0.9459 - val_loss: 0.9070 - val_acc: 0.7543\n",
      "[[ 90   2   1   0   0]\n",
      " [  1 194   0   0   1]\n",
      " [  0   0  59   7   1]\n",
      " [  0   0   0  30   1]\n",
      " [  0   0   4   0  16]]\n",
      "407/407 [==============================] - 0s 207us/step\n",
      "[0.11665250822279319, 0.9557739557739557]\n",
      "[[26  6  0  0  1]\n",
      " [ 2 89  4  0  0]\n",
      " [ 0  9 10  5  1]\n",
      " [ 0  2  3  6  0]\n",
      " [ 2  7  1  0  1]]\n",
      "175/175 [==============================] - 0s 198us/step\n",
      "[0.9070384499004909, 0.7542857149669102]\n"
     ]
    }
   ],
   "source": [
    "#International and Specific Audiences-Audience Type\n",
    "\n",
    "padded_docs = pad_sequences(International_And_Specific_Audiences_Audience_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(International_And_Specific_Audiences_Audience_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, International_And_Specific_Audiences_Audience_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'international_specific_audiences_audience_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('international_specific_audiences_audience_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1931 samples, validate on 828 samples\n",
      "Epoch 1/10\n",
      "1931/1931 [==============================] - 3s 1ms/step - loss: 1.1719 - acc: 0.4619 - val_loss: 1.0627 - val_acc: 0.5181\n",
      "Epoch 2/10\n",
      "1931/1931 [==============================] - 2s 1ms/step - loss: 0.7032 - acc: 0.7276 - val_loss: 1.1582 - val_acc: 0.5169\n",
      "Epoch 3/10\n",
      "1931/1931 [==============================] - 1s 651us/step - loss: 0.5079 - acc: 0.8058 - val_loss: 1.2718 - val_acc: 0.5145\n",
      "Epoch 4/10\n",
      "1931/1931 [==============================] - 1s 636us/step - loss: 0.3776 - acc: 0.8483 - val_loss: 1.4151 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1931/1931 [==============================] - 1s 637us/step - loss: 0.3108 - acc: 0.8690 - val_loss: 1.5159 - val_acc: 0.5097\n",
      "Epoch 6/10\n",
      "1931/1931 [==============================] - 1s 650us/step - loss: 0.2664 - acc: 0.8757 - val_loss: 1.6457 - val_acc: 0.4952\n",
      "Epoch 7/10\n",
      "1931/1931 [==============================] - 1s 651us/step - loss: 0.2441 - acc: 0.8804 - val_loss: 1.7102 - val_acc: 0.5024\n",
      "Epoch 8/10\n",
      "1931/1931 [==============================] - 1s 669us/step - loss: 0.2269 - acc: 0.8762 - val_loss: 1.7637 - val_acc: 0.4928\n",
      "Epoch 9/10\n",
      "1931/1931 [==============================] - 1s 655us/step - loss: 0.2052 - acc: 0.8892 - val_loss: 1.8637 - val_acc: 0.5048\n",
      "Epoch 10/10\n",
      "1931/1931 [==============================] - 1s 662us/step - loss: 0.1998 - acc: 0.8876 - val_loss: 1.8825 - val_acc: 0.4988\n",
      "[[636  28  20   0]\n",
      " [ 38 368  14  11]\n",
      " [ 20  32 453   1]\n",
      " [  4   4   0 302]]\n",
      "1931/1931 [==============================] - 0s 220us/step\n",
      "[0.16463272073289262, 0.9109269808389435]\n",
      "[[163  58  54   9]\n",
      " [ 64  56  59  19]\n",
      " [ 68  55  80   3]\n",
      " [ 11  11   4 114]]\n",
      "828/828 [==============================] - 0s 256us/step\n",
      "[1.8825280021354196, 0.49879227009948324]\n"
     ]
    }
   ],
   "source": [
    "#Other-Other Type\n",
    "\n",
    "padded_docs = pad_sequences(Other_Other_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Other_Other_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Other_Other_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'other_other_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('other_other_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 238 samples, validate on 103 samples\n",
      "Epoch 1/10\n",
      "238/238 [==============================] - 1s 5ms/step - loss: 1.3865 - acc: 0.4412 - val_loss: 1.2409 - val_acc: 0.5825\n",
      "Epoch 2/10\n",
      "238/238 [==============================] - 0s 718us/step - loss: 0.8489 - acc: 0.7479 - val_loss: 1.2775 - val_acc: 0.5922\n",
      "Epoch 3/10\n",
      "238/238 [==============================] - 0s 797us/step - loss: 0.6908 - acc: 0.7731 - val_loss: 1.3344 - val_acc: 0.5631\n",
      "Epoch 4/10\n",
      "238/238 [==============================] - 0s 702us/step - loss: 0.5685 - acc: 0.7899 - val_loss: 1.3316 - val_acc: 0.6311\n",
      "Epoch 5/10\n",
      "238/238 [==============================] - 0s 730us/step - loss: 0.4868 - acc: 0.8193 - val_loss: 1.3552 - val_acc: 0.5825\n",
      "Epoch 6/10\n",
      "238/238 [==============================] - 0s 709us/step - loss: 0.4300 - acc: 0.8529 - val_loss: 1.3964 - val_acc: 0.5728\n",
      "Epoch 7/10\n",
      "238/238 [==============================] - 0s 727us/step - loss: 0.3917 - acc: 0.8613 - val_loss: 1.4500 - val_acc: 0.5825\n",
      "Epoch 8/10\n",
      "238/238 [==============================] - 0s 753us/step - loss: 0.3507 - acc: 0.8782 - val_loss: 1.5156 - val_acc: 0.6019\n",
      "Epoch 9/10\n",
      "238/238 [==============================] - 0s 725us/step - loss: 0.3171 - acc: 0.8824 - val_loss: 1.5679 - val_acc: 0.5922\n",
      "Epoch 10/10\n",
      "238/238 [==============================] - 0s 743us/step - loss: 0.2854 - acc: 0.8782 - val_loss: 1.6205 - val_acc: 0.5728\n",
      "[[  5   0   0   0   2]\n",
      " [  0   3   0   2   6]\n",
      " [  0   1   5   0   2]\n",
      " [  0   0   1  78   7]\n",
      " [  0   0   1   3 122]]\n",
      "238/238 [==============================] - 0s 200us/step\n",
      "[0.2657628252225764, 0.8949579816906392]\n",
      "[[ 0  0  0  3  2]\n",
      " [ 0  0  0  2  6]\n",
      " [ 0  0  0  2  3]\n",
      " [ 0  0  0 26 16]\n",
      " [ 0  0  0 10 33]]\n",
      "103/103 [==============================] - 0s 299us/step\n",
      "[1.6205232421171318, 0.5728155348486114]\n"
     ]
    }
   ],
   "source": [
    "#Policy Change-Change Type\n",
    "\n",
    "padded_docs = pad_sequences(Policy_Change_Change_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Policy_Change_Change_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Policy_Change_Change_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'policy_change_change_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('policy_change_change_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 282 samples, validate on 121 samples\n",
      "Epoch 1/10\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 1.6957 - acc: 0.3191 - val_loss: 1.4651 - val_acc: 0.6033\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 0s 646us/step - loss: 1.2061 - acc: 0.6560 - val_loss: 1.2924 - val_acc: 0.6033\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 0s 655us/step - loss: 0.9022 - acc: 0.7376 - val_loss: 1.2435 - val_acc: 0.6116\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 0s 605us/step - loss: 0.7223 - acc: 0.8227 - val_loss: 1.2354 - val_acc: 0.6364\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 0s 653us/step - loss: 0.5849 - acc: 0.8546 - val_loss: 1.2277 - val_acc: 0.6116\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 0s 620us/step - loss: 0.4892 - acc: 0.8759 - val_loss: 1.2126 - val_acc: 0.6198\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 0s 679us/step - loss: 0.4091 - acc: 0.9043 - val_loss: 1.2293 - val_acc: 0.6281\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 0s 657us/step - loss: 0.3508 - acc: 0.9149 - val_loss: 1.2565 - val_acc: 0.6281\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 0s 613us/step - loss: 0.2992 - acc: 0.9149 - val_loss: 1.2936 - val_acc: 0.6198\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 0s 647us/step - loss: 0.2587 - acc: 0.9326 - val_loss: 1.3397 - val_acc: 0.6281\n",
      "[[93  2  0  0  0  0]\n",
      " [ 3 60  0  0  1  1]\n",
      " [ 0  0 16  0  0  0]\n",
      " [ 1  3  0 13  1  1]\n",
      " [ 0  0  0  0 51  0]\n",
      " [ 1  0  0  1  1 33]]\n",
      "282/282 [==============================] - 0s 192us/step\n",
      "[0.23268790194328795, 0.9432624088111499]\n",
      "[[35  4  2  1  4  1]\n",
      " [ 4 19  0  1  2  1]\n",
      " [ 1  1  0  0  0  0]\n",
      " [ 4  0  1  0  0  1]\n",
      " [ 4  0  0  0 19  1]\n",
      " [ 5  2  1  1  3  3]]\n",
      "121/121 [==============================] - 0s 263us/step\n",
      "[1.3396885099489826, 0.6280991794649234]\n"
     ]
    }
   ],
   "source": [
    "#Policy Change-Notification Type\n",
    "\n",
    "padded_docs = pad_sequences(Policy_Change_Notification_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Policy_Change_Notification_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Policy_Change_Notification_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'policy_change_notification_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('policy_change_notification_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 131 samples, validate on 57 samples\n",
      "Epoch 1/10\n",
      "131/131 [==============================] - 1s 10ms/step - loss: 1.7351 - acc: 0.2901 - val_loss: 1.5575 - val_acc: 0.4912\n",
      "Epoch 2/10\n",
      "131/131 [==============================] - 0s 783us/step - loss: 1.0692 - acc: 0.6641 - val_loss: 1.4839 - val_acc: 0.4737\n",
      "Epoch 3/10\n",
      "131/131 [==============================] - 0s 798us/step - loss: 0.7948 - acc: 0.7176 - val_loss: 1.4614 - val_acc: 0.4386\n",
      "Epoch 4/10\n",
      "131/131 [==============================] - 0s 912us/step - loss: 0.6320 - acc: 0.8092 - val_loss: 1.4794 - val_acc: 0.4561\n",
      "Epoch 5/10\n",
      "131/131 [==============================] - 0s 1ms/step - loss: 0.5173 - acc: 0.8397 - val_loss: 1.5139 - val_acc: 0.4561\n",
      "Epoch 6/10\n",
      "131/131 [==============================] - 0s 876us/step - loss: 0.4460 - acc: 0.8779 - val_loss: 1.5664 - val_acc: 0.4561\n",
      "Epoch 7/10\n",
      "131/131 [==============================] - 0s 839us/step - loss: 0.3954 - acc: 0.8931 - val_loss: 1.6499 - val_acc: 0.4561\n",
      "Epoch 8/10\n",
      "131/131 [==============================] - 0s 864us/step - loss: 0.3572 - acc: 0.9084 - val_loss: 1.7135 - val_acc: 0.4561\n",
      "Epoch 9/10\n",
      "131/131 [==============================] - 0s 823us/step - loss: 0.3293 - acc: 0.9084 - val_loss: 1.7476 - val_acc: 0.4386\n",
      "Epoch 10/10\n",
      "131/131 [==============================] - 0s 822us/step - loss: 0.2993 - acc: 0.9160 - val_loss: 1.7527 - val_acc: 0.4386\n",
      "[[28  0  0  0  3  2]\n",
      " [ 0 10  0  1  0  0]\n",
      " [ 0  0  3  0  0  0]\n",
      " [ 0  0  0  7  0  0]\n",
      " [ 0  0  1  0 53  0]\n",
      " [ 3  0  0  0  2 18]]\n",
      "131/131 [==============================] - 0s 212us/step\n",
      "[0.2749881280571905, 0.9083969465648855]\n",
      "[[ 5  0  0  0  3  7]\n",
      " [ 0  2  0  0  3  0]\n",
      " [ 1  1  0  0  2  0]\n",
      " [ 2  0  0  0  0  1]\n",
      " [ 4  1  0  0 16  0]\n",
      " [ 4  0  0  0  3  2]]\n",
      "57/57 [==============================] - 0s 243us/step\n",
      "[1.7526925885886477, 0.438596493842309]\n"
     ]
    }
   ],
   "source": [
    "#Policy Change-User Choice\n",
    "\n",
    "padded_docs = pad_sequences(Policy_Change_User_Choice.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Policy_Change_User_Choice.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Policy_Change_User_Choice.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'policy_change_user_choice_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('policy_change_user_choice_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1322 samples, validate on 567 samples\n",
      "Epoch 1/10\n",
      "1322/1322 [==============================] - 3s 2ms/step - loss: 1.3382 - acc: 0.5439 - val_loss: 1.1322 - val_acc: 0.6102\n",
      "Epoch 2/10\n",
      "1322/1322 [==============================] - 1s 618us/step - loss: 0.9356 - acc: 0.6921 - val_loss: 1.0902 - val_acc: 0.6261\n",
      "Epoch 3/10\n",
      "1322/1322 [==============================] - 1s 955us/step - loss: 0.7629 - acc: 0.7549 - val_loss: 1.0873 - val_acc: 0.6243\n",
      "Epoch 4/10\n",
      "1322/1322 [==============================] - 1s 746us/step - loss: 0.6418 - acc: 0.7920 - val_loss: 1.1227 - val_acc: 0.6243\n",
      "Epoch 5/10\n",
      "1322/1322 [==============================] - 1s 721us/step - loss: 0.5513 - acc: 0.8238 - val_loss: 1.1559 - val_acc: 0.6138\n",
      "Epoch 6/10\n",
      "1322/1322 [==============================] - 1s 1ms/step - loss: 0.4831 - acc: 0.8442 - val_loss: 1.1934 - val_acc: 0.6190\n",
      "Epoch 7/10\n",
      "1322/1322 [==============================] - 1s 695us/step - loss: 0.4352 - acc: 0.8676 - val_loss: 1.2659 - val_acc: 0.6049\n",
      "Epoch 8/10\n",
      "1322/1322 [==============================] - 1s 659us/step - loss: 0.3956 - acc: 0.8782 - val_loss: 1.3148 - val_acc: 0.5996\n",
      "Epoch 9/10\n",
      "1322/1322 [==============================] - 1s 669us/step - loss: 0.3585 - acc: 0.8843 - val_loss: 1.3560 - val_acc: 0.6032\n",
      "Epoch 10/10\n",
      "1322/1322 [==============================] - 1s 648us/step - loss: 0.3239 - acc: 0.8971 - val_loss: 1.3908 - val_acc: 0.5996\n",
      "[[152   1   4   0   6   0]\n",
      " [  0 103  27   0   1   1]\n",
      " [  4   8 686   1   2   1]\n",
      " [  2   2  19  59   0   0]\n",
      " [  5   1   6   0 156   0]\n",
      " [  1   3  24   1   3  43]]\n",
      "1322/1322 [==============================] - 0s 264us/step\n",
      "[0.29130658493821093, 0.9069591528889631]\n",
      "[[ 24   7  14   1  16   2]\n",
      " [  5   7  36   3   9   1]\n",
      " [  7  13 274   3   7   3]\n",
      " [  2   2  20   6   4   2]\n",
      " [ 22   9   6   0  27   1]\n",
      " [  4   2  18   1   7   2]]\n",
      "567/567 [==============================] - 0s 284us/step\n",
      "[1.390758944777164, 0.5996472665241787]\n"
     ]
    }
   ],
   "source": [
    "#Third Party Sharing/Collection-Action Third Party\n",
    "\n",
    "padded_docs = pad_sequences(Third_Party_Sharing_Collection_Action_Third_Party.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Third_Party_Sharing_Collection_Action_Third_Party.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Third_Party_Sharing_Collection_Action_Third_Party.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'third_party_sharing_collection_action_third_party_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('third_party_sharing_collection_action_third_party_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 239 samples, validate on 103 samples\n",
      "Epoch 1/10\n",
      "239/239 [==============================] - 4s 19ms/step - loss: 1.3608 - acc: 0.3264 - val_loss: 1.2603 - val_acc: 0.4175\n",
      "Epoch 2/10\n",
      "239/239 [==============================] - 0s 1ms/step - loss: 0.9519 - acc: 0.7197 - val_loss: 1.2342 - val_acc: 0.4660\n",
      "Epoch 3/10\n",
      "239/239 [==============================] - 0s 1ms/step - loss: 0.7450 - acc: 0.7364 - val_loss: 1.2600 - val_acc: 0.4369\n",
      "Epoch 4/10\n",
      "239/239 [==============================] - 0s 1ms/step - loss: 0.6011 - acc: 0.8285 - val_loss: 1.3552 - val_acc: 0.4175\n",
      "Epoch 5/10\n",
      "239/239 [==============================] - 0s 773us/step - loss: 0.5105 - acc: 0.8536 - val_loss: 1.4609 - val_acc: 0.3981\n",
      "Epoch 6/10\n",
      "239/239 [==============================] - 0s 732us/step - loss: 0.4302 - acc: 0.8787 - val_loss: 1.5199 - val_acc: 0.4078\n",
      "Epoch 7/10\n",
      "239/239 [==============================] - 0s 907us/step - loss: 0.3797 - acc: 0.8661 - val_loss: 1.5424 - val_acc: 0.4175\n",
      "Epoch 8/10\n",
      "239/239 [==============================] - 0s 2ms/step - loss: 0.3308 - acc: 0.8954 - val_loss: 1.6016 - val_acc: 0.4466\n",
      "Epoch 9/10\n",
      "239/239 [==============================] - 0s 1ms/step - loss: 0.2891 - acc: 0.9163 - val_loss: 1.6773 - val_acc: 0.4272\n",
      "Epoch 10/10\n",
      "239/239 [==============================] - 0s 795us/step - loss: 0.2563 - acc: 0.9289 - val_loss: 1.7655 - val_acc: 0.4272\n",
      "[[26  1  0  3]\n",
      " [ 0 98  0  8]\n",
      " [ 0  1 17  0]\n",
      " [ 0  5  1 79]]\n",
      "239/239 [==============================] - 0s 204us/step\n",
      "[0.2427607616370692, 0.9205020925489928]\n",
      "[[ 3  9  2  7]\n",
      " [ 2 23  1 18]\n",
      " [ 0  0  0  2]\n",
      " [ 1 13  4 18]]\n",
      "103/103 [==============================] - 0s 341us/step\n",
      "[1.7654980817466106, 0.42718446659810333]\n"
     ]
    }
   ],
   "source": [
    "#Third Party Sharing/Collection-Choice Scope\n",
    "\n",
    "padded_docs = pad_sequences(Third_Party_Sharing_Collection_Choice_Scope.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Third_Party_Sharing_Collection_Choice_Scope.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Third_Party_Sharing_Collection_Choice_Scope.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'third_party_sharing_collection_choice_scope_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('third_party_sharing_collection_choice_scope_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 476 samples, validate on 205 samples\n",
      "Epoch 1/10\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 2.0690 - acc: 0.2773 - val_loss: 1.7241 - val_acc: 0.4829\n",
      "Epoch 2/10\n",
      "476/476 [==============================] - 1s 1ms/step - loss: 1.3797 - acc: 0.5567 - val_loss: 1.5661 - val_acc: 0.5220\n",
      "Epoch 3/10\n",
      "476/476 [==============================] - 0s 732us/step - loss: 1.0608 - acc: 0.7038 - val_loss: 1.4749 - val_acc: 0.5171\n",
      "Epoch 4/10\n",
      "476/476 [==============================] - 0s 641us/step - loss: 0.8639 - acc: 0.7500 - val_loss: 1.5445 - val_acc: 0.4780\n",
      "Epoch 5/10\n",
      "476/476 [==============================] - 0s 719us/step - loss: 0.7074 - acc: 0.8193 - val_loss: 1.4935 - val_acc: 0.5024\n",
      "Epoch 6/10\n",
      "476/476 [==============================] - 0s 722us/step - loss: 0.6041 - acc: 0.8298 - val_loss: 1.5604 - val_acc: 0.4927\n",
      "Epoch 7/10\n",
      "476/476 [==============================] - 0s 710us/step - loss: 0.5256 - acc: 0.8634 - val_loss: 1.5881 - val_acc: 0.4927\n",
      "Epoch 8/10\n",
      "476/476 [==============================] - 0s 742us/step - loss: 0.4651 - acc: 0.8718 - val_loss: 1.6161 - val_acc: 0.4927\n",
      "Epoch 9/10\n",
      "476/476 [==============================] - 0s 726us/step - loss: 0.4094 - acc: 0.8887 - val_loss: 1.6897 - val_acc: 0.4634\n",
      "Epoch 10/10\n",
      "476/476 [==============================] - 0s 686us/step - loss: 0.3671 - acc: 0.9013 - val_loss: 1.6632 - val_acc: 0.4976\n",
      "[[ 27   0   0   0   0   0   0   0   0]\n",
      " [  0  52   0   8   0   0   0   0   0]\n",
      " [  0   0   9   0   1   1   0   0   0]\n",
      " [  0   0   0 164   0   0   0   0   0]\n",
      " [  0   1   0   1  35   0   1   2   0]\n",
      " [  0   1   0   1   0  38   0   0   0]\n",
      " [  0   0   0  10   2   1  43   1   0]\n",
      " [  0   0   0   2   0   0   0  36   0]\n",
      " [  0   0   0  10   1   1   0   0  27]]\n",
      "476/476 [==============================] - 0s 245us/step\n",
      "[0.3281209536460267, 0.9054621863765877]\n",
      "[[ 3  0  0  2  1  0  0  0  0]\n",
      " [ 0  5  0 13  0  1  3  0  1]\n",
      " [ 0  1  0  1  3  0  1  5  1]\n",
      " [ 0 11  1 67  0  1  4  0  0]\n",
      " [ 0  0  0  2  6  1  1  4  1]\n",
      " [ 0  0  0  2  0  7  2  2  0]\n",
      " [ 0  1  0  6  0  2  0  1  1]\n",
      " [ 1  0  0  4  3  2  0 11  0]\n",
      " [ 0  0  0 10  4  1  1  1  3]]\n",
      "205/205 [==============================] - 0s 247us/step\n",
      "[1.6631975697308052, 0.49756097604588767]\n"
     ]
    }
   ],
   "source": [
    "#Third Party Sharing/Collection-Choice Type\n",
    "\n",
    "padded_docs = pad_sequences(Third_Party_Sharing_Collection_Choice_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Third_Party_Sharing_Collection_Choice_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Third_Party_Sharing_Collection_Choice_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'third_party_sharing_collection_choice_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('third_party_sharing_collection_choice_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 655 samples, validate on 282 samples\n",
      "Epoch 1/10\n",
      "655/655 [==============================] - 2s 3ms/step - loss: 0.4900 - acc: 0.7267 - val_loss: 0.4037 - val_acc: 0.8369\n",
      "Epoch 2/10\n",
      "655/655 [==============================] - 0s 680us/step - loss: 0.2745 - acc: 0.8931 - val_loss: 0.3047 - val_acc: 0.9149\n",
      "Epoch 3/10\n",
      "655/655 [==============================] - 0s 653us/step - loss: 0.1596 - acc: 0.9496 - val_loss: 0.2700 - val_acc: 0.9184\n",
      "Epoch 4/10\n",
      "655/655 [==============================] - 0s 666us/step - loss: 0.1088 - acc: 0.9588 - val_loss: 0.2602 - val_acc: 0.9326\n",
      "Epoch 5/10\n",
      "655/655 [==============================] - 0s 749us/step - loss: 0.0825 - acc: 0.9695 - val_loss: 0.2689 - val_acc: 0.9326\n",
      "Epoch 6/10\n",
      "655/655 [==============================] - 0s 707us/step - loss: 0.0674 - acc: 0.9817 - val_loss: 0.2802 - val_acc: 0.9362\n",
      "Epoch 7/10\n",
      "655/655 [==============================] - 0s 632us/step - loss: 0.0516 - acc: 0.9817 - val_loss: 0.2965 - val_acc: 0.9362\n",
      "Epoch 8/10\n",
      "655/655 [==============================] - 0s 679us/step - loss: 0.0433 - acc: 0.9832 - val_loss: 0.3156 - val_acc: 0.9255\n",
      "Epoch 9/10\n",
      "655/655 [==============================] - 0s 684us/step - loss: 0.0451 - acc: 0.9878 - val_loss: 0.3216 - val_acc: 0.9397\n",
      "Epoch 10/10\n",
      "655/655 [==============================] - 0s 675us/step - loss: 0.0337 - acc: 0.9878 - val_loss: 0.3259 - val_acc: 0.9397\n",
      "[[552   0]\n",
      " [  7  96]]\n",
      "655/655 [==============================] - 0s 228us/step\n",
      "[0.03293369755945133, 0.9893129771902361]\n",
      "[[232   4]\n",
      " [ 13  33]]\n",
      "282/282 [==============================] - 0s 211us/step\n",
      "[0.3259097086199632, 0.9397163124794655]\n"
     ]
    }
   ],
   "source": [
    "#Third Party Sharing/Collection-Does/Does Not\n",
    "\n",
    "padded_docs = pad_sequences(Third_Party_Sharing_Collection_Does_Does_Not.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Third_Party_Sharing_Collection_Does_Does_Not.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Third_Party_Sharing_Collection_Does_Does_Not.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'third_party_sharing_collection_does_does_not_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('third_party_sharing_collection_does_does_not_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 270 samples, validate on 116 samples\n",
      "Epoch 1/10\n",
      "270/270 [==============================] - 2s 6ms/step - loss: 1.3163 - acc: 0.4148 - val_loss: 1.2671 - val_acc: 0.4655\n",
      "Epoch 2/10\n",
      "270/270 [==============================] - 0s 699us/step - loss: 0.9941 - acc: 0.6481 - val_loss: 1.2475 - val_acc: 0.4483\n",
      "Epoch 3/10\n",
      "270/270 [==============================] - 0s 750us/step - loss: 0.8218 - acc: 0.7259 - val_loss: 1.1812 - val_acc: 0.5086\n",
      "Epoch 4/10\n",
      "270/270 [==============================] - 0s 658us/step - loss: 0.7010 - acc: 0.7926 - val_loss: 1.1606 - val_acc: 0.5172\n",
      "Epoch 5/10\n",
      "270/270 [==============================] - 0s 882us/step - loss: 0.6130 - acc: 0.8074 - val_loss: 1.1506 - val_acc: 0.5345\n",
      "Epoch 6/10\n",
      "270/270 [==============================] - 0s 747us/step - loss: 0.5518 - acc: 0.8074 - val_loss: 1.1669 - val_acc: 0.5431\n",
      "Epoch 7/10\n",
      "270/270 [==============================] - 0s 682us/step - loss: 0.4959 - acc: 0.8296 - val_loss: 1.1850 - val_acc: 0.5690\n",
      "Epoch 8/10\n",
      "270/270 [==============================] - 0s 764us/step - loss: 0.4459 - acc: 0.8481 - val_loss: 1.2054 - val_acc: 0.5603\n",
      "Epoch 9/10\n",
      "270/270 [==============================] - 0s 774us/step - loss: 0.4052 - acc: 0.8630 - val_loss: 1.2371 - val_acc: 0.5776\n",
      "Epoch 10/10\n",
      "270/270 [==============================] - 0s 713us/step - loss: 0.3731 - acc: 0.8778 - val_loss: 1.2626 - val_acc: 0.5690\n",
      "[[99  0  2  0]\n",
      " [ 5 93  1  2]\n",
      " [ 8  1 12  0]\n",
      " [ 4  2  1 40]]\n",
      "270/270 [==============================] - 0s 189us/step\n",
      "[0.34810414623331143, 0.9037037023791561]\n",
      "[[33  2  1  1]\n",
      " [ 8 22  0  9]\n",
      " [ 9  0  1  4]\n",
      " [10  5  1 10]]\n",
      "116/116 [==============================] - 0s 294us/step\n",
      "[1.2625778707964668, 0.5689655192967119]\n"
     ]
    }
   ],
   "source": [
    "#Third Party Sharing/Collection-Identifiability\n",
    "\n",
    "padded_docs = pad_sequences(Third_Party_Sharing_Collection_Identifiability.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Third_Party_Sharing_Collection_Identifiability.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Third_Party_Sharing_Collection_Identifiability.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'third_party_sharing_collection_identifiability_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('third_party_sharing_collection_identifiability_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1024 samples, validate on 439 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 2s 2ms/step - loss: 2.3549 - acc: 0.2422 - val_loss: 2.0063 - val_acc: 0.3485\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 1s 677us/step - loss: 1.6987 - acc: 0.5127 - val_loss: 1.7505 - val_acc: 0.4829\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 1s 696us/step - loss: 1.3945 - acc: 0.6064 - val_loss: 1.6034 - val_acc: 0.5057\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 1s 676us/step - loss: 1.2020 - acc: 0.6768 - val_loss: 1.5422 - val_acc: 0.5148\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 1s 690us/step - loss: 1.0563 - acc: 0.7090 - val_loss: 1.5054 - val_acc: 0.5399\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 1s 680us/step - loss: 0.9460 - acc: 0.7451 - val_loss: 1.4913 - val_acc: 0.5535\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 1s 651us/step - loss: 0.8652 - acc: 0.7578 - val_loss: 1.4923 - val_acc: 0.5330\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 1s 662us/step - loss: 0.8025 - acc: 0.7852 - val_loss: 1.4896 - val_acc: 0.5421\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 1s 719us/step - loss: 0.7283 - acc: 0.7998 - val_loss: 1.4884 - val_acc: 0.5421\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 1s 700us/step - loss: 0.6840 - acc: 0.8125 - val_loss: 1.5047 - val_acc: 0.5421\n",
      "[[ 39   0   0   0   0   0   0   0   1   1   0   0   0   0   2]\n",
      " [  0  97   0   0   0   1   0   0   0   5   0   0   2   0   0]\n",
      " [  0   0 106   0   0   0   0   0   0   0   0   0   1   0   0]\n",
      " [  0   0   0  16   0   2   0   0   0   2   0   0   0   1   4]\n",
      " [  0   0   0   0  40   1   0   0   0   1   0   0   2   1   0]\n",
      " [  0   5   0   0   0  85   0   0   0   6   0   0  11   0   1]\n",
      " [  0   0   0   0   1   3   1   0   0   0   0   0   2   0   2]\n",
      " [  1   0   1   0   0   1   0  36   0   2   0   0   1   0   0]\n",
      " [  3   6   0   0   0   3   0   2   6   1   0   0   1   1   1]\n",
      " [  0   1   2   0   0   5   0   0   0  61   0   0  12   7   7]\n",
      " [  0   1   0   0   0   4   0   1   0   2   1   0   1   0   0]\n",
      " [  0   1   0   0   0   1   0   0   0   0   0   2   2   4   0]\n",
      " [  0   1   1   0   0  13   0   0   0  10   0   0 121   0   1]\n",
      " [  0   3   0   0   1   3   0   0   0   1   0   0   0  45   2]\n",
      " [  1   0   1   0   0   1   0   0   0   1   0   0   1   3 191]]\n",
      "1024/1024 [==============================] - 0s 221us/step\n",
      "[0.6385370492935181, 0.8271484375]\n",
      "[[ 9  0  1  0  0  0  0  1  0  1  0  0  1  0  3]\n",
      " [ 0 40  0  0  1  1  0  1  0  2  0  0  1  2  1]\n",
      " [ 0  0 49  0  0  1  0  1  0  2  0  0  1  0  4]\n",
      " [ 0  2  0  4  0  0  0  0  0  0  0  0  3  0  2]\n",
      " [ 0  0  0  0 12  1  0  0  0  3  0  0  3  1  4]\n",
      " [ 0  1  0  0  1 20  0  0  0  9  0  0 13  2  4]\n",
      " [ 0  0  0  0  0  1  0  0  0  1  0  0  4  0  0]\n",
      " [ 2  1  2  0  0  0  0 12  0  0  0  0  0  0  2]\n",
      " [ 2  3  0  1  0  0  0  2  1  0  0  0  0  2  1]\n",
      " [ 0  2  1  0  0  3  0  0  0  8  0  0 11  7  6]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  0  0  2  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  1  0  0  0  0  2]\n",
      " [ 0  0  2  1  0  9  0  0  0 11  0  0 17  1 12]\n",
      " [ 0  2  0  0  0  2  0  0  0  5  0  0  2  5  6]\n",
      " [ 1  0  2  0  1  1  0  0  0  0  0  0  2  6 61]]\n",
      "439/439 [==============================] - 0s 255us/step\n",
      "[1.5047028683855757, 0.5421412312903002]\n"
     ]
    }
   ],
   "source": [
    "#Third Party Sharing/Collection-Personal Information Type\n",
    "\n",
    "padded_docs = pad_sequences(Third_Party_Sharing_Collection_Personal_Information_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Third_Party_Sharing_Collection_Personal_Information_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Third_Party_Sharing_Collection_Personal_Information_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'third_party_sharing_collection_personal_information_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('third_party_sharing_collection_personal_information_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1908 samples, validate on 819 samples\n",
      "Epoch 1/10\n",
      "1908/1908 [==============================] - 3s 2ms/step - loss: 1.8922 - acc: 0.3884 - val_loss: 1.5478 - val_acc: 0.5116\n",
      "Epoch 2/10\n",
      "1908/1908 [==============================] - 1s 636us/step - loss: 1.1365 - acc: 0.6787 - val_loss: 1.4306 - val_acc: 0.5470\n",
      "Epoch 3/10\n",
      "1908/1908 [==============================] - 2s 898us/step - loss: 0.8555 - acc: 0.7390 - val_loss: 1.3755 - val_acc: 0.5641\n",
      "Epoch 4/10\n",
      "1908/1908 [==============================] - 2s 840us/step - loss: 0.6773 - acc: 0.8077 - val_loss: 1.3637 - val_acc: 0.5714\n",
      "Epoch 5/10\n",
      "1908/1908 [==============================] - 1s 647us/step - loss: 0.5627 - acc: 0.8386 - val_loss: 1.3982 - val_acc: 0.5617\n",
      "Epoch 6/10\n",
      "1908/1908 [==============================] - 1s 658us/step - loss: 0.4977 - acc: 0.8585 - val_loss: 1.4293 - val_acc: 0.5604\n",
      "Epoch 7/10\n",
      "1908/1908 [==============================] - 1s 731us/step - loss: 0.4281 - acc: 0.8763 - val_loss: 1.4569 - val_acc: 0.5592\n",
      "Epoch 8/10\n",
      "1908/1908 [==============================] - 1s 703us/step - loss: 0.3614 - acc: 0.9025 - val_loss: 1.4781 - val_acc: 0.5580\n",
      "Epoch 9/10\n",
      "1908/1908 [==============================] - 1s 780us/step - loss: 0.3224 - acc: 0.9167 - val_loss: 1.5128 - val_acc: 0.5629\n",
      "Epoch 10/10\n",
      "1908/1908 [==============================] - 2s 889us/step - loss: 0.2746 - acc: 0.9271 - val_loss: 1.5202 - val_acc: 0.5641\n",
      "[[199   1   0   5   0   4   0   3   0   1   1]\n",
      " [  0 233   1   1   0   5   0   2   2   0   0]\n",
      " [  1   1 153   0   0   2   0   3   1   0   0]\n",
      " [  7   1   0 242   0   0   1   2   0   1   0]\n",
      " [  0   0   0   0 213   0   1   2   0   1   0]\n",
      " [  3   1   1   1   0 157   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 132   1   0   0   0]\n",
      " [  1   1   1   2   0   3   1 112   0   0   1]\n",
      " [  0   1   1   0   0   0   0   0  40   0   0]\n",
      " [  4   0   2   4   3   0   1   3   0 188   1]\n",
      " [  0   1   0   4   0   2   0   4   0   1 140]]\n",
      "1908/1908 [==============================] - 0s 231us/step\n",
      "[0.23321998025636254, 0.9481132075471698]\n",
      "[[37  7  3 15  0  6  1  5  4  6  5]\n",
      " [ 3 67 10  1  0 12  0  3  2  1  1]\n",
      " [ 1 11 39  2  0  7  0  0  1  7  4]\n",
      " [18  2  4 67  0 11  1  3  2 11  8]\n",
      " [ 0  0  0  2 65  0  3  1  0  7  0]\n",
      " [12  8  0  5  0 40  1  3  0  0  2]\n",
      " [ 0  0  0  1  1  1 45  1  0  0  0]\n",
      " [ 3  1  1  7  2  4  0 10  0  7  2]\n",
      " [ 4  4  6  0  0  0  0  0  3  2  0]\n",
      " [ 6  2  6 10  9  2  1  8  2 58  2]\n",
      " [ 5  2  1  5  3  4  1 15  0  4 31]]\n",
      "819/819 [==============================] - 0s 354us/step\n",
      "[1.5202211617550134, 0.5641025645392281]\n"
     ]
    }
   ],
   "source": [
    "#Third Party Sharing/Collection-Purpose\n",
    "\n",
    "padded_docs = pad_sequences(Third_Party_Sharing_Collection_Purpose.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Third_Party_Sharing_Collection_Purpose.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Third_Party_Sharing_Collection_Purpose.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'third_party_sharing_collection_purpose_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('third_party_sharing_collection_purpose_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1465 samples, validate on 629 samples\n",
      "Epoch 1/10\n",
      "1465/1465 [==============================] - 3s 2ms/step - loss: 1.4666 - acc: 0.4198 - val_loss: 1.2717 - val_acc: 0.5183\n",
      "Epoch 2/10\n",
      "1465/1465 [==============================] - 1s 643us/step - loss: 1.0869 - acc: 0.5686 - val_loss: 1.2047 - val_acc: 0.5390\n",
      "Epoch 3/10\n",
      "1465/1465 [==============================] - 1s 710us/step - loss: 0.9096 - acc: 0.6519 - val_loss: 1.2098 - val_acc: 0.5533\n",
      "Epoch 4/10\n",
      "1465/1465 [==============================] - 1s 598us/step - loss: 0.7912 - acc: 0.7106 - val_loss: 1.2005 - val_acc: 0.5596\n",
      "Epoch 5/10\n",
      "1465/1465 [==============================] - 1s 521us/step - loss: 0.6910 - acc: 0.7440 - val_loss: 1.2254 - val_acc: 0.5580\n",
      "Epoch 6/10\n",
      "1465/1465 [==============================] - 1s 642us/step - loss: 0.6141 - acc: 0.7768 - val_loss: 1.2717 - val_acc: 0.5564\n",
      "Epoch 7/10\n",
      "1465/1465 [==============================] - 1s 665us/step - loss: 0.5552 - acc: 0.7973 - val_loss: 1.2911 - val_acc: 0.5453\n",
      "Epoch 8/10\n",
      "1465/1465 [==============================] - 1s 714us/step - loss: 0.4986 - acc: 0.8157 - val_loss: 1.3588 - val_acc: 0.5469\n",
      "Epoch 9/10\n",
      "1465/1465 [==============================] - 1s 623us/step - loss: 0.4633 - acc: 0.8348 - val_loss: 1.3882 - val_acc: 0.5469\n",
      "Epoch 10/10\n",
      "1465/1465 [==============================] - 1s 577us/step - loss: 0.4120 - acc: 0.8532 - val_loss: 1.4268 - val_acc: 0.5485\n",
      "[[460   3   1   0   0  37   0]\n",
      " [ 11  24   0   0   0  11   0]\n",
      " [ 12   0 147   0   0   9   0]\n",
      " [  1   0   0  29   0   3   0]\n",
      " [  1   0   0   1  43   2   1]\n",
      " [ 57   2   2   0   0 547   1]\n",
      " [  5   0   0   0   0  19  36]]\n",
      "1465/1465 [==============================] - 0s 192us/step\n",
      "[0.36893264431595396, 0.8778156992111597]\n",
      "[[144   5  15   1   2  67   0]\n",
      " [  8   0   1   0   1   9   1]\n",
      " [ 10   1  24   0   0  18   0]\n",
      " [  2   1   1   1   4   4   0]\n",
      " [  4   0   0   0   7   3   0]\n",
      " [ 85   1   9   1   0 167   6]\n",
      " [  3   0   2   0   0  19   2]]\n",
      "629/629 [==============================] - 0s 195us/step\n",
      "[1.426778320762046, 0.5484896668948126]\n"
     ]
    }
   ],
   "source": [
    "#Third Party Sharing/Collection-Third Party Entity\n",
    "\n",
    "padded_docs = pad_sequences(Third_Party_Sharing_Collection_Third_Party_Entity.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Third_Party_Sharing_Collection_Third_Party_Entity.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Third_Party_Sharing_Collection_Third_Party_Entity.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'third_party_sharing_collection_third_party_entity_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('third_party_sharing_collection_third_party_entity_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95 samples, validate on 41 samples\n",
      "Epoch 1/10\n",
      "95/95 [==============================] - 2s 17ms/step - loss: 1.3686 - acc: 0.3684 - val_loss: 1.2939 - val_acc: 0.4146\n",
      "Epoch 2/10\n",
      "95/95 [==============================] - 0s 670us/step - loss: 1.1679 - acc: 0.7579 - val_loss: 1.2152 - val_acc: 0.4634\n",
      "Epoch 3/10\n",
      "95/95 [==============================] - 0s 641us/step - loss: 1.0316 - acc: 0.7368 - val_loss: 1.1238 - val_acc: 0.4878\n",
      "Epoch 4/10\n",
      "95/95 [==============================] - 0s 628us/step - loss: 0.8913 - acc: 0.7368 - val_loss: 1.0621 - val_acc: 0.5122\n",
      "Epoch 5/10\n",
      "95/95 [==============================] - 0s 695us/step - loss: 0.7897 - acc: 0.7158 - val_loss: 1.0390 - val_acc: 0.5122\n",
      "Epoch 6/10\n",
      "95/95 [==============================] - 0s 670us/step - loss: 0.7205 - acc: 0.7368 - val_loss: 1.0314 - val_acc: 0.5366\n",
      "Epoch 7/10\n",
      "95/95 [==============================] - 0s 676us/step - loss: 0.6557 - acc: 0.8000 - val_loss: 1.0355 - val_acc: 0.5854\n",
      "Epoch 8/10\n",
      "95/95 [==============================] - 0s 621us/step - loss: 0.6013 - acc: 0.7684 - val_loss: 1.0400 - val_acc: 0.5854\n",
      "Epoch 9/10\n",
      "95/95 [==============================] - 0s 664us/step - loss: 0.5550 - acc: 0.7895 - val_loss: 1.0456 - val_acc: 0.5854\n",
      "Epoch 10/10\n",
      "95/95 [==============================] - 0s 671us/step - loss: 0.5165 - acc: 0.7789 - val_loss: 1.0551 - val_acc: 0.5366\n",
      "[[17  3  5  0]\n",
      " [ 1 19  4  0]\n",
      " [ 1  0 39  0]\n",
      " [ 1  1  2  2]]\n",
      "95/95 [==============================] - 0s 188us/step\n",
      "[0.48385267006723504, 0.8105263176717257]\n",
      "[[ 3  4  4  0]\n",
      " [ 2  3  2  0]\n",
      " [ 3  2 16  0]\n",
      " [ 0  1  1  0]]\n",
      "41/41 [==============================] - 0s 334us/step\n",
      "[1.055105604776522, 0.536585370214974]\n"
     ]
    }
   ],
   "source": [
    "#Third Party Sharing/Collection-User Type\n",
    "\n",
    "padded_docs = pad_sequences(Third_Party_Sharing_Collection_User_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(Third_Party_Sharing_Collection_User_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, Third_Party_Sharing_Collection_User_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'third_party_sharing_collection_user_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('third_party_sharing_collection_user_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 294 samples, validate on 127 samples\n",
      "Epoch 1/10\n",
      "294/294 [==============================] - 2s 6ms/step - loss: 1.6466 - acc: 0.3980 - val_loss: 1.4734 - val_acc: 0.4961\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 0s 567us/step - loss: 1.1996 - acc: 0.5340 - val_loss: 1.4941 - val_acc: 0.4882\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 0s 583us/step - loss: 0.9901 - acc: 0.6224 - val_loss: 1.4637 - val_acc: 0.5197\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 0s 605us/step - loss: 0.8588 - acc: 0.7109 - val_loss: 1.4696 - val_acc: 0.4961\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 0s 617us/step - loss: 0.7465 - acc: 0.7823 - val_loss: 1.4980 - val_acc: 0.4882\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 0s 718us/step - loss: 0.6538 - acc: 0.7959 - val_loss: 1.5501 - val_acc: 0.4803\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 0s 547us/step - loss: 0.5809 - acc: 0.8095 - val_loss: 1.5956 - val_acc: 0.4646\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 0s 637us/step - loss: 0.5191 - acc: 0.8299 - val_loss: 1.6158 - val_acc: 0.4724\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 0s 600us/step - loss: 0.4724 - acc: 0.8537 - val_loss: 1.6460 - val_acc: 0.4803\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 0s 563us/step - loss: 0.4254 - acc: 0.8639 - val_loss: 1.6980 - val_acc: 0.4961\n",
      "[[ 20   0   0   0   1   6]\n",
      " [  0  24   0   0   2   3]\n",
      " [  1   0  24   0   2   5]\n",
      " [  0   0   0  18   0   3]\n",
      " [  1   0   0   0  30   4]\n",
      " [  1   2   0   1   4 142]]\n",
      "294/294 [==============================] - 0s 212us/step\n",
      "[0.39268965319711335, 0.8775510200026895]\n",
      "[[ 2  2  1  0  2  4]\n",
      " [ 0  1  0  0  1  8]\n",
      " [ 1  1  1  0  2 12]\n",
      " [ 0  0  1  2  0  3]\n",
      " [ 3  0  1  0  5 11]\n",
      " [ 1  0  2  2  6 52]]\n",
      "127/127 [==============================] - 0s 206us/step\n",
      "[1.6980496213192076, 0.49606298954468075]\n"
     ]
    }
   ],
   "source": [
    "#User Access, Edit and Deletion-Access Scope\n",
    "\n",
    "padded_docs = pad_sequences(User_Access_Edit_And_Deletion_Access_Scope.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(User_Access_Edit_And_Deletion_Access_Scope.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, User_Access_Edit_And_Deletion_Access_Scope.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'user_access_edit_deletion_access_scope_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('user_access_edit_deletion_access_scope_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 359 samples, validate on 155 samples\n",
      "Epoch 1/10\n",
      "359/359 [==============================] - 2s 6ms/step - loss: 1.9657 - acc: 0.3955 - val_loss: 1.7632 - val_acc: 0.4516\n",
      "Epoch 2/10\n",
      "359/359 [==============================] - 0s 603us/step - loss: 1.3366 - acc: 0.5599 - val_loss: 1.6631 - val_acc: 0.4452\n",
      "Epoch 3/10\n",
      "359/359 [==============================] - 0s 680us/step - loss: 1.1152 - acc: 0.6435 - val_loss: 1.6238 - val_acc: 0.4258\n",
      "Epoch 4/10\n",
      "359/359 [==============================] - 0s 689us/step - loss: 0.9491 - acc: 0.6936 - val_loss: 1.6168 - val_acc: 0.4516\n",
      "Epoch 5/10\n",
      "359/359 [==============================] - 0s 698us/step - loss: 0.8238 - acc: 0.7187 - val_loss: 1.6161 - val_acc: 0.4774\n",
      "Epoch 6/10\n",
      "359/359 [==============================] - 0s 664us/step - loss: 0.7294 - acc: 0.7744 - val_loss: 1.6111 - val_acc: 0.4903\n",
      "Epoch 7/10\n",
      "359/359 [==============================] - 0s 612us/step - loss: 0.6372 - acc: 0.8078 - val_loss: 1.6368 - val_acc: 0.4968\n",
      "Epoch 8/10\n",
      "359/359 [==============================] - 0s 718us/step - loss: 0.5619 - acc: 0.8329 - val_loss: 1.6514 - val_acc: 0.4968\n",
      "Epoch 9/10\n",
      "359/359 [==============================] - 0s 669us/step - loss: 0.4991 - acc: 0.8468 - val_loss: 1.6733 - val_acc: 0.5032\n",
      "Epoch 10/10\n",
      "359/359 [==============================] - 0s 640us/step - loss: 0.4491 - acc: 0.8607 - val_loss: 1.7032 - val_acc: 0.5226\n",
      "[[  5   0   0   4   0   0   0   0   0]\n",
      " [  0  11   0   6   0   0   0   0   1]\n",
      " [  0   0  32   6   0   0   0   0   0]\n",
      " [  0   0   2 185   0   0   0   0   1]\n",
      " [  0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   1   0   6   0   0   1]\n",
      " [  0   0   2   1   0   0  34   0   2]\n",
      " [  1   0   0   2   0   0   0   8   4]\n",
      " [  0   0   0  10   0   0   0   0  33]]\n",
      "359/359 [==============================] - 0s 211us/step\n",
      "[0.4098074173860895, 0.8774373262373518]\n",
      "[[ 1  0  0  6  0  0  0  0]\n",
      " [ 0  0  1  7  0  0  0  0]\n",
      " [ 0  1  6  8  0  2  0  0]\n",
      " [ 0  0  2 65  0  2  0  1]\n",
      " [ 0  0  0  0  0  1  0  0]\n",
      " [ 0  0  0 16  0  3  0  2]\n",
      " [ 0  1  1  6  0  1  0  3]\n",
      " [ 0  0  0  9  0  4  0  6]]\n",
      "155/155 [==============================] - 0s 217us/step\n",
      "[1.703209937003351, 0.5225806497758435]\n"
     ]
    }
   ],
   "source": [
    "#User Access, Edit and Deletion-Access Type\n",
    "\n",
    "padded_docs = pad_sequences(User_Access_Edit_And_Deletion_Access_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(User_Access_Edit_And_Deletion_Access_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, User_Access_Edit_And_Deletion_Access_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'user_access_edit_deletion_access_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('user_access_edit_deletion_access_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 111 samples, validate on 48 samples\n",
      "Epoch 1/10\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 1.3183 - acc: 0.4144 - val_loss: 0.9643 - val_acc: 0.7500\n",
      "Epoch 2/10\n",
      "111/111 [==============================] - 0s 1ms/step - loss: 0.8678 - acc: 0.7117 - val_loss: 0.7652 - val_acc: 0.7500\n",
      "Epoch 3/10\n",
      "111/111 [==============================] - 0s 885us/step - loss: 0.6641 - acc: 0.7207 - val_loss: 0.7254 - val_acc: 0.7500\n",
      "Epoch 4/10\n",
      "111/111 [==============================] - 0s 855us/step - loss: 0.5624 - acc: 0.7748 - val_loss: 0.6721 - val_acc: 0.7083\n",
      "Epoch 5/10\n",
      "111/111 [==============================] - 0s 908us/step - loss: 0.4670 - acc: 0.8198 - val_loss: 0.6743 - val_acc: 0.8125\n",
      "Epoch 6/10\n",
      "111/111 [==============================] - 0s 925us/step - loss: 0.4348 - acc: 0.8649 - val_loss: 0.6702 - val_acc: 0.8125\n",
      "Epoch 7/10\n",
      "111/111 [==============================] - 0s 903us/step - loss: 0.4031 - acc: 0.8739 - val_loss: 0.6504 - val_acc: 0.8125\n",
      "Epoch 8/10\n",
      "111/111 [==============================] - 0s 953us/step - loss: 0.3645 - acc: 0.9009 - val_loss: 0.6377 - val_acc: 0.8125\n",
      "Epoch 9/10\n",
      "111/111 [==============================] - 0s 971us/step - loss: 0.3289 - acc: 0.9099 - val_loss: 0.6330 - val_acc: 0.7917\n",
      "Epoch 10/10\n",
      "111/111 [==============================] - 0s 868us/step - loss: 0.2956 - acc: 0.9099 - val_loss: 0.6243 - val_acc: 0.7917\n",
      "[[11  0  2  0]\n",
      " [ 0 13  4  0]\n",
      " [ 0  2 74  0]\n",
      " [ 0  0  0  5]]\n",
      "111/111 [==============================] - 0s 209us/step\n",
      "[0.26761574519647136, 0.9279279284649067]\n",
      "[[ 5  2  1  0]\n",
      " [ 0  0  1  1]\n",
      " [ 1  2 33  0]\n",
      " [ 0  2  0  0]]\n",
      "48/48 [==============================] - 0s 229us/step\n",
      "[0.6242954730987549, 0.7916666666666666]\n"
     ]
    }
   ],
   "source": [
    "#User Access, Edit and Deletion-User Type\n",
    "\n",
    "padded_docs = pad_sequences(User_Access_Edit_And_Deletion_User_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(User_Access_Edit_And_Deletion_User_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, User_Access_Edit_And_Deletion_User_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'user_access_edit_deletion_user_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('user_access_edit_deletion_user_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 769 samples, validate on 330 samples\n",
      "Epoch 1/10\n",
      "769/769 [==============================] - 2s 3ms/step - loss: 1.4005 - acc: 0.4226 - val_loss: 1.2235 - val_acc: 0.5242\n",
      "Epoch 2/10\n",
      "769/769 [==============================] - 0s 529us/step - loss: 0.9525 - acc: 0.6437 - val_loss: 1.1521 - val_acc: 0.5273\n",
      "Epoch 3/10\n",
      "769/769 [==============================] - 0s 554us/step - loss: 0.7359 - acc: 0.7516 - val_loss: 1.1676 - val_acc: 0.5515\n",
      "Epoch 4/10\n",
      "769/769 [==============================] - 0s 535us/step - loss: 0.5919 - acc: 0.7945 - val_loss: 1.1822 - val_acc: 0.5364\n",
      "Epoch 5/10\n",
      "769/769 [==============================] - 0s 546us/step - loss: 0.4879 - acc: 0.8349 - val_loss: 1.2019 - val_acc: 0.5545\n",
      "Epoch 6/10\n",
      "769/769 [==============================] - 0s 562us/step - loss: 0.4048 - acc: 0.8674 - val_loss: 1.2453 - val_acc: 0.5545\n",
      "Epoch 7/10\n",
      "769/769 [==============================] - 0s 633us/step - loss: 0.3460 - acc: 0.8895 - val_loss: 1.3195 - val_acc: 0.5636\n",
      "Epoch 8/10\n",
      "769/769 [==============================] - 0s 547us/step - loss: 0.2983 - acc: 0.9103 - val_loss: 1.3472 - val_acc: 0.5606\n",
      "Epoch 9/10\n",
      "769/769 [==============================] - 0s 608us/step - loss: 0.2566 - acc: 0.9220 - val_loss: 1.4109 - val_acc: 0.5667\n",
      "Epoch 10/10\n",
      "769/769 [==============================] - 0s 541us/step - loss: 0.2280 - acc: 0.9246 - val_loss: 1.4560 - val_acc: 0.5455\n",
      "[[147   4   4   0   0]\n",
      " [  2 296   0   6   1]\n",
      " [  5   3 148   3   1]\n",
      " [  1   1   1  67   2]\n",
      " [  2   6   0   0  69]]\n",
      "769/769 [==============================] - 0s 211us/step\n",
      "[0.20221248176524123, 0.9453836150845254]\n",
      "[[ 24  17  10   2   7]\n",
      " [ 10 113   7  10   6]\n",
      " [  3   9  31  11   3]\n",
      " [  3  15   4   6   1]\n",
      " [  9  13   9   1   6]]\n",
      "330/330 [==============================] - 0s 211us/step\n",
      "[1.4559499942895138, 0.545454546177026]\n"
     ]
    }
   ],
   "source": [
    "#User Choice/Control-Choice Scope\n",
    "\n",
    "padded_docs = pad_sequences(User_Choice_Control_Choice_Scope.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(User_Choice_Control_Choice_Scope.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, User_Choice_Control_Choice_Scope.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'user_choice_control_choice_scope_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('user_choice_control_choice_scope_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 883 samples, validate on 379 samples\n",
      "Epoch 1/10\n",
      "883/883 [==============================] - 2s 3ms/step - loss: 2.0484 - acc: 0.2718 - val_loss: 1.8397 - val_acc: 0.4327\n",
      "Epoch 2/10\n",
      "883/883 [==============================] - 0s 545us/step - loss: 1.3821 - acc: 0.6251 - val_loss: 1.6038 - val_acc: 0.5172\n",
      "Epoch 3/10\n",
      "883/883 [==============================] - 0s 517us/step - loss: 1.0216 - acc: 0.7259 - val_loss: 1.5567 - val_acc: 0.4987\n",
      "Epoch 4/10\n",
      "883/883 [==============================] - 0s 516us/step - loss: 0.7763 - acc: 0.7803 - val_loss: 1.5149 - val_acc: 0.5251\n",
      "Epoch 5/10\n",
      "883/883 [==============================] - 1s 600us/step - loss: 0.6178 - acc: 0.8505 - val_loss: 1.5246 - val_acc: 0.5251\n",
      "Epoch 6/10\n",
      "883/883 [==============================] - 1s 579us/step - loss: 0.5023 - acc: 0.8698 - val_loss: 1.5731 - val_acc: 0.5119\n",
      "Epoch 7/10\n",
      "883/883 [==============================] - 0s 537us/step - loss: 0.4218 - acc: 0.8901 - val_loss: 1.5985 - val_acc: 0.5330\n",
      "Epoch 8/10\n",
      "883/883 [==============================] - 0s 536us/step - loss: 0.3679 - acc: 0.9071 - val_loss: 1.6665 - val_acc: 0.5198\n",
      "Epoch 9/10\n",
      "883/883 [==============================] - 0s 535us/step - loss: 0.3246 - acc: 0.9037 - val_loss: 1.6680 - val_acc: 0.5145\n",
      "Epoch 10/10\n",
      "883/883 [==============================] - 0s 535us/step - loss: 0.2841 - acc: 0.9207 - val_loss: 1.7383 - val_acc: 0.5119\n",
      "[[102   0   1   1   0   0   0   0   1]\n",
      " [  0  67   0   4   0   0   1   0   0]\n",
      " [  0   0  72   0   4   0   0   0   2]\n",
      " [  0   0   0 111   1   0   0   0   0]\n",
      " [  0   1   1   3 135   1   2   4   6]\n",
      " [  0   0   0   3   1 141   0   0   3]\n",
      " [  0   1   0   2   4   3  59   0   2]\n",
      " [  0   0   0   3   1   0   0  52   1]\n",
      " [  0   0   0   0   4   1   0   1  81]]\n",
      "883/883 [==============================] - 0s 185us/step\n",
      "[0.250091122980161, 0.9286523212932919]\n",
      "[[57  1  1  2  3  0  1  0  3]\n",
      " [ 1 18  0  5  1  3  0  0  2]\n",
      " [ 5  3  7  1  7  2  2  2  4]\n",
      " [ 1  3  3 27  0  6  2  0  3]\n",
      " [ 0  2  3  4 34 11  2  3  8]\n",
      " [ 0  0  1  4  5 41  1  0  2]\n",
      " [ 1  4  2  8  5  8  1  0  6]\n",
      " [ 0  1  1  1  5  0  0  3  3]\n",
      " [ 3  2  3  3  8  6  2  0  6]]\n",
      "379/379 [==============================] - 0s 184us/step\n",
      "[1.7382987662166907, 0.5118733517098238]\n"
     ]
    }
   ],
   "source": [
    "#User Choice/Control-Choice Type\n",
    "\n",
    "padded_docs = pad_sequences(User_Choice_Control_Choice_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(User_Choice_Control_Choice_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, User_Choice_Control_Choice_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'user_choice_control_choice_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('user_choice_control_choice_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 344 samples, validate on 148 samples\n",
      "Epoch 1/10\n",
      "344/344 [==============================] - 2s 6ms/step - loss: 2.6762 - acc: 0.2442 - val_loss: 2.3635 - val_acc: 0.4932\n",
      "Epoch 2/10\n",
      "344/344 [==============================] - 0s 626us/step - loss: 1.9912 - acc: 0.5988 - val_loss: 1.9980 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "344/344 [==============================] - 0s 692us/step - loss: 1.6148 - acc: 0.5727 - val_loss: 1.8698 - val_acc: 0.4527\n",
      "Epoch 4/10\n",
      "344/344 [==============================] - 0s 589us/step - loss: 1.3672 - acc: 0.6453 - val_loss: 1.7696 - val_acc: 0.5135\n",
      "Epoch 5/10\n",
      "344/344 [==============================] - 0s 641us/step - loss: 1.2003 - acc: 0.7209 - val_loss: 1.7266 - val_acc: 0.5135\n",
      "Epoch 6/10\n",
      "344/344 [==============================] - 0s 639us/step - loss: 1.0751 - acc: 0.7413 - val_loss: 1.6860 - val_acc: 0.5203\n",
      "Epoch 7/10\n",
      "344/344 [==============================] - 0s 604us/step - loss: 0.9776 - acc: 0.7587 - val_loss: 1.6571 - val_acc: 0.5473\n",
      "Epoch 8/10\n",
      "344/344 [==============================] - 0s 593us/step - loss: 0.8990 - acc: 0.7703 - val_loss: 1.6291 - val_acc: 0.5405\n",
      "Epoch 9/10\n",
      "344/344 [==============================] - 0s 606us/step - loss: 0.8293 - acc: 0.7849 - val_loss: 1.6260 - val_acc: 0.5405\n",
      "Epoch 10/10\n",
      "344/344 [==============================] - 0s 609us/step - loss: 0.7652 - acc: 0.8023 - val_loss: 1.6135 - val_acc: 0.5338\n",
      "[[ 2  0  0  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0 96  1  0  0  1  0  0  1  0  0  0  1  0  0]\n",
      " [ 0  0 58  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  1  0  3  0  0  0  0  2  0  0  0  1  0  0]\n",
      " [ 0  1  0  0  3  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 23  0  0  0  0  0  0  3  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  2  0  0]\n",
      " [ 0  2  0  0  0  0  0  0  4  0  0  0  1  0  2]\n",
      " [ 0  7  0  0  1  2  0  0 20  0  0  0  5  0  1]\n",
      " [ 0  0  0  0  0  2  0  0  0  0  0  0  2  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  0  0  0  1]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  3  1  0  0  1  0  0  1  0  0  0 46  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  1  0  0  0  1 15  0]\n",
      " [ 0  3  0  0  0  0  0  0  1  0  0  0  0  0 14]]\n",
      "344/344 [==============================] - 0s 203us/step\n",
      "[0.7200813695441844, 0.8168604637301246]\n",
      "[[ 0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 36  0  0  0  0  0  0  0  2  0  0  3  0  0]\n",
      " [ 0  0 19  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0  0  0  7  0  1]\n",
      " [ 0  0  0  0  0  3  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  3  0  0  0  1  0  0  1  0  0]\n",
      " [ 0  4  0  0  0  2  0  0  0  2  0  0  3  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  2  0  0]\n",
      " [ 0  5  4  0  0  3  0  0  0  2  0  0 12  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  4  0  0]\n",
      " [ 0  2  0  0  0  0  0  0  0  2  0  0  1  0  1]]\n",
      "148/148 [==============================] - 0s 202us/step\n",
      "[1.6135363449921478, 0.5337837805619111]\n"
     ]
    }
   ],
   "source": [
    "#User Choice/Control-Personal Information Type\n",
    "\n",
    "padded_docs = pad_sequences(User_Choice_Control_Personal_Information_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(User_Choice_Control_Personal_Information_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, User_Choice_Control_Personal_Information_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'user_choice_control_personal_information_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('user_choice_control_personal_information_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 561 samples, validate on 241 samples\n",
      "Epoch 1/10\n",
      "561/561 [==============================] - 2s 4ms/step - loss: 2.1347 - acc: 0.3369 - val_loss: 1.8039 - val_acc: 0.4149\n",
      "Epoch 2/10\n",
      "561/561 [==============================] - 0s 542us/step - loss: 1.3947 - acc: 0.6043 - val_loss: 1.6354 - val_acc: 0.4979\n",
      "Epoch 3/10\n",
      "561/561 [==============================] - 0s 550us/step - loss: 1.0863 - acc: 0.7077 - val_loss: 1.5889 - val_acc: 0.4772\n",
      "Epoch 4/10\n",
      "561/561 [==============================] - 0s 567us/step - loss: 0.8864 - acc: 0.7718 - val_loss: 1.5655 - val_acc: 0.4730\n",
      "Epoch 5/10\n",
      "561/561 [==============================] - 0s 630us/step - loss: 0.7359 - acc: 0.8146 - val_loss: 1.5584 - val_acc: 0.4813\n",
      "Epoch 6/10\n",
      "561/561 [==============================] - 0s 560us/step - loss: 0.6252 - acc: 0.8342 - val_loss: 1.5640 - val_acc: 0.4689\n",
      "Epoch 7/10\n",
      "561/561 [==============================] - 0s 613us/step - loss: 0.5412 - acc: 0.8592 - val_loss: 1.5922 - val_acc: 0.4813\n",
      "Epoch 8/10\n",
      "561/561 [==============================] - 0s 541us/step - loss: 0.4741 - acc: 0.8841 - val_loss: 1.6342 - val_acc: 0.4730\n",
      "Epoch 9/10\n",
      "561/561 [==============================] - 0s 548us/step - loss: 0.4195 - acc: 0.8930 - val_loss: 1.6663 - val_acc: 0.4647\n",
      "Epoch 10/10\n",
      "561/561 [==============================] - 0s 545us/step - loss: 0.3750 - acc: 0.9109 - val_loss: 1.6775 - val_acc: 0.4855\n",
      "[[ 70   0   0   0   0   3   0   0   1   0   0]\n",
      " [  0  76   0   0   0   2   0   1   0   0   0]\n",
      " [  0   2   4   0   0   1   0   0   0   0   0]\n",
      " [  2   0   0  71   0   4   0   0   0   1   3]\n",
      " [  1   0   0   0   7   0   0   0   0   0   0]\n",
      " [  1   1   0   1   0 176   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   2   0   0   0   0]\n",
      " [  0   1   0   1   0   6   0  20   0   1   3]\n",
      " [  0   0   0   0   0   1   0   0  19   0   1]\n",
      " [  0   0   0   0   0   0   0   1   0  15   0]\n",
      " [  0   0   0   0   0   5   0   0   0   0  57]]\n",
      "561/561 [==============================] - 0s 206us/step\n",
      "[0.3430140250081897, 0.9215686274509803]\n",
      "[[ 9  3  0  8  0  4  0  3  2  1  2]\n",
      " [ 3 20  0  1  0  6  0  0  0  0  4]\n",
      " [ 1  1  0  1  0  1  0  0  0  0  1]\n",
      " [ 9  1  0 12  0  5  0  0  1  1  5]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  2]\n",
      " [ 3  2  0  3  0 57  0  0  1  0  4]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 2  0  0  1  0  9  0  0  0  0  4]\n",
      " [ 1  4  0  4  0  1  0  1  3  0  1]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  2  0  6  0  4  0  1  0  0 16]]\n",
      "241/241 [==============================] - 0s 203us/step\n",
      "[1.6775373863481386, 0.48547718015449176]\n"
     ]
    }
   ],
   "source": [
    "#User Choice/Control-Purpose\n",
    "\n",
    "padded_docs = pad_sequences(User_Choice_Control_Purpose.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(User_Choice_Control_Purpose.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, User_Choice_Control_Purpose.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'user_choice_control_purpose_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('user_choice_control_purpose_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 43 samples\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 1.3843 - acc: 0.2800 - val_loss: 1.2986 - val_acc: 0.6279\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 636us/step - loss: 1.1785 - acc: 0.7700 - val_loss: 1.1735 - val_acc: 0.7442\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.9954 - acc: 0.7500 - val_loss: 1.0449 - val_acc: 0.7442\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 622us/step - loss: 0.8326 - acc: 0.7500 - val_loss: 0.9452 - val_acc: 0.7442\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 677us/step - loss: 0.7078 - acc: 0.7400 - val_loss: 0.8920 - val_acc: 0.7442\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 679us/step - loss: 0.6271 - acc: 0.7400 - val_loss: 0.8727 - val_acc: 0.7442\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 683us/step - loss: 0.5511 - acc: 0.8000 - val_loss: 0.8845 - val_acc: 0.7442\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 590us/step - loss: 0.4912 - acc: 0.8700 - val_loss: 0.9169 - val_acc: 0.7442\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 678us/step - loss: 0.4442 - acc: 0.9200 - val_loss: 0.9461 - val_acc: 0.6977\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 585us/step - loss: 0.3995 - acc: 0.9200 - val_loss: 0.9673 - val_acc: 0.6744\n",
      "[[18  1  2  0]\n",
      " [ 0 10  4  0]\n",
      " [ 0  0 53  0]\n",
      " [ 1  0  1 10]]\n",
      "100/100 [==============================] - 0s 313us/step\n",
      "[0.3603315758705139, 0.91]\n",
      "[[ 3  1  6  0]\n",
      " [ 0  0  2  0]\n",
      " [ 1  0 25  3]\n",
      " [ 0  0  1  1]]\n",
      "43/43 [==============================] - 0s 264us/step\n",
      "[0.9673040897347206, 0.6744186018788537]\n"
     ]
    }
   ],
   "source": [
    "#User Choice/Control-User Type\n",
    "\n",
    "padded_docs = pad_sequences(User_Choice_Control_User_Type.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "onehot=encode.fit(np.array(User_Choice_Control_User_Type.label).reshape(-1,1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, User_Choice_Control_User_Type.label, test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "onehotlabels_train = encode.transform(np.array(y_train).reshape(-1,1))\n",
    "onehotlabels_test = encode.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(onehotlabels_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, onehotlabels_train, epochs=10, batch_size=100, validation_data=(x_test, onehotlabels_test))\n",
    "\n",
    "filename = 'user_choice_control_user_type_value_encode.sav'\n",
    "pickle.dump(encode, open(filename, 'wb'))\n",
    "\n",
    "model.save('user_choice_control_user_type_value.h5')\n",
    "\n",
    "y_pred_train=model.predict_classes(x_train)\n",
    "y_train_label=np.argmax(onehotlabels_train, axis = 1)\n",
    "print(confusion_matrix(y_train_label,y_pred_train))\n",
    "print(model.evaluate(x_train, onehotlabels_train))\n",
    "y_pred_test=model.predict_classes(x_test)\n",
    "y_test_label=np.argmax(onehotlabels_test, axis = 1)\n",
    "print(confusion_matrix(y_test_label,y_pred_test))\n",
    "print(model.evaluate(x_test, onehotlabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
