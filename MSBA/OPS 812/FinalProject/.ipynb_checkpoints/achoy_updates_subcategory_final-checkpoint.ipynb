{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations=glob.glob('data/OPP-115/annotations/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_list=[]\n",
    "for file in annotations:\n",
    "    annotations_list.append(pd.read_csv(file, header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot=pd.concat(annotations_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot.columns = ['annotation_ID', 'batch_ID', 'annotator_ID', 'policy_ID', 'segment_ID','category_name',\n",
    "            'attribute_value_pairs','date','policy_URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot['dict'] = annot.attribute_value_pairs.apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_ID</th>\n",
       "      <th>batch_ID</th>\n",
       "      <th>annotator_ID</th>\n",
       "      <th>policy_ID</th>\n",
       "      <th>segment_ID</th>\n",
       "      <th>category_name</th>\n",
       "      <th>attribute_value_pairs</th>\n",
       "      <th>date</th>\n",
       "      <th>policy_URL</th>\n",
       "      <th>dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13160</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>{\"Other Type\": {\"endIndexInSegment\": 575, \"sta...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Other Type': {'endIndexInSegment': 575, 'sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13161</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13162</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13163</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13164</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   annotation_ID                                       batch_ID  annotator_ID  \\\n",
       "0          13160  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "1          13161  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "2          13162  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "3          13163  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "4          13164  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "\n",
       "   policy_ID  segment_ID               category_name  \\\n",
       "0       3828           0                       Other   \n",
       "1       3828           1  First Party Collection/Use   \n",
       "2       3828           1  First Party Collection/Use   \n",
       "3       3828           1  First Party Collection/Use   \n",
       "4       3828           1  First Party Collection/Use   \n",
       "\n",
       "                               attribute_value_pairs    date  \\\n",
       "0  {\"Other Type\": {\"endIndexInSegment\": 575, \"sta...  5/7/15   \n",
       "1  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "2  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "3  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "4  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "\n",
       "                                          policy_URL  \\\n",
       "0  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "1  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "2  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "3  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "4  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "\n",
       "                                                dict  \n",
       "0  {'Other Type': {'endIndexInSegment': 575, 'sta...  \n",
       "1  {'Collection Mode': {'endIndexInSegment': -1, ...  \n",
       "2  {'Collection Mode': {'endIndexInSegment': -1, ...  \n",
       "3  {'Collection Mode': {'endIndexInSegment': -1, ...  \n",
       "4  {'Collection Mode': {'endIndexInSegment': -1, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in annot['dict']:\n",
    "    for key, value in x.items():\n",
    "        value.pop('endIndexInSegment', None)\n",
    "        value.pop('startIndexInSegment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[]\n",
    "category=[]\n",
    "subcat=[]\n",
    "label=[]\n",
    "counter=0\n",
    "for i,x in enumerate(annot['dict']):\n",
    "    for key, value in x.items():\n",
    "        subcat.append(key)\n",
    "        if value.get('selectedText')==None:\n",
    "            text.append('noSelectedText')\n",
    "        else:\n",
    "            text.append(value.get('selectedText'))  \n",
    "        category.append(annot['category_name'][i])\n",
    "        for k, v in value.items():\n",
    "            if k=='value':\n",
    "                label.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=list(zip(text, category, subcat, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(d, columns=['text', 'category', 'subcategory', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Effective Date: May 7, 2015 Kraft Site Privacy...</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other Type</td>\n",
       "      <td>Introductory/Generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>noSelectedText</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Collection Mode</td>\n",
       "      <td>not-selected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>collec</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Choice Scope</td>\n",
       "      <td>Collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>register on our website or participate in our ...</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Action First-Party</td>\n",
       "      <td>Collect on website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>personally-identifiable information, such as</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Personal Information Type</td>\n",
       "      <td>Generic personal information</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Effective Date: May 7, 2015 Kraft Site Privacy...   \n",
       "1                                     noSelectedText   \n",
       "2                                             collec   \n",
       "3  register on our website or participate in our ...   \n",
       "4       personally-identifiable information, such as   \n",
       "\n",
       "                     category                subcategory  \\\n",
       "0                       Other                 Other Type   \n",
       "1  First Party Collection/Use            Collection Mode   \n",
       "2  First Party Collection/Use               Choice Scope   \n",
       "3  First Party Collection/Use         Action First-Party   \n",
       "4  First Party Collection/Use  Personal Information Type   \n",
       "\n",
       "                          label  \n",
       "0          Introductory/Generic  \n",
       "1                  not-selected  \n",
       "2                    Collection  \n",
       "3            Collect on website  \n",
       "4  Generic personal information  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cat_sub'] = data['category'] +'-'+ data['subcategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cat_sub_lab'] = data['category'] +'-'+ data['subcategory'] +'-'+ data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "data['text'] = data['text'].apply(lambda x : remove_punct(x.lower()))\n",
    "\n",
    "data = data[data['text']!='null']\n",
    "data = data[data['text']!='noselectedtext']\n",
    "data = data[data['text']!='']\n",
    "data = data[data['text']!=' ']\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "ds = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36717, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "      <th>cat_sub</th>\n",
       "      <th>cat_sub_lab</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Data Retention</th>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data Security</th>\n",
       "      <td>808</td>\n",
       "      <td>808</td>\n",
       "      <td>808</td>\n",
       "      <td>808</td>\n",
       "      <td>808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Do Not Track</th>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First Party Collection/Use</th>\n",
       "      <td>15346</td>\n",
       "      <td>15346</td>\n",
       "      <td>15346</td>\n",
       "      <td>15346</td>\n",
       "      <td>15346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>International and Specific Audiences</th>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>2759</td>\n",
       "      <td>2759</td>\n",
       "      <td>2759</td>\n",
       "      <td>2759</td>\n",
       "      <td>2759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Policy Change</th>\n",
       "      <td>932</td>\n",
       "      <td>932</td>\n",
       "      <td>932</td>\n",
       "      <td>932</td>\n",
       "      <td>932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Third Party Sharing/Collection</th>\n",
       "      <td>10655</td>\n",
       "      <td>10655</td>\n",
       "      <td>10655</td>\n",
       "      <td>10655</td>\n",
       "      <td>10655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Access, Edit and Deletion</th>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Choice/Control</th>\n",
       "      <td>3798</td>\n",
       "      <td>3798</td>\n",
       "      <td>3798</td>\n",
       "      <td>3798</td>\n",
       "      <td>3798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text  subcategory  label  cat_sub  \\\n",
       "category                                                                   \n",
       "Data Retention                          676          676    676      676   \n",
       "Data Security                           808          808    808      808   \n",
       "Do Not Track                             67           67     67       67   \n",
       "First Party Collection/Use            15346        15346  15346    15346   \n",
       "International and Specific Audiences    582          582    582      582   \n",
       "Other                                  2759         2759   2759     2759   \n",
       "Policy Change                           932          932    932      932   \n",
       "Third Party Sharing/Collection        10655        10655  10655    10655   \n",
       "User Access, Edit and Deletion         1094         1094   1094     1094   \n",
       "User Choice/Control                    3798         3798   3798     3798   \n",
       "\n",
       "                                      cat_sub_lab  \n",
       "category                                           \n",
       "Data Retention                                676  \n",
       "Data Security                                 808  \n",
       "Do Not Track                                   67  \n",
       "First Party Collection/Use                  15346  \n",
       "International and Specific Audiences          582  \n",
       "Other                                        2759  \n",
       "Policy Change                                 932  \n",
       "Third Party Sharing/Collection              10655  \n",
       "User Access, Edit and Deletion               1094  \n",
       "User Choice/Control                          3798  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.groupby('category').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.groupby(['category','subcategory']).count().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(259, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.groupby(['category','subcategory','label']).count().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "ds['tokenized'] = ds['text'].apply(lambda x : re.split(' ', x))\n",
    "# ds['tokenized_text']=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    " \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-20 18:00:46,637 : INFO : loading Word2Vec object from word2vec.model\n",
      "/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-09-20 18:00:46,678 : INFO : loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "2019-09-20 18:00:46,678 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-09-20 18:00:46,679 : INFO : loading vocabulary recursively from word2vec.model.vocabulary.* with mmap=None\n",
      "2019-09-20 18:00:46,680 : INFO : loading trainables recursively from word2vec.model.trainables.* with mmap=None\n",
      "2019-09-20 18:00:46,684 : INFO : setting ignored attribute cum_table to None\n",
      "2019-09-20 18:00:46,691 : INFO : loaded word2vec.model\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# encode = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {k: v.index for k, v in word_vectors.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_word(new_word, new_vector, new_index, embedding_matrix, word2id):\n",
    "    embedding_matrix = np.insert(embedding_matrix, [new_index], [new_vector], axis=0)\n",
    "    \n",
    "    word2id = {word: (index+1) if index >= new_index else index for word, index in word2id.items()}\n",
    "    word2id[new_word] = new_index\n",
    "    return embedding_matrix, word2id\n",
    "\n",
    "UNK_INDEX = 0\n",
    "UNK_TOKEN = 'UNK'\n",
    "\n",
    "embedding_matrix = word_vectors.vectors\n",
    "unk_vector = embedding_matrix.mean(0)\n",
    "embedding_matrix, word2id = add_new_word(UNK_TOKEN, unk_vector, UNK_INDEX, embedding_matrix, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.insert(embedding_matrix, len(embedding_matrix), [np.zeros((100,))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created. Percentage of unknown words: 7006.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36717,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_int_data(token_text, word2id):\n",
    "    x = []\n",
    "    unk_count = 0\n",
    "    for item in token_text:\n",
    "        temp=[]\n",
    "        x.append(temp)\n",
    "        for word in item:\n",
    "            if word in word2id:\n",
    "                temp.append(word2id.get(word))\n",
    "            else:\n",
    "                temp.append(UNK_INDEX)\n",
    "                unk_count += 1\n",
    "    print('Data created. Percentage of unknown words: %.3f' % (unk_count))\n",
    "    return np.array(x)\n",
    "\n",
    "x=get_int_data(ds.tokenized, word2id)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "ds['enumerated_text']=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "      <th>cat_sub</th>\n",
       "      <th>cat_sub_lab</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>enumerated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>effective date may 7 2015 kraft site privacy n...</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other Type</td>\n",
       "      <td>Introductory/Generic</td>\n",
       "      <td>Other-Other Type</td>\n",
       "      <td>Other-Other Type-Introductory/Generic</td>\n",
       "      <td>[effective, date, may, 7, 2015, kraft, site, p...</td>\n",
       "      <td>[553, 244, 11, 2150, 839, 1332, 35, 26, 139, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>collec</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Choice Scope</td>\n",
       "      <td>Collection</td>\n",
       "      <td>First Party Collection/Use-Choice Scope</td>\n",
       "      <td>First Party Collection/Use-Choice Scope-Collec...</td>\n",
       "      <td>[collec]</td>\n",
       "      <td>[2151]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>register on our website or participate in our ...</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Action First-Party</td>\n",
       "      <td>Collect on website</td>\n",
       "      <td>First Party Collection/Use-Action First-Party</td>\n",
       "      <td>First Party Collection/Use-Action First-Party-...</td>\n",
       "      <td>[register, on, our, website, or, participate, ...</td>\n",
       "      <td>[204, 19, 10, 52, 6, 212, 13, 10, 168, 3, 363]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>personallyidentifiable information such as</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Personal Information Type</td>\n",
       "      <td>Generic personal information</td>\n",
       "      <td>First Party Collection/Use-Personal Informatio...</td>\n",
       "      <td>First Party Collection/Use-Personal Informatio...</td>\n",
       "      <td>[personallyidentifiable, information, such, as]</td>\n",
       "      <td>[630, 5, 31, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if you choose to register on our website or pa...</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Choice Type</td>\n",
       "      <td>Opt-in</td>\n",
       "      <td>First Party Collection/Use-Choice Type</td>\n",
       "      <td>First Party Collection/Use-Choice Type-Opt-in</td>\n",
       "      <td>[if, you, choose, to, register, on, our, websi...</td>\n",
       "      <td>[32, 4, 105, 1, 204, 19, 10, 52, 6, 212, 13, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  effective date may 7 2015 kraft site privacy n...   \n",
       "1                                             collec   \n",
       "2  register on our website or participate in our ...   \n",
       "3         personallyidentifiable information such as   \n",
       "4  if you choose to register on our website or pa...   \n",
       "\n",
       "                     category                subcategory  \\\n",
       "0                       Other                 Other Type   \n",
       "1  First Party Collection/Use               Choice Scope   \n",
       "2  First Party Collection/Use         Action First-Party   \n",
       "3  First Party Collection/Use  Personal Information Type   \n",
       "4  First Party Collection/Use                Choice Type   \n",
       "\n",
       "                          label  \\\n",
       "0          Introductory/Generic   \n",
       "1                    Collection   \n",
       "2            Collect on website   \n",
       "3  Generic personal information   \n",
       "4                        Opt-in   \n",
       "\n",
       "                                             cat_sub  \\\n",
       "0                                   Other-Other Type   \n",
       "1            First Party Collection/Use-Choice Scope   \n",
       "2      First Party Collection/Use-Action First-Party   \n",
       "3  First Party Collection/Use-Personal Informatio...   \n",
       "4             First Party Collection/Use-Choice Type   \n",
       "\n",
       "                                         cat_sub_lab  \\\n",
       "0              Other-Other Type-Introductory/Generic   \n",
       "1  First Party Collection/Use-Choice Scope-Collec...   \n",
       "2  First Party Collection/Use-Action First-Party-...   \n",
       "3  First Party Collection/Use-Personal Informatio...   \n",
       "4      First Party Collection/Use-Choice Type-Opt-in   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [effective, date, may, 7, 2015, kraft, site, p...   \n",
       "1                                           [collec]   \n",
       "2  [register, on, our, website, or, participate, ...   \n",
       "3    [personallyidentifiable, information, such, as]   \n",
       "4  [if, you, choose, to, register, on, our, websi...   \n",
       "\n",
       "                                     enumerated_text  \n",
       "0  [553, 244, 11, 2150, 839, 1332, 35, 26, 139, 2...  \n",
       "1                                             [2151]  \n",
       "2     [204, 19, 10, 52, 6, 212, 13, 10, 168, 3, 363]  \n",
       "3                                   [630, 5, 31, 23]  \n",
       "4  [32, 4, 105, 1, 204, 19, 10, 52, 6, 212, 13, 1...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "data_retention = ds[ds.category=='Data Retention']\n",
    "data_retention['Personal_Information_Type'] = data_retention['subcategory'].apply(lambda x: 1 if x=='Personal Information Type' else 0)\n",
    "data_retention['Retention_Period'] = data_retention['subcategory'].apply(lambda x: 1 if x=='Retention Period' else 0)\n",
    "data_retention['Retention_Purpose'] = data_retention['subcategory'].apply(lambda x: 1 if x=='Retention Purpose' else 0)\n",
    "\n",
    "first_party_collection_use = ds[ds.category=='First Party Collection/Use']\n",
    "first_party_collection_use['Action_First_Party'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Action First-Party' else 0)\n",
    "first_party_collection_use['Choice_Scope'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Choice Scope' else 0)\n",
    "first_party_collection_use['Choice_Type'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Choice Type' else 0)\n",
    "first_party_collection_use['Collection_Mode'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Collection Mode' else 0)\n",
    "first_party_collection_use['Does_Does_Not'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Does/Does Not' else 0)\n",
    "first_party_collection_use['Identifiability'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Identifiability' else 0)\n",
    "first_party_collection_use['Personal_Information_Type'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Personal Information Type' else 0)\n",
    "first_party_collection_use['Purpose'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Purpose' else 0)\n",
    "first_party_collection_use['User_Type'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='User Type' else 0)\n",
    "\n",
    "policy_change = ds[ds.category=='Policy Change']\n",
    "policy_change['Change_Type'] = policy_change['subcategory'].apply(lambda x: 1 if x=='Change Type' else 0)\n",
    "policy_change['Notification_Type'] = policy_change['subcategory'].apply(lambda x: 1 if x=='Notification Type' else 0)\n",
    "policy_change['User_Choice'] = policy_change['subcategory'].apply(lambda x: 1 if x=='User Choice' else 0)\n",
    "\n",
    "third_party_sharing_collection = ds[ds.category=='Third Party Sharing/Collection']\n",
    "third_party_sharing_collection['Action_Third_Party'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Action Third Party' else 0)\n",
    "third_party_sharing_collection['Choice_Scope'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Choice Scope' else 0)\n",
    "third_party_sharing_collection['Choice_Type'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Choice Type' else 0)\n",
    "third_party_sharing_collection['Does_Does_Not'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Does/Does Not' else 0)\n",
    "third_party_sharing_collection['Identifiability'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Identifiability' else 0)\n",
    "third_party_sharing_collection['Personal_Information_Type'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Personal Information Type' else 0)\n",
    "third_party_sharing_collection['Purpose'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Purpose' else 0)\n",
    "third_party_sharing_collection['Third_Party_Entity'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Third Party Entity' else 0)\n",
    "third_party_sharing_collection['User_Type'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='User Type' else 0)\n",
    "\n",
    "user_access_edit_deletion = ds[ds.category=='User Access, Edit and Deletion']\n",
    "user_access_edit_deletion['Access_Scope'] = user_access_edit_deletion['subcategory'].apply(lambda x: 1 if x=='Access Scope' else 0)\n",
    "user_access_edit_deletion['Access_Type'] = user_access_edit_deletion['subcategory'].apply(lambda x: 1 if x=='Access Type' else 0)\n",
    "user_access_edit_deletion['User_Type'] = user_access_edit_deletion['subcategory'].apply(lambda x: 1 if x=='User Type' else 0)\n",
    "\n",
    "user_choice_control = ds[ds.category=='User Choice/Control']\n",
    "user_choice_control['Choice_Scope'] = user_choice_control['subcategory'].apply(lambda x: 1 if x=='Choice Scope' else 0)\n",
    "user_choice_control['Choice_Type'] = user_choice_control['subcategory'].apply(lambda x: 1 if x=='Choice Type' else 0)\n",
    "user_choice_control['Personal_Information_Type'] = user_choice_control['subcategory'].apply(lambda x: 1 if x=='Personal Information Type' else 0)\n",
    "user_choice_control['Purpose'] = user_choice_control['subcategory'].apply(lambda x: 1 if x=='Purpose' else 0)\n",
    "user_choice_control['User_Type'] = user_choice_control['subcategory'].apply(lambda x: 1 if x=='User Type' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_party_collection_use.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Activation, Flatten, Dropout\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "max_length = max(ds.enumerated_text.apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA RETENTION MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-20 18:00:53,736 : WARNING : From /anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-09-20 18:00:54,040 : WARNING : From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[325  18]\n",
      " [  1 129]]\n",
      "0.9314079422382673\n",
      "0.9923076923076923\n",
      "[[122  23]\n",
      " [  6  52]]\n",
      "0.7819548872180451\n",
      "0.896551724137931\n"
     ]
    }
   ],
   "source": [
    "# Personal Information Type Model (Data Retention)\n",
    "\n",
    "padded_docs = pad_sequences(data_retention.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, data_retention.Personal_Information_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 3.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('data_retention_personal_information_type.h5')\n",
    "model = load_model('data_retention_personal_information_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[242  69]\n",
      " [  3 159]]\n",
      "0.8153846153846153\n",
      "0.9814814814814815\n",
      "[[90 41]\n",
      " [15 57]]\n",
      "0.6705882352941176\n",
      "0.7916666666666666\n"
     ]
    }
   ],
   "source": [
    "# Retention Period Model (Data Retention)\n",
    "\n",
    "padded_docs = pad_sequences(data_retention.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, data_retention.Retention_Period, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 5.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('data_retention_rentention_period.h5')\n",
    "\n",
    "model=load_model('data_retention_rentention_period.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-20 18:00:57,250 : WARNING : From /anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[150 142]\n",
      " [  0 181]]\n",
      "0.7182539682539683\n",
      "1.0\n",
      "[[50 80]\n",
      " [ 8 65]]\n",
      "0.5963302752293578\n",
      "0.8904109589041096\n"
     ]
    }
   ],
   "source": [
    "# Retention Purpose Model (Data Retention)\n",
    "\n",
    "padded_docs = pad_sequences(data_retention.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, data_retention.Retention_Purpose, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 10.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('data_retention_retention_purpose.h5')\n",
    "model=load_model('data_retention_retention_purpose.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIRST PARTY COLLECTION/USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5103 3698]\n",
      " [  12 1929]]\n",
      "0.5097780126849895\n",
      "0.9938176197836167\n",
      "[[1971 1841]\n",
      " [ 111  681]]\n",
      "0.41098370549185276\n",
      "0.8598484848484849\n"
     ]
    }
   ],
   "source": [
    "# Action First-Party Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Action_First_Party, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 20.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=10, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('first_party_collection_use_action_first_party.h5')\n",
    "model=load_model('first_party_collection_use_action_first_party.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7130 3263]\n",
      " [   0  349]]\n",
      "0.1762181267356728\n",
      "1.0\n",
      "[[2843 1582]\n",
      " [  46  133]]\n",
      "0.14044350580781415\n",
      "0.7430167597765364\n"
     ]
    }
   ],
   "source": [
    "# Choice Scope Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Choice_Scope, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 75.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=10, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('first_party_collection_use_choice_scope.h5')\n",
    "\n",
    "model = load_model('first_party_collection_use_choice_scope.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8076 2037]\n",
      " [   0  629]]\n",
      "0.3817905918057663\n",
      "1.0\n",
      "[[3258 1038]\n",
      " [  46  262]]\n",
      "0.32587064676616917\n",
      "0.8506493506493507\n"
     ]
    }
   ],
   "source": [
    "# Choice Type Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Choice_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 50.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=10, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('first_party_collection_use_choice_type.h5')\n",
    "model = load_model('first_party_collection_use_choice_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5134 4565]\n",
      " [   5 1038]]\n",
      "0.31236834185976525\n",
      "0.9952061361457335\n",
      "[[1952 2167]\n",
      " [  81  404]]\n",
      "0.2643979057591623\n",
      "0.8329896907216495\n"
     ]
    }
   ],
   "source": [
    "# Collection Mode Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Collection_Mode, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 50.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=10, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('first_party_collection_collection_mode.h5')\n",
    "model = load_model('first_party_collection_collection_mode.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6034 3787]\n",
      " [   5  916]]\n",
      "0.32574679943101\n",
      "0.99457111834962\n",
      "[[2408 1809]\n",
      " [  42  345]]\n",
      "0.2715466351829988\n",
      "0.8914728682170543\n"
     ]
    }
   ],
   "source": [
    "# Does/Does Not Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Does_Does_Not, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 50.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=10, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('first_party_collection_use_does_does_not.h5')\n",
    "model = load_model('first_party_collection_use_does_does_not.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8821 1466]\n",
      " [   0  455]]\n",
      "0.382996632996633\n",
      "1.0\n",
      "[[3642  781]\n",
      " [  43  138]]\n",
      "0.2509090909090909\n",
      "0.7624309392265194\n"
     ]
    }
   ],
   "source": [
    "# Identifiability Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Identifiability, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 75.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=10, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('first_party_collection_use_identifiability.h5')\n",
    "model = load_model('first_party_collection_use_identifiability.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7712  803]\n",
      " [  39 2188]]\n",
      "0.8386354925258719\n",
      "0.9824876515491693\n",
      "[[3176  536]\n",
      " [ 184  708]]\n",
      "0.6629213483146067\n",
      "0.7937219730941704\n"
     ]
    }
   ],
   "source": [
    "# Personal Information Type Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Personal_Information_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 5.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('first_party_collection_use_personal_information_type.h5')\n",
    "model = load_model('first_party_collection_use_personal_information_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7156  648]\n",
      " [  55 2883]]\n",
      "0.8913278713866132\n",
      "0.9812797821647379\n",
      "[[2842  486]\n",
      " [ 225 1051]]\n",
      "0.7472449342339139\n",
      "0.8236677115987461\n"
     ]
    }
   ],
   "source": [
    "# Purpose Model (First Party Collection/Use)\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Activation, Flatten\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Purpose, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 5.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('first_party_collection_use_purpose.h5')\n",
    "\n",
    "model = load_model('first_party_collection_use_purpose.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9748  755]\n",
      " [   3  236]]\n",
      "0.383739837398374\n",
      "0.9874476987447699\n",
      "[[4079  421]\n",
      " [  28   76]]\n",
      "0.2529118136439268\n",
      "0.7307692307692307\n"
     ]
    }
   ],
   "source": [
    "# User Type Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.User_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 50.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('first_party_collection_use_user_type.h5')\n",
    "model = load_model('first_party_collection_use_user_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POLICY CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[318  90]\n",
      " [  3 241]]\n",
      "0.8382608695652175\n",
      "0.9877049180327869\n",
      "[[120  63]\n",
      " [ 26  71]]\n",
      "0.6147186147186147\n",
      "0.7319587628865979\n"
     ]
    }
   ],
   "source": [
    "# Change Type Model (Policy Change)\n",
    "\n",
    "padded_docs = pad_sequences(policy_change.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, policy_change.Change_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 5.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('policy_change_change_type.h5')\n",
    "model = load_model('policy_change_change_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[273 113]\n",
      " [  0 266]]\n",
      "0.8248062015503876\n",
      "1.0\n",
      "[[ 73  70]\n",
      " [ 26 111]]\n",
      "0.6981132075471699\n",
      "0.8102189781021898\n"
     ]
    }
   ],
   "source": [
    "# Notification Type Model (Policy Change)\n",
    "\n",
    "padded_docs = pad_sequences(policy_change.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, policy_change.Notification_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 5.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('policy_change_notification_type.h5')\n",
    "\n",
    "model = load_model('policy_change_notification_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[365 145]\n",
      " [  0 142]]\n",
      "0.662004662004662\n",
      "1.0\n",
      "[[128 106]\n",
      " [ 12  34]]\n",
      "0.3655913978494624\n",
      "0.7391304347826086\n"
     ]
    }
   ],
   "source": [
    "# User Choice Model (Policy Change)\n",
    "\n",
    "padded_docs = pad_sequences(policy_change.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, policy_change.User_Choice, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 20.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=10, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('policy_change_user_choice.h5')\n",
    "\n",
    "model = load_model('policy_change_user_choice.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIRD PARTY SHARING/COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4548 1602]\n",
      " [   7 1301]]\n",
      "0.6179054856328663\n",
      "0.9946483180428135\n",
      "[[1784  832]\n",
      " [ 121  460]]\n",
      "0.4911906033101975\n",
      "0.7917383820998278\n"
     ]
    }
   ],
   "source": [
    "# Action Third-Party Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Action_Third_Party, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 20.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('third_party_sharing_collection_action_third_party.h5')\n",
    "model = load_model('third_party_sharing_collection_action_third_party.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5075 2140]\n",
      " [   1  242]]\n",
      "0.18438095238095234\n",
      "0.9958847736625515\n",
      "[[2142  956]\n",
      " [  22   77]]\n",
      "0.13604240282685512\n",
      "0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "# Choice Scope Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Choice_Scope, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 75.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('third_party_sharing_collection_choice_scope.h5')\n",
    "model = load_model('third_party_sharing_collection_choice_scope.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6804  185]\n",
      " [   6  463]]\n",
      "0.829006266786034\n",
      "0.9872068230277186\n",
      "[[2866  119]\n",
      " [  57  155]]\n",
      "0.6378600823045268\n",
      "0.7311320754716981\n"
     ]
    }
   ],
   "source": [
    "# Choice Type Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Choice_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 10.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('third_party_sharing_collection_choice_type.h5')\n",
    "model = load_model('third_party_sharing_collection_choice_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4789 2010]\n",
      " [  30  629]]\n",
      "0.3814432989690722\n",
      "0.9544764795144158\n",
      "[[1987  932]\n",
      " [  60  218]]\n",
      "0.30532212885154064\n",
      "0.7841726618705036\n"
     ]
    }
   ],
   "source": [
    "# Does/Does Not Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Does_Does_Not, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 10.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('third_party_sharing_collection_does_does_not.h5')\n",
    "model = load_model('third_party_sharing_collection_does_does_not.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6037 1147]\n",
      " [   4  270]]\n",
      "0.31933767001774105\n",
      "0.9854014598540146\n",
      "[[2559  526]\n",
      " [  25   87]]\n",
      "0.24000000000000002\n",
      "0.7767857142857143\n"
     ]
    }
   ],
   "source": [
    "# Identifiability Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Identifiability, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 25.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('third_party_sharing_collection_identifiability.h5')\n",
    "model = load_model('third_party_sharing_collection_identifiability.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5950  478]\n",
      " [  24 1006]]\n",
      "0.8003182179793159\n",
      "0.9766990291262136\n",
      "[[2484  280]\n",
      " [  71  362]]\n",
      "0.6734883720930233\n",
      "0.836027713625866\n"
     ]
    }
   ],
   "source": [
    "# Personal Information Type Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Personal_Information_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 5.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('third_party_sharing_collection_personal_information_type.h5')\n",
    "model = load_model('third_party_sharing_collection_personal_information_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5087  451]\n",
      " [  46 1874]]\n",
      "0.8829210836277974\n",
      "0.9760416666666667\n",
      "[[1997  393]\n",
      " [ 145  662]]\n",
      "0.7110633727175081\n",
      "0.8203221809169765\n"
     ]
    }
   ],
   "source": [
    "# Purpose Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Purpose, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 5.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('third_party_sharing_collection_purpose.h5')\n",
    "model = load_model('third_party_sharing_collection_purpose.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5501  503]\n",
      " [  70 1384]]\n",
      "0.8284944627357079\n",
      "0.951856946354883\n",
      "[[2194  363]\n",
      " [ 165  475]]\n",
      "0.6427604871447903\n",
      "0.7421875\n"
     ]
    }
   ],
   "source": [
    "# Third Part Entity Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Third_Party_Entity, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 5.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('third_party_sharing_collection_third_party_entity.h5')\n",
    "model = load_model('third_party_sharing_collection_third_party_entity.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5544  460]\n",
      " [1399   55]]\n",
      "0.055865921787709494\n",
      "0.03782668500687758\n",
      "[[2342  215]\n",
      " [ 613   27]]\n",
      "0.061224489795918366\n",
      "0.0421875\n"
     ]
    }
   ],
   "source": [
    "# User Type Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.User_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 40.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('third_party_sharing_collection_user_type.h5')\n",
    "model = load_model('third_party_sharing_collection_user_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER ACCESS, EDIT, AND DELETION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[375  96]\n",
      " [ 13 281]]\n",
      "0.8375558867362145\n",
      "0.95578231292517\n",
      "[[135  67]\n",
      " [ 35  92]]\n",
      "0.6433566433566434\n",
      "0.7244094488188977\n"
     ]
    }
   ],
   "source": [
    "# Access Scope Model (User Access, Edit and Deletion)\n",
    "\n",
    "padded_docs = pad_sequences(user_access_edit_deletion.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_access_edit_deletion.Access_Scope, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 5.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('user_access_edit_deletion_access_scope.h5')\n",
    "model = load_model('user_access_edit_deletion_access_scope.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[304  98]\n",
      " [  4 359]]\n",
      "0.875609756097561\n",
      "0.9889807162534435\n",
      "[[114  64]\n",
      " [ 27 124]]\n",
      "0.7315634218289085\n",
      "0.8211920529801324\n"
     ]
    }
   ],
   "source": [
    "# Access Type Model (User Access, Edit and Deletion)\n",
    "\n",
    "padded_docs = pad_sequences(user_access_edit_deletion.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_access_edit_deletion.Access_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 5.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('user_access_edit_deletion_access_type.h5')\n",
    "model = load_model('user_access_edit_deletion_access_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[585  72]\n",
      " [  5 103]]\n",
      "0.7279151943462898\n",
      "0.9537037037037037\n",
      "[[243  35]\n",
      " [ 11  40]]\n",
      "0.634920634920635\n",
      "0.7843137254901961\n"
     ]
    }
   ],
   "source": [
    "# User Type Model (User Access, Edit and Deletion)\n",
    "\n",
    "padded_docs = pad_sequences(user_access_edit_deletion.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_access_edit_deletion.User_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 5.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('user_access_edit_deletion_user_type.h5')\n",
    "model = load_model('user_access_edit_deletion_user_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER CHOICE/CONTROL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1027  876]\n",
      " [   7  748]]\n",
      "0.6288356452290879\n",
      "0.990728476821192\n",
      "[[288 508]\n",
      " [ 61 283]]\n",
      "0.49867841409691627\n",
      "0.8226744186046512\n"
     ]
    }
   ],
   "source": [
    "# Choice Scope Model (User Choice/Control)\n",
    "\n",
    "padded_docs = pad_sequences(user_choice_control.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_choice_control.Choice_Scope, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 10.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('user_choice_control_choice_scope.h5')\n",
    "model = load_model('user_choice_control_choice_scope.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1514  249]\n",
      " [   6  889]]\n",
      "0.8745696015740285\n",
      "0.9932960893854749\n",
      "[[574 199]\n",
      " [ 68 299]]\n",
      "0.6913294797687862\n",
      "0.8147138964577657\n"
     ]
    }
   ],
   "source": [
    "# Choice Type Model (User Choice/Control)\n",
    "\n",
    "padded_docs = pad_sequences(user_choice_control.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_choice_control.Choice_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 5.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('user_choice_control_choice_type.h5')\n",
    "model = load_model('user_choice_control_choice_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1736  580]\n",
      " [   0  342]]\n",
      "0.5411392405063291\n",
      "1.0\n",
      "[[648 342]\n",
      " [ 38 112]]\n",
      "0.3708609271523179\n",
      "0.7466666666666667\n"
     ]
    }
   ],
   "source": [
    "# Personal Information Type Model (User Choice/Control)\n",
    "\n",
    "padded_docs = pad_sequences(user_choice_control.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_choice_control.Personal_Information_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 25.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=10, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('user_choice_control_personal_information_type.h5')\n",
    "model = load_model('user_choice_control_personal_information_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1231  852]\n",
      " [   3  572]]\n",
      "0.5722861430715358\n",
      "0.9947826086956522\n",
      "[[435 478]\n",
      " [ 43 184]]\n",
      "0.41394825646794153\n",
      "0.8105726872246696\n"
     ]
    }
   ],
   "source": [
    "# Purpose Model (User Choice/Control)\n",
    "\n",
    "padded_docs = pad_sequences(user_choice_control.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_choice_control.Purpose, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 15.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('user_choice_control_purpose.h5')\n",
    "model = load_model('user_choice_control_purpose.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2453  114]\n",
      " [   1   90]]\n",
      "0.6101694915254238\n",
      "0.989010989010989\n",
      "[[1029   59]\n",
      " [  15   37]]\n",
      "0.5\n",
      "0.7115384615384616\n"
     ]
    }
   ],
   "source": [
    "# User Type Model (User Choice/Control)\n",
    "\n",
    "padded_docs = pad_sequences(user_choice_control.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_choice_control.User_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "# vocab_length = len(embedding_matrix)\n",
    "\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 15.}\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_length,\n",
    "#                    output_dim=100,\n",
    "#                    weights=[embedding_matrix],\n",
    "#                    input_length=padded_docs.shape[1]))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history=model.fit(x_train, y_train, epochs=10, batch_size=100,\\\n",
    "#                   validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "# model.save('user_choice_control_user_type.h5')\n",
    "model = load_model('user_choice_control_user_type.h5')\n",
    "\n",
    "print(confusion_matrix(y_train,model.predict_classes(x_train)))\n",
    "print(f1_score(y_train, model.predict_classes(x_train)))\n",
    "print(recall_score(y_train, model.predict_classes(x_train)))\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))\n",
    "print(recall_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
