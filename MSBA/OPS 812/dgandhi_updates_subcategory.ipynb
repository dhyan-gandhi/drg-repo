{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations=glob.glob('data/OPP-115/annotations/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_list=[]\n",
    "for file in annotations:\n",
    "    annotations_list.append(pd.read_csv(file, header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot=pd.concat(annotations_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot.columns = ['annotation_ID', 'batch_ID', 'annotator_ID', 'policy_ID', 'segment_ID','category_name',\n",
    "            'attribute_value_pairs','date','policy_URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot['dict'] = annot.attribute_value_pairs.apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_ID</th>\n",
       "      <th>batch_ID</th>\n",
       "      <th>annotator_ID</th>\n",
       "      <th>policy_ID</th>\n",
       "      <th>segment_ID</th>\n",
       "      <th>category_name</th>\n",
       "      <th>attribute_value_pairs</th>\n",
       "      <th>date</th>\n",
       "      <th>policy_URL</th>\n",
       "      <th>dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13160</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>{\"Other Type\": {\"endIndexInSegment\": 575, \"sta...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Other Type': {'endIndexInSegment': 575, 'sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13161</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13162</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13163</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13164</td>\n",
       "      <td>test_category_labeling_highlight_fordham_aaaa</td>\n",
       "      <td>121</td>\n",
       "      <td>3828</td>\n",
       "      <td>1</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>{\"Collection Mode\": {\"endIndexInSegment\": -1, ...</td>\n",
       "      <td>5/7/15</td>\n",
       "      <td>http://www.kraftrecipes.com/about/privacynotic...</td>\n",
       "      <td>{'Collection Mode': {'endIndexInSegment': -1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   annotation_ID                                       batch_ID  annotator_ID  \\\n",
       "0          13160  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "1          13161  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "2          13162  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "3          13163  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "4          13164  test_category_labeling_highlight_fordham_aaaa           121   \n",
       "\n",
       "   policy_ID  segment_ID               category_name  \\\n",
       "0       3828           0                       Other   \n",
       "1       3828           1  First Party Collection/Use   \n",
       "2       3828           1  First Party Collection/Use   \n",
       "3       3828           1  First Party Collection/Use   \n",
       "4       3828           1  First Party Collection/Use   \n",
       "\n",
       "                               attribute_value_pairs    date  \\\n",
       "0  {\"Other Type\": {\"endIndexInSegment\": 575, \"sta...  5/7/15   \n",
       "1  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "2  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "3  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "4  {\"Collection Mode\": {\"endIndexInSegment\": -1, ...  5/7/15   \n",
       "\n",
       "                                          policy_URL  \\\n",
       "0  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "1  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "2  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "3  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "4  http://www.kraftrecipes.com/about/privacynotic...   \n",
       "\n",
       "                                                dict  \n",
       "0  {'Other Type': {'endIndexInSegment': 575, 'sta...  \n",
       "1  {'Collection Mode': {'endIndexInSegment': -1, ...  \n",
       "2  {'Collection Mode': {'endIndexInSegment': -1, ...  \n",
       "3  {'Collection Mode': {'endIndexInSegment': -1, ...  \n",
       "4  {'Collection Mode': {'endIndexInSegment': -1, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in annot['dict']:\n",
    "    for key, value in x.items():\n",
    "        value.pop('endIndexInSegment', None)\n",
    "        value.pop('startIndexInSegment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[]\n",
    "category=[]\n",
    "subcat=[]\n",
    "label=[]\n",
    "counter=0\n",
    "for i,x in enumerate(annot['dict']):\n",
    "    for key, value in x.items():\n",
    "        subcat.append(key)\n",
    "        if value.get('selectedText')==None:\n",
    "            text.append('noSelectedText')\n",
    "        else:\n",
    "            text.append(value.get('selectedText'))  \n",
    "        category.append(annot['category_name'][i])\n",
    "        for k, v in value.items():\n",
    "            if k=='value':\n",
    "                label.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=list(zip(text, category, subcat, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(d, columns=['text', 'category', 'subcategory', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Effective Date: May 7, 2015 Kraft Site Privacy...</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other Type</td>\n",
       "      <td>Introductory/Generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>noSelectedText</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Collection Mode</td>\n",
       "      <td>not-selected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>collec</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Choice Scope</td>\n",
       "      <td>Collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>register on our website or participate in our ...</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Action First-Party</td>\n",
       "      <td>Collect on website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>personally-identifiable information, such as</td>\n",
       "      <td>First Party Collection/Use</td>\n",
       "      <td>Personal Information Type</td>\n",
       "      <td>Generic personal information</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Effective Date: May 7, 2015 Kraft Site Privacy...   \n",
       "1                                     noSelectedText   \n",
       "2                                             collec   \n",
       "3  register on our website or participate in our ...   \n",
       "4       personally-identifiable information, such as   \n",
       "\n",
       "                     category                subcategory  \\\n",
       "0                       Other                 Other Type   \n",
       "1  First Party Collection/Use            Collection Mode   \n",
       "2  First Party Collection/Use               Choice Scope   \n",
       "3  First Party Collection/Use         Action First-Party   \n",
       "4  First Party Collection/Use  Personal Information Type   \n",
       "\n",
       "                          label  \n",
       "0          Introductory/Generic  \n",
       "1                  not-selected  \n",
       "2                    Collection  \n",
       "3            Collect on website  \n",
       "4  Generic personal information  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cat_sub'] = data['category'] +'-'+ data['subcategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_sub</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Data Retention-Personal Information Type</th>\n",
       "      <td>370</td>\n",
       "      <td>370</td>\n",
       "      <td>370</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data Retention-Retention Period</th>\n",
       "      <td>370</td>\n",
       "      <td>370</td>\n",
       "      <td>370</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data Retention-Retention Purpose</th>\n",
       "      <td>370</td>\n",
       "      <td>370</td>\n",
       "      <td>370</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data Security-Security Measure</th>\n",
       "      <td>1008</td>\n",
       "      <td>1008</td>\n",
       "      <td>1008</td>\n",
       "      <td>1008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Do Not Track-Do Not Track policy</th>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First Party Collection/Use-Action First-Party</th>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First Party Collection/Use-Choice Scope</th>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First Party Collection/Use-Choice Type</th>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First Party Collection/Use-Collection Mode</th>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First Party Collection/Use-Does/Does Not</th>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First Party Collection/Use-Identifiability</th>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First Party Collection/Use-Personal Information Type</th>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First Party Collection/Use-Purpose</th>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First Party Collection/Use-User Type</th>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "      <td>8935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>International and Specific Audiences-Audience Type</th>\n",
       "      <td>939</td>\n",
       "      <td>939</td>\n",
       "      <td>939</td>\n",
       "      <td>939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other-Other Type</th>\n",
       "      <td>3548</td>\n",
       "      <td>3548</td>\n",
       "      <td>3548</td>\n",
       "      <td>3548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Policy Change-Change Type</th>\n",
       "      <td>548</td>\n",
       "      <td>548</td>\n",
       "      <td>548</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Policy Change-Notification Type</th>\n",
       "      <td>548</td>\n",
       "      <td>548</td>\n",
       "      <td>548</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Policy Change-User Choice</th>\n",
       "      <td>548</td>\n",
       "      <td>548</td>\n",
       "      <td>548</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Third Party Sharing/Collection-Action Third Party</th>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Third Party Sharing/Collection-Choice Scope</th>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Third Party Sharing/Collection-Choice Type</th>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Third Party Sharing/Collection-Does/Does Not</th>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Third Party Sharing/Collection-Identifiability</th>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Third Party Sharing/Collection-Personal Information Type</th>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Third Party Sharing/Collection-Purpose</th>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Third Party Sharing/Collection-Third Party Entity</th>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Third Party Sharing/Collection-User Type</th>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Access, Edit and Deletion-Access Scope</th>\n",
       "      <td>746</td>\n",
       "      <td>746</td>\n",
       "      <td>746</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Access, Edit and Deletion-Access Type</th>\n",
       "      <td>746</td>\n",
       "      <td>746</td>\n",
       "      <td>746</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Access, Edit and Deletion-User Type</th>\n",
       "      <td>746</td>\n",
       "      <td>746</td>\n",
       "      <td>746</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Choice/Control-Choice Scope</th>\n",
       "      <td>1789</td>\n",
       "      <td>1789</td>\n",
       "      <td>1789</td>\n",
       "      <td>1789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Choice/Control-Choice Type</th>\n",
       "      <td>1789</td>\n",
       "      <td>1789</td>\n",
       "      <td>1789</td>\n",
       "      <td>1789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Choice/Control-Personal Information Type</th>\n",
       "      <td>1789</td>\n",
       "      <td>1789</td>\n",
       "      <td>1789</td>\n",
       "      <td>1789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Choice/Control-Purpose</th>\n",
       "      <td>1789</td>\n",
       "      <td>1789</td>\n",
       "      <td>1789</td>\n",
       "      <td>1789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Choice/Control-User Type</th>\n",
       "      <td>1789</td>\n",
       "      <td>1789</td>\n",
       "      <td>1789</td>\n",
       "      <td>1789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  category  \\\n",
       "cat_sub                                                              \n",
       "Data Retention-Personal Information Type             370       370   \n",
       "Data Retention-Retention Period                      370       370   \n",
       "Data Retention-Retention Purpose                     370       370   \n",
       "Data Security-Security Measure                      1008      1008   \n",
       "Do Not Track-Do Not Track policy                      90        90   \n",
       "First Party Collection/Use-Action First-Party       8935      8935   \n",
       "First Party Collection/Use-Choice Scope             8935      8935   \n",
       "First Party Collection/Use-Choice Type              8935      8935   \n",
       "First Party Collection/Use-Collection Mode          8935      8935   \n",
       "First Party Collection/Use-Does/Does Not            8935      8935   \n",
       "First Party Collection/Use-Identifiability          8935      8935   \n",
       "First Party Collection/Use-Personal Information...  8935      8935   \n",
       "First Party Collection/Use-Purpose                  8935      8935   \n",
       "First Party Collection/Use-User Type                8935      8935   \n",
       "International and Specific Audiences-Audience Type   939       939   \n",
       "Other-Other Type                                    3548      3548   \n",
       "Policy Change-Change Type                            548       548   \n",
       "Policy Change-Notification Type                      548       548   \n",
       "Policy Change-User Choice                            548       548   \n",
       "Third Party Sharing/Collection-Action Third Party   5221      5221   \n",
       "Third Party Sharing/Collection-Choice Scope         5221      5221   \n",
       "Third Party Sharing/Collection-Choice Type          5221      5221   \n",
       "Third Party Sharing/Collection-Does/Does Not        5221      5221   \n",
       "Third Party Sharing/Collection-Identifiability      5221      5221   \n",
       "Third Party Sharing/Collection-Personal Informa...  5221      5221   \n",
       "Third Party Sharing/Collection-Purpose              5221      5221   \n",
       "Third Party Sharing/Collection-Third Party Entity   5221      5221   \n",
       "Third Party Sharing/Collection-User Type            5221      5221   \n",
       "User Access, Edit and Deletion-Access Scope          746       746   \n",
       "User Access, Edit and Deletion-Access Type           746       746   \n",
       "User Access, Edit and Deletion-User Type             746       746   \n",
       "User Choice/Control-Choice Scope                    1789      1789   \n",
       "User Choice/Control-Choice Type                     1789      1789   \n",
       "User Choice/Control-Personal Information Type       1789      1789   \n",
       "User Choice/Control-Purpose                         1789      1789   \n",
       "User Choice/Control-User Type                       1789      1789   \n",
       "\n",
       "                                                    subcategory  label  \n",
       "cat_sub                                                                 \n",
       "Data Retention-Personal Information Type                    370    370  \n",
       "Data Retention-Retention Period                             370    370  \n",
       "Data Retention-Retention Purpose                            370    370  \n",
       "Data Security-Security Measure                             1008   1008  \n",
       "Do Not Track-Do Not Track policy                             90     90  \n",
       "First Party Collection/Use-Action First-Party              8935   8935  \n",
       "First Party Collection/Use-Choice Scope                    8935   8935  \n",
       "First Party Collection/Use-Choice Type                     8935   8935  \n",
       "First Party Collection/Use-Collection Mode                 8935   8935  \n",
       "First Party Collection/Use-Does/Does Not                   8935   8935  \n",
       "First Party Collection/Use-Identifiability                 8935   8935  \n",
       "First Party Collection/Use-Personal Information...         8935   8935  \n",
       "First Party Collection/Use-Purpose                         8935   8935  \n",
       "First Party Collection/Use-User Type                       8935   8935  \n",
       "International and Specific Audiences-Audience Type          939    939  \n",
       "Other-Other Type                                           3548   3548  \n",
       "Policy Change-Change Type                                   548    548  \n",
       "Policy Change-Notification Type                             548    548  \n",
       "Policy Change-User Choice                                   548    548  \n",
       "Third Party Sharing/Collection-Action Third Party          5221   5221  \n",
       "Third Party Sharing/Collection-Choice Scope                5221   5221  \n",
       "Third Party Sharing/Collection-Choice Type                 5221   5221  \n",
       "Third Party Sharing/Collection-Does/Does Not               5221   5221  \n",
       "Third Party Sharing/Collection-Identifiability             5221   5221  \n",
       "Third Party Sharing/Collection-Personal Informa...         5221   5221  \n",
       "Third Party Sharing/Collection-Purpose                     5221   5221  \n",
       "Third Party Sharing/Collection-Third Party Entity          5221   5221  \n",
       "Third Party Sharing/Collection-User Type                   5221   5221  \n",
       "User Access, Edit and Deletion-Access Scope                 746    746  \n",
       "User Access, Edit and Deletion-Access Type                  746    746  \n",
       "User Access, Edit and Deletion-User Type                    746    746  \n",
       "User Choice/Control-Choice Scope                           1789   1789  \n",
       "User Choice/Control-Choice Type                            1789   1789  \n",
       "User Choice/Control-Personal Information Type              1789   1789  \n",
       "User Choice/Control-Purpose                                1789   1789  \n",
       "User Choice/Control-User Type                              1789   1789  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('cat_sub').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           36\n",
       "category       36\n",
       "subcategory    36\n",
       "label          36\n",
       "cat_sub        36\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['category','subcategory']).nunique().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cat_sub_lab'] = data['category'] +'-'+ data['subcategory'] +'-'+ data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "data['text'] = data['text'].apply(lambda x : remove_punct(x.lower()))\n",
    "\n",
    "data = data[data['text']!='null']\n",
    "data = data[data['text']!='noselectedtext']\n",
    "data = data[data['text']!='']\n",
    "data = data[data['text']!=' ']\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "ds = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36717, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(ds.groupby('category').size())\n",
    "x.to_csv('cat_distro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(ds.groupby(['cat_sub']).size())\n",
    "x.to_csv('subscategory_distro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "ds.groupby(['cat_sub_lab']).size().to_csv('value_distro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "ds['tokenized'] = ds['text'].apply(lambda x : re.split(' ', x))\n",
    "# ds['tokenized_text']=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    " \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-15 16:45:03,997 : INFO : collecting all words and their counts\n",
      "2019-09-15 16:45:03,998 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-09-15 16:45:04,061 : INFO : PROGRESS: at sentence #10000, processed 150297 words, keeping 3400 word types\n",
      "2019-09-15 16:45:04,130 : INFO : PROGRESS: at sentence #20000, processed 311668 words, keeping 5095 word types\n",
      "2019-09-15 16:45:04,188 : INFO : PROGRESS: at sentence #30000, processed 476465 words, keeping 6341 word types\n",
      "2019-09-15 16:45:04,251 : INFO : collected 7149 word types from a corpus of 596891 raw words and 36717 sentences\n",
      "2019-09-15 16:45:04,251 : INFO : Loading a fresh vocabulary\n",
      "2019-09-15 16:45:04,272 : INFO : min_count=5 retains 3932 unique words (55% of original 7149, drops 3217)\n",
      "2019-09-15 16:45:04,272 : INFO : min_count=5 leaves 589885 word corpus (98% of original 596891, drops 7006)\n",
      "2019-09-15 16:45:04,310 : INFO : deleting the raw counts dictionary of 7149 items\n",
      "2019-09-15 16:45:04,312 : INFO : sample=0.001 downsamples 58 most-common words\n",
      "2019-09-15 16:45:04,314 : INFO : downsampling leaves estimated 405586 word corpus (68.8% of prior 589885)\n",
      "2019-09-15 16:45:04,353 : INFO : estimated required memory for 3932 words and 100 dimensions: 5111600 bytes\n",
      "2019-09-15 16:45:04,354 : INFO : resetting layer weights\n",
      "2019-09-15 16:45:04,461 : INFO : training model with 3 workers on 3932 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-09-15 16:45:04,885 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-15 16:45:04,890 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-15 16:45:04,898 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-15 16:45:04,899 : INFO : EPOCH - 1 : training on 596891 raw words (405488 effective words) took 0.4s, 941480 effective words/s\n",
      "2019-09-15 16:45:05,354 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-15 16:45:05,364 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-15 16:45:05,369 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-15 16:45:05,370 : INFO : EPOCH - 2 : training on 596891 raw words (405691 effective words) took 0.4s, 917437 effective words/s\n",
      "2019-09-15 16:45:05,826 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-15 16:45:05,835 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-15 16:45:05,836 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-15 16:45:05,837 : INFO : EPOCH - 3 : training on 596891 raw words (405395 effective words) took 0.5s, 897662 effective words/s\n",
      "2019-09-15 16:45:06,309 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-15 16:45:06,319 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-15 16:45:06,323 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-15 16:45:06,324 : INFO : EPOCH - 4 : training on 596891 raw words (405562 effective words) took 0.5s, 886022 effective words/s\n",
      "2019-09-15 16:45:06,775 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-15 16:45:06,786 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-15 16:45:06,789 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-15 16:45:06,790 : INFO : EPOCH - 5 : training on 596891 raw words (405406 effective words) took 0.5s, 889858 effective words/s\n",
      "2019-09-15 16:45:06,791 : INFO : training on a 2984455 raw words (2027542 effective words) took 2.3s, 870611 effective words/s\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2Vec(ds['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encode = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {k: v.index for k, v in word_vectors.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_word(new_word, new_vector, new_index, embedding_matrix, word2id):\n",
    "    embedding_matrix = np.insert(embedding_matrix, [new_index], [new_vector], axis=0)\n",
    "    \n",
    "    word2id = {word: (index+1) if index >= new_index else index for word, index in word2id.items()}\n",
    "    word2id[new_word] = new_index\n",
    "    return embedding_matrix, word2id\n",
    "\n",
    "UNK_INDEX = 0\n",
    "UNK_TOKEN = 'UNK'\n",
    "\n",
    "embedding_matrix = word_vectors.vectors\n",
    "unk_vector = embedding_matrix.mean(0)\n",
    "embedding_matrix, word2id = add_new_word(UNK_TOKEN, unk_vector, UNK_INDEX, embedding_matrix, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3933"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3933, 100)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.insert(embedding_matrix, len(embedding_matrix), [np.zeros((100,))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3934"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[3933].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created. Percentage of unknown words: 7006.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36717,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_int_data(token_text, word2id):\n",
    "    x = []\n",
    "    unk_count = 0\n",
    "    for item in token_text:\n",
    "        temp=[]\n",
    "        x.append(temp)\n",
    "        for word in item:\n",
    "            if word in word2id:\n",
    "                temp.append(word2id.get(word))\n",
    "            else:\n",
    "                temp.append(UNK_INDEX)\n",
    "                unk_count += 1\n",
    "    print('Data created. Percentage of unknown words: %.3f' % (unk_count))\n",
    "    return np.array(x)\n",
    "\n",
    "x=get_int_data(ds.tokenized, word2id)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "ds['enumerated_text']=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "data_retention = ds[ds.category=='Data Retention']\n",
    "data_retention['Personal_Information_Type'] = data_retention['subcategory'].apply(lambda x: 1 if x=='Personal Information Type' else 0)\n",
    "data_retention['Retention_Period'] = data_retention['subcategory'].apply(lambda x: 1 if x=='Retention Period' else 0)\n",
    "data_retention['Retention_Purpose'] = data_retention['subcategory'].apply(lambda x: 1 if x=='Retention Purpose' else 0)\n",
    "\n",
    "first_party_collection_use = ds[ds.category=='First Party Collection/Use']\n",
    "first_party_collection_use['Action_First_Party'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Action First-Party' else 0)\n",
    "first_party_collection_use['Choice_Scope'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Choice Scope' else 0)\n",
    "first_party_collection_use['Choice_Type'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Choice Type' else 0)\n",
    "first_party_collection_use['Collection_Mode'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Collection Mode' else 0)\n",
    "first_party_collection_use['Does_Does_Not'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Does/Does Not' else 0)\n",
    "first_party_collection_use['Identifiability'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Identifiability' else 0)\n",
    "first_party_collection_use['Personal_Information_Type'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Personal Information Type' else 0)\n",
    "first_party_collection_use['Purpose'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='Purpose' else 0)\n",
    "first_party_collection_use['User_Type'] = first_party_collection_use['subcategory'].apply(lambda x: 1 if x=='User Type' else 0)\n",
    "\n",
    "policy_change = ds[ds.category=='Policy Change']\n",
    "policy_change['Change_Type'] = policy_change['subcategory'].apply(lambda x: 1 if x=='Change Type' else 0)\n",
    "policy_change['Notification_Type'] = policy_change['subcategory'].apply(lambda x: 1 if x=='Notification Type' else 0)\n",
    "policy_change['User_Choice'] = policy_change['subcategory'].apply(lambda x: 1 if x=='User Choice' else 0)\n",
    "\n",
    "third_party_sharing_collection = ds[ds.category=='Third Party Sharing/Collection']\n",
    "third_party_sharing_collection['Action_Third_Party'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Action Third Party' else 0)\n",
    "third_party_sharing_collection['Choice_Scope'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Choice Scope' else 0)\n",
    "third_party_sharing_collection['Choice_Type'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Choice Type' else 0)\n",
    "third_party_sharing_collection['Does_Does_Not'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Does/Does Not' else 0)\n",
    "third_party_sharing_collection['Identifiability'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Identifiability' else 0)\n",
    "third_party_sharing_collection['Personal_Information_Type'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Personal Information Type' else 0)\n",
    "third_party_sharing_collection['Purpose'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Purpose' else 0)\n",
    "third_party_sharing_collection['Third_Party_Entity'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='Third Party Entity' else 0)\n",
    "third_party_sharing_collection['User_Type'] = third_party_sharing_collection['subcategory'].apply(lambda x: 1 if x=='User Type' else 0)\n",
    "\n",
    "user_access_edit_deletion = ds[ds.category=='User Access, Edit and Deletion']\n",
    "user_access_edit_deletion['Access_Scope'] = user_access_edit_deletion['subcategory'].apply(lambda x: 1 if x=='Access Scope' else 0)\n",
    "user_access_edit_deletion['Access_Type'] = user_access_edit_deletion['subcategory'].apply(lambda x: 1 if x=='Access Type' else 0)\n",
    "user_access_edit_deletion['User_Type'] = user_access_edit_deletion['subcategory'].apply(lambda x: 1 if x=='User Type' else 0)\n",
    "\n",
    "user_choice_control = ds[ds.category=='User Choice/Control']\n",
    "user_choice_control['Choice_Scope'] = user_choice_control['subcategory'].apply(lambda x: 1 if x=='Choice Scope' else 0)\n",
    "user_choice_control['Choice_Type'] = user_choice_control['subcategory'].apply(lambda x: 1 if x=='Choice Type' else 0)\n",
    "user_choice_control['Personal_Information_Type'] = user_choice_control['subcategory'].apply(lambda x: 1 if x=='Personal Information Type' else 0)\n",
    "user_choice_control['Purpose'] = user_choice_control['subcategory'].apply(lambda x: 1 if x=='Purpose' else 0)\n",
    "user_choice_control['User_Type'] = user_choice_control['subcategory'].apply(lambda x: 1 if x=='User Type' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                         collecregister on our website or participate i...\n",
       "category                     First Party Collection/UseFirst Party Collecti...\n",
       "subcategory                  Choice ScopeAction First-PartyPersonal Informa...\n",
       "label                        CollectionCollect on websiteGeneric personal i...\n",
       "cat_sub                      First Party Collection/Use-Choice ScopeFirst P...\n",
       "cat_sub_lab                  First Party Collection/Use-Choice Scope-Collec...\n",
       "tokenized                    [collec, register, on, our, website, or, parti...\n",
       "enumerated_text              [2151, 204, 19, 10, 52, 6, 212, 13, 10, 168, 3...\n",
       "Action_First_Party                                                        2733\n",
       "Choice_Scope                                                               528\n",
       "Choice_Type                                                                937\n",
       "Collection_Mode                                                           1528\n",
       "Does_Does_Not                                                             1308\n",
       "Identifiability                                                            636\n",
       "Personal_Information_Type                                                 3119\n",
       "Purpose                                                                   4214\n",
       "User_Type                                                                  343\n",
       "dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first_party_collection_use.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Activation, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_length = max(ds.enumerated_text.apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA RETENTION MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 473 samples, validate on 203 samples\n",
      "Epoch 1/5\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 0.8942 - acc: 0.7104 - val_loss: 0.4809 - val_acc: 0.7044\n",
      "Epoch 2/5\n",
      "473/473 [==============================] - 0s 691us/step - loss: 0.4694 - acc: 0.8457 - val_loss: 0.3167 - val_acc: 0.8621\n",
      "Epoch 3/5\n",
      "473/473 [==============================] - 0s 699us/step - loss: 0.3120 - acc: 0.9197 - val_loss: 0.3481 - val_acc: 0.8424\n",
      "Epoch 4/5\n",
      "473/473 [==============================] - 0s 737us/step - loss: 0.2343 - acc: 0.9408 - val_loss: 0.3094 - val_acc: 0.8522\n",
      "Epoch 5/5\n",
      "473/473 [==============================] - 0s 735us/step - loss: 0.1930 - acc: 0.9556 - val_loss: 0.3217 - val_acc: 0.8424\n",
      "[[124  21]\n",
      " [ 11  47]]\n",
      "0.746031746031746\n"
     ]
    }
   ],
   "source": [
    "# Personal Information Type Model (Data Retention)\n",
    "\n",
    "padded_docs = pad_sequences(data_retention.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, data_retention.Personal_Information_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 3.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 473 samples, validate on 203 samples\n",
      "Epoch 1/5\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 0.8483 - acc: 0.6342 - val_loss: 0.6234 - val_acc: 0.7488\n",
      "Epoch 2/5\n",
      "473/473 [==============================] - 0s 753us/step - loss: 0.5192 - acc: 0.8499 - val_loss: 0.6548 - val_acc: 0.7291\n",
      "Epoch 3/5\n",
      "473/473 [==============================] - 0s 693us/step - loss: 0.3986 - acc: 0.8795 - val_loss: 0.6656 - val_acc: 0.7291\n",
      "Epoch 4/5\n",
      "473/473 [==============================] - 0s 704us/step - loss: 0.3217 - acc: 0.9154 - val_loss: 0.7123 - val_acc: 0.7241\n",
      "Epoch 5/5\n",
      "473/473 [==============================] - 0s 674us/step - loss: 0.2718 - acc: 0.9260 - val_loss: 0.7573 - val_acc: 0.7389\n",
      "[[104  27]\n",
      " [ 26  46]]\n",
      "0.6344827586206897\n"
     ]
    }
   ],
   "source": [
    "# Retention Period Model (Data Retention)\n",
    "\n",
    "padded_docs = pad_sequences(data_retention.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, data_retention.Retention_Period, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 2.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 473 samples, validate on 203 samples\n",
      "Epoch 1/5\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 0.8730 - acc: 0.6512 - val_loss: 0.7313 - val_acc: 0.5911\n",
      "Epoch 2/5\n",
      "473/473 [==============================] - 0s 715us/step - loss: 0.5129 - acc: 0.8224 - val_loss: 0.6825 - val_acc: 0.7241\n",
      "Epoch 3/5\n",
      "473/473 [==============================] - 0s 730us/step - loss: 0.3759 - acc: 0.8943 - val_loss: 0.7578 - val_acc: 0.7291\n",
      "Epoch 4/5\n",
      "473/473 [==============================] - 0s 678us/step - loss: 0.2959 - acc: 0.9112 - val_loss: 0.8299 - val_acc: 0.7094\n",
      "Epoch 5/5\n",
      "473/473 [==============================] - 0s 745us/step - loss: 0.2471 - acc: 0.9260 - val_loss: 0.8731 - val_acc: 0.7488\n",
      "[[105  25]\n",
      " [ 26  47]]\n",
      "0.6482758620689656\n"
     ]
    }
   ],
   "source": [
    "# Retention Purpose Model (Data Retention)\n",
    "\n",
    "padded_docs = pad_sequences(data_retention.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, data_retention.Retention_Purpose, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 2.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIRST PARTY COLLECTION/USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 7s 682us/step - loss: 1.0313 - acc: 0.6438 - val_loss: 0.5943 - val_acc: 0.6738\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 7s 628us/step - loss: 0.8297 - acc: 0.7524 - val_loss: 0.5790 - val_acc: 0.7042\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 7s 639us/step - loss: 0.7190 - acc: 0.7940 - val_loss: 0.5339 - val_acc: 0.7400\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 7s 660us/step - loss: 0.6144 - acc: 0.8316 - val_loss: 0.5437 - val_acc: 0.7517\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 7s 664us/step - loss: 0.5424 - acc: 0.8498 - val_loss: 0.5855 - val_acc: 0.7496\n",
      "[[2978  834]\n",
      " [ 319  473]]\n",
      "0.45069080514530724\n"
     ]
    }
   ],
   "source": [
    "# Action First-Party Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Action_First_Party, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 7s 696us/step - loss: 0.8772 - acc: 0.8470 - val_loss: 0.4273 - val_acc: 0.8262\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 7s 633us/step - loss: 0.6523 - acc: 0.8692 - val_loss: 0.3608 - val_acc: 0.8351\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 7s 638us/step - loss: 0.5159 - acc: 0.8905 - val_loss: 0.2702 - val_acc: 0.9036\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 7s 650us/step - loss: 0.4220 - acc: 0.9152 - val_loss: 0.3000 - val_acc: 0.8792\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 7s 665us/step - loss: 0.3496 - acc: 0.9295 - val_loss: 0.2683 - val_acc: 0.9014\n",
      "[[4085  340]\n",
      " [ 114   65]]\n",
      "0.22260273972602743\n"
     ]
    }
   ],
   "source": [
    "# Choice Scope Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Choice_Scope, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 15.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 8s 716us/step - loss: 0.5194 - acc: 0.9022 - val_loss: 0.2792 - val_acc: 0.9153\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 7s 686us/step - loss: 0.3431 - acc: 0.9320 - val_loss: 0.2401 - val_acc: 0.9146\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 7s 651us/step - loss: 0.2733 - acc: 0.9421 - val_loss: 0.2278 - val_acc: 0.9207\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 7s 675us/step - loss: 0.2198 - acc: 0.9521 - val_loss: 0.2408 - val_acc: 0.9146\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 7s 675us/step - loss: 0.1894 - acc: 0.9538 - val_loss: 0.2396 - val_acc: 0.9203\n",
      "[[4073  223]\n",
      " [ 144  164]]\n",
      "0.47194244604316543\n"
     ]
    }
   ],
   "source": [
    "# Choice Type Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Choice_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 8s 714us/step - loss: 0.8561 - acc: 0.8039 - val_loss: 0.4714 - val_acc: 0.8282\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 7s 670us/step - loss: 0.7181 - acc: 0.8373 - val_loss: 0.4452 - val_acc: 0.8199\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 7s 664us/step - loss: 0.6330 - acc: 0.8588 - val_loss: 0.4206 - val_acc: 0.8280\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 7s 667us/step - loss: 0.5478 - acc: 0.8732 - val_loss: 0.4163 - val_acc: 0.8371\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 7s 681us/step - loss: 0.4860 - acc: 0.8827 - val_loss: 0.4317 - val_acc: 0.8265\n",
      "[[3650  469]\n",
      " [ 330  155]]\n",
      "0.2795311091073039\n"
     ]
    }
   ],
   "source": [
    "# Collection Mode Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Collection_Mode, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 8s 774us/step - loss: 0.7276 - acc: 0.8308 - val_loss: 0.4212 - val_acc: 0.8084\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 7s 697us/step - loss: 0.5890 - acc: 0.8555 - val_loss: 0.3961 - val_acc: 0.8284\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 7s 687us/step - loss: 0.5001 - acc: 0.8736 - val_loss: 0.3521 - val_acc: 0.8540\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 7s 691us/step - loss: 0.4332 - acc: 0.8895 - val_loss: 0.4558 - val_acc: 0.7967\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 8s 726us/step - loss: 0.3846 - acc: 0.8981 - val_loss: 0.3951 - val_acc: 0.8395\n",
      "[[3696  521]\n",
      " [ 218  169]]\n",
      "0.3138347260909935\n"
     ]
    }
   ],
   "source": [
    "# Does/Does Not Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Does_Does_Not, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 8s 742us/step - loss: 0.4551 - acc: 0.9427 - val_loss: 0.2001 - val_acc: 0.9509\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 7s 681us/step - loss: 0.3159 - acc: 0.9454 - val_loss: 0.1788 - val_acc: 0.9494\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 7s 696us/step - loss: 0.2529 - acc: 0.9513 - val_loss: 0.1845 - val_acc: 0.9401\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 8s 712us/step - loss: 0.2072 - acc: 0.9556 - val_loss: 0.1756 - val_acc: 0.9424\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 7s 688us/step - loss: 0.1699 - acc: 0.9616 - val_loss: 0.1675 - val_acc: 0.9496\n",
      "[[4301  122]\n",
      " [ 110   71]]\n",
      "0.3796791443850267\n"
     ]
    }
   ],
   "source": [
    "# Identifiability Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Identifiability, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 9s 797us/step - loss: 0.8812 - acc: 0.7388 - val_loss: 0.5018 - val_acc: 0.7700\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 8s 705us/step - loss: 0.6506 - acc: 0.8259 - val_loss: 0.4852 - val_acc: 0.7837\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 8s 699us/step - loss: 0.5274 - acc: 0.8612 - val_loss: 0.4104 - val_acc: 0.8399\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 8s 708us/step - loss: 0.4289 - acc: 0.8875 - val_loss: 0.4167 - val_acc: 0.8475\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 8s 703us/step - loss: 0.3801 - acc: 0.9036 - val_loss: 0.4560 - val_acc: 0.8315\n",
      "[[3108  604]\n",
      " [ 172  720]]\n",
      "0.6498194945848376\n"
     ]
    }
   ],
   "source": [
    "# Personal Information Type Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Personal_Information_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 8s 771us/step - loss: 0.8946 - acc: 0.7171 - val_loss: 0.4565 - val_acc: 0.7919\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 8s 713us/step - loss: 0.6606 - acc: 0.8227 - val_loss: 0.4341 - val_acc: 0.8208\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 8s 735us/step - loss: 0.5379 - acc: 0.8655 - val_loss: 0.4421 - val_acc: 0.8256\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 7s 667us/step - loss: 0.4384 - acc: 0.8953 - val_loss: 0.4396 - val_acc: 0.8308\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 8s 702us/step - loss: 0.3650 - acc: 0.9145 - val_loss: 0.4304 - val_acc: 0.8475\n",
      "[[2841  487]\n",
      " [ 215 1061]]\n",
      "0.7514164305949008\n"
     ]
    }
   ],
   "source": [
    "# Purpose Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.Purpose, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10742 samples, validate on 4604 samples\n",
      "Epoch 1/5\n",
      "10742/10742 [==============================] - 8s 753us/step - loss: 0.2935 - acc: 0.9626 - val_loss: 0.1136 - val_acc: 0.9728\n",
      "Epoch 2/5\n",
      "10742/10742 [==============================] - 7s 661us/step - loss: 0.1802 - acc: 0.9721 - val_loss: 0.1469 - val_acc: 0.9461\n",
      "Epoch 3/5\n",
      "10742/10742 [==============================] - 7s 653us/step - loss: 0.1403 - acc: 0.9745 - val_loss: 0.0958 - val_acc: 0.9694\n",
      "Epoch 4/5\n",
      "10742/10742 [==============================] - 7s 667us/step - loss: 0.1095 - acc: 0.9809 - val_loss: 0.1078 - val_acc: 0.9676\n",
      "Epoch 5/5\n",
      "10742/10742 [==============================] - 7s 670us/step - loss: 0.0891 - acc: 0.9832 - val_loss: 0.1054 - val_acc: 0.9687\n",
      "[[4411   89]\n",
      " [  55   49]]\n",
      "0.4049586776859504\n"
     ]
    }
   ],
   "source": [
    "# User Type Model (First Party Collection/Use)\n",
    "\n",
    "padded_docs = pad_sequences(first_party_collection_use.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, first_party_collection_use.User_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POLICY CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 652 samples, validate on 280 samples\n",
      "Epoch 1/5\n",
      "652/652 [==============================] - 2s 2ms/step - loss: 1.4793 - acc: 0.4003 - val_loss: 0.7818 - val_acc: 0.5071\n",
      "Epoch 2/5\n",
      "652/652 [==============================] - 0s 692us/step - loss: 0.9945 - acc: 0.5813 - val_loss: 0.8878 - val_acc: 0.5786\n",
      "Epoch 3/5\n",
      "652/652 [==============================] - 0s 725us/step - loss: 0.8208 - acc: 0.7270 - val_loss: 0.7658 - val_acc: 0.6679\n",
      "Epoch 4/5\n",
      "652/652 [==============================] - 0s 722us/step - loss: 0.6597 - acc: 0.7807 - val_loss: 0.7863 - val_acc: 0.6821\n",
      "Epoch 5/5\n",
      "652/652 [==============================] - 0s 726us/step - loss: 0.5578 - acc: 0.8344 - val_loss: 0.8538 - val_acc: 0.6643\n",
      "[[111  72]\n",
      " [ 22  75]]\n",
      "0.6147540983606558\n"
     ]
    }
   ],
   "source": [
    "# Change Type Model (Policy Change)\n",
    "\n",
    "padded_docs = pad_sequences(policy_change.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, policy_change.Change_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 652 samples, validate on 280 samples\n",
      "Epoch 1/5\n",
      "652/652 [==============================] - 2s 3ms/step - loss: 1.3698 - acc: 0.4969 - val_loss: 0.7989 - val_acc: 0.5786\n",
      "Epoch 2/5\n",
      "652/652 [==============================] - 0s 684us/step - loss: 0.8231 - acc: 0.6840 - val_loss: 0.8175 - val_acc: 0.6464\n",
      "Epoch 3/5\n",
      "652/652 [==============================] - 0s 692us/step - loss: 0.6771 - acc: 0.7730 - val_loss: 0.7890 - val_acc: 0.6750\n",
      "Epoch 4/5\n",
      "652/652 [==============================] - 0s 687us/step - loss: 0.5700 - acc: 0.7929 - val_loss: 0.8644 - val_acc: 0.6750\n",
      "Epoch 5/5\n",
      "652/652 [==============================] - 0s 697us/step - loss: 0.5210 - acc: 0.8313 - val_loss: 0.8830 - val_acc: 0.6750\n",
      "[[ 83  60]\n",
      " [ 31 106]]\n",
      "0.6996699669966996\n"
     ]
    }
   ],
   "source": [
    "# Notification Type Model (Policy Change)\n",
    "\n",
    "padded_docs = pad_sequences(policy_change.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, policy_change.Notification_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 652 samples, validate on 280 samples\n",
      "Epoch 1/5\n",
      "652/652 [==============================] - 2s 3ms/step - loss: 1.2225 - acc: 0.5307 - val_loss: 0.7328 - val_acc: 0.6500\n",
      "Epoch 2/5\n",
      "652/652 [==============================] - 0s 662us/step - loss: 0.8166 - acc: 0.7945 - val_loss: 0.5647 - val_acc: 0.7714\n",
      "Epoch 3/5\n",
      "652/652 [==============================] - 0s 729us/step - loss: 0.6426 - acc: 0.8804 - val_loss: 0.6203 - val_acc: 0.7071\n",
      "Epoch 4/5\n",
      "652/652 [==============================] - 0s 695us/step - loss: 0.5389 - acc: 0.8252 - val_loss: 0.5698 - val_acc: 0.7429\n",
      "Epoch 5/5\n",
      "652/652 [==============================] - 0s 692us/step - loss: 0.4742 - acc: 0.8773 - val_loss: 0.6159 - val_acc: 0.7393\n",
      "[[181  53]\n",
      " [ 20  26]]\n",
      "0.416\n"
     ]
    }
   ],
   "source": [
    "# User Choice Model (Policy Change)\n",
    "\n",
    "padded_docs = pad_sequences(policy_change.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, policy_change.User_Choice, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THIRD PARTY SHARING/COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 6s 805us/step - loss: 1.0042 - acc: 0.6828 - val_loss: 0.5504 - val_acc: 0.7251\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 5s 684us/step - loss: 0.7691 - acc: 0.7809 - val_loss: 0.5558 - val_acc: 0.7219\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 5s 668us/step - loss: 0.6424 - acc: 0.8199 - val_loss: 0.4839 - val_acc: 0.7692\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 5s 669us/step - loss: 0.5519 - acc: 0.8430 - val_loss: 0.6117 - val_acc: 0.7382\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 5s 666us/step - loss: 0.4850 - acc: 0.8663 - val_loss: 0.5979 - val_acc: 0.7579\n",
      "[[2024  592]\n",
      " [ 182  399]]\n",
      "0.5076335877862596\n"
     ]
    }
   ],
   "source": [
    "# Action Third-Party Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Action_Third_Party, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 7s 905us/step - loss: 0.4508 - acc: 0.9519 - val_loss: 0.1965 - val_acc: 0.9637\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 5s 675us/step - loss: 0.3150 - acc: 0.9578 - val_loss: 0.1805 - val_acc: 0.9537\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 5s 645us/step - loss: 0.2637 - acc: 0.9582 - val_loss: 0.1669 - val_acc: 0.9509\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 5s 667us/step - loss: 0.2216 - acc: 0.9619 - val_loss: 0.2183 - val_acc: 0.9218\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 5s 734us/step - loss: 0.1871 - acc: 0.9634 - val_loss: 0.1855 - val_acc: 0.9399\n",
      "[[2979  119]\n",
      " [  73   26]]\n",
      "0.21311475409836064\n"
     ]
    }
   ],
   "source": [
    "# Choice Scope Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Choice_Scope, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 6s 855us/step - loss: 0.4699 - acc: 0.9040 - val_loss: 0.1529 - val_acc: 0.9496\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 5s 670us/step - loss: 0.2617 - acc: 0.9513 - val_loss: 0.1486 - val_acc: 0.9481\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 5s 618us/step - loss: 0.1870 - acc: 0.9630 - val_loss: 0.1492 - val_acc: 0.9481\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 6s 770us/step - loss: 0.1433 - acc: 0.9732 - val_loss: 0.1565 - val_acc: 0.9453\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 5s 708us/step - loss: 0.1214 - acc: 0.9781 - val_loss: 0.1509 - val_acc: 0.9531\n",
      "[[2896   89]\n",
      " [  61  151]]\n",
      "0.668141592920354\n"
     ]
    }
   ],
   "source": [
    "# Choice Type Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Choice_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 7s 891us/step - loss: 0.7438 - acc: 0.8238 - val_loss: 0.3517 - val_acc: 0.8621\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 5s 692us/step - loss: 0.5832 - acc: 0.8623 - val_loss: 0.4257 - val_acc: 0.7807\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 5s 685us/step - loss: 0.5098 - acc: 0.8563 - val_loss: 0.3740 - val_acc: 0.8239\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 5s 686us/step - loss: 0.4543 - acc: 0.8703 - val_loss: 0.4002 - val_acc: 0.8183\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 5s 665us/step - loss: 0.4135 - acc: 0.8844 - val_loss: 0.4547 - val_acc: 0.7917\n",
      "[[2401  518]\n",
      " [ 148  130]]\n",
      "0.28077753779697623\n"
     ]
    }
   ],
   "source": [
    "# Does/Does Not Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Does_Does_Not, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 6s 811us/step - loss: 0.4007 - acc: 0.9427 - val_loss: 0.1821 - val_acc: 0.9515\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 5s 634us/step - loss: 0.2597 - acc: 0.9476 - val_loss: 0.1497 - val_acc: 0.9481\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 5s 642us/step - loss: 0.2125 - acc: 0.9541 - val_loss: 0.1619 - val_acc: 0.9406\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 5s 656us/step - loss: 0.1805 - acc: 0.9553 - val_loss: 0.1463 - val_acc: 0.9503\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 5s 644us/step - loss: 0.1509 - acc: 0.9614 - val_loss: 0.1503 - val_acc: 0.9443\n",
      "[[2960  125]\n",
      " [  53   59]]\n",
      "0.3986486486486487\n"
     ]
    }
   ],
   "source": [
    "# Identifiability Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Identifiability, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 6s 828us/step - loss: 0.6948 - acc: 0.8215 - val_loss: 0.3806 - val_acc: 0.8605\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 5s 647us/step - loss: 0.4581 - acc: 0.8882 - val_loss: 0.3242 - val_acc: 0.8805\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 5s 691us/step - loss: 0.3755 - acc: 0.9096 - val_loss: 0.2867 - val_acc: 0.8990\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 5s 699us/step - loss: 0.3198 - acc: 0.9248 - val_loss: 0.2834 - val_acc: 0.8987\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 5s 678us/step - loss: 0.2702 - acc: 0.9347 - val_loss: 0.2718 - val_acc: 0.9087\n",
      "[[2557  207]\n",
      " [  85  348]]\n",
      "0.7044534412955465\n"
     ]
    }
   ],
   "source": [
    "# Personal Information Type Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Personal_Information_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 7s 896us/step - loss: 0.9489 - acc: 0.7005 - val_loss: 0.5620 - val_acc: 0.7116\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 5s 686us/step - loss: 0.6752 - acc: 0.8139 - val_loss: 0.5313 - val_acc: 0.7660\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 5s 697us/step - loss: 0.5522 - acc: 0.8608 - val_loss: 0.4795 - val_acc: 0.7998\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 5s 665us/step - loss: 0.4748 - acc: 0.8864 - val_loss: 0.5197 - val_acc: 0.8120\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 5s 654us/step - loss: 0.4020 - acc: 0.9094 - val_loss: 0.4998 - val_acc: 0.8117\n",
      "[[1925  465]\n",
      " [ 137  670]]\n",
      "0.6900102986611741\n"
     ]
    }
   ],
   "source": [
    "# Purpose Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Purpose, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 7s 905us/step - loss: 0.9000 - acc: 0.7233 - val_loss: 0.5264 - val_acc: 0.7282\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 5s 668us/step - loss: 0.6619 - acc: 0.8215 - val_loss: 0.4921 - val_acc: 0.7667\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 5s 662us/step - loss: 0.5539 - acc: 0.8626 - val_loss: 0.4201 - val_acc: 0.8198\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 5s 636us/step - loss: 0.4700 - acc: 0.8898 - val_loss: 0.4509 - val_acc: 0.8226\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 5s 666us/step - loss: 0.4180 - acc: 0.9052 - val_loss: 0.4580 - val_acc: 0.8317\n",
      "[[2194  363]\n",
      " [ 175  465]]\n",
      "0.6335149863760218\n"
     ]
    }
   ],
   "source": [
    "# Third Part Entity Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.Third_Party_Entity, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7458 samples, validate on 3197 samples\n",
      "Epoch 1/5\n",
      "7458/7458 [==============================] - 6s 849us/step - loss: 0.2225 - acc: 0.9803 - val_loss: 0.0717 - val_acc: 0.9884\n",
      "Epoch 2/5\n",
      "7458/7458 [==============================] - 5s 633us/step - loss: 0.1242 - acc: 0.9873 - val_loss: 0.0700 - val_acc: 0.9859\n",
      "Epoch 3/5\n",
      "7458/7458 [==============================] - 5s 633us/step - loss: 0.0943 - acc: 0.9890 - val_loss: 0.0565 - val_acc: 0.9859\n",
      "Epoch 4/5\n",
      "7458/7458 [==============================] - 5s 674us/step - loss: 0.0667 - acc: 0.9893 - val_loss: 0.0627 - val_acc: 0.9819\n",
      "Epoch 5/5\n",
      "7458/7458 [==============================] - 5s 696us/step - loss: 0.0493 - acc: 0.9924 - val_loss: 0.0453 - val_acc: 0.9891\n",
      "[[3143   19]\n",
      " [  16   19]]\n",
      "0.5205479452054795\n"
     ]
    }
   ],
   "source": [
    "# User Type Model (Third Party Sharing/Collection)\n",
    "\n",
    "padded_docs = pad_sequences(third_party_sharing_collection.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, third_party_sharing_collection.User_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER ACCESS, EDIT, AND DELETION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 765 samples, validate on 329 samples\n",
      "Epoch 1/5\n",
      "765/765 [==============================] - 3s 3ms/step - loss: 1.5152 - acc: 0.4183 - val_loss: 0.9642 - val_acc: 0.3860\n",
      "Epoch 2/5\n",
      "765/765 [==============================] - 1s 718us/step - loss: 1.0176 - acc: 0.5451 - val_loss: 0.7972 - val_acc: 0.5380\n",
      "Epoch 3/5\n",
      "765/765 [==============================] - 1s 699us/step - loss: 0.8596 - acc: 0.7046 - val_loss: 0.8274 - val_acc: 0.5897\n",
      "Epoch 4/5\n",
      "765/765 [==============================] - 1s 695us/step - loss: 0.7530 - acc: 0.7869 - val_loss: 0.8161 - val_acc: 0.6383\n",
      "Epoch 5/5\n",
      "765/765 [==============================] - 1s 739us/step - loss: 0.6878 - acc: 0.7856 - val_loss: 0.8093 - val_acc: 0.6657\n",
      "[[125  77]\n",
      " [ 33  94]]\n",
      "0.6308724832214764\n"
     ]
    }
   ],
   "source": [
    "# Access Scope Model (User Access, Edit and Deletion)\n",
    "\n",
    "padded_docs = pad_sequences(user_access_edit_deletion.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_access_edit_deletion.Access_Scope, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 765 samples, validate on 329 samples\n",
      "Epoch 1/5\n",
      "765/765 [==============================] - 3s 3ms/step - loss: 1.6352 - acc: 0.4863 - val_loss: 1.0672 - val_acc: 0.4894\n",
      "Epoch 2/5\n",
      "765/765 [==============================] - 1s 681us/step - loss: 1.1409 - acc: 0.6131 - val_loss: 0.7761 - val_acc: 0.6322\n",
      "Epoch 3/5\n",
      "765/765 [==============================] - 1s 750us/step - loss: 0.8875 - acc: 0.6771 - val_loss: 0.9422 - val_acc: 0.6049\n",
      "Epoch 4/5\n",
      "765/765 [==============================] - 1s 677us/step - loss: 0.7433 - acc: 0.7608 - val_loss: 0.8363 - val_acc: 0.6930\n",
      "Epoch 5/5\n",
      "765/765 [==============================] - 1s 697us/step - loss: 0.6259 - acc: 0.7843 - val_loss: 0.7836 - val_acc: 0.7143\n",
      "[[105  73]\n",
      " [ 21 130]]\n",
      "0.7344632768361582\n"
     ]
    }
   ],
   "source": [
    "# Access Type Model (User Access, Edit and Deletion)\n",
    "\n",
    "padded_docs = pad_sequences(user_access_edit_deletion.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_access_edit_deletion.Access_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 765 samples, validate on 329 samples\n",
      "Epoch 1/5\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.9203 - acc: 0.7621 - val_loss: 0.4637 - val_acc: 0.8116\n",
      "Epoch 2/5\n",
      "765/765 [==============================] - 1s 671us/step - loss: 0.5468 - acc: 0.8418 - val_loss: 0.4594 - val_acc: 0.8450\n",
      "Epoch 3/5\n",
      "765/765 [==============================] - 1s 704us/step - loss: 0.4368 - acc: 0.8758 - val_loss: 0.5088 - val_acc: 0.8389\n",
      "Epoch 4/5\n",
      "765/765 [==============================] - 1s 692us/step - loss: 0.3778 - acc: 0.8967 - val_loss: 0.4909 - val_acc: 0.8571\n",
      "Epoch 5/5\n",
      "765/765 [==============================] - 1s 664us/step - loss: 0.3330 - acc: 0.9255 - val_loss: 0.5178 - val_acc: 0.8450\n",
      "[[242  36]\n",
      " [ 15  36]]\n",
      "0.5853658536585366\n"
     ]
    }
   ],
   "source": [
    "# User Type Model (User Access, Edit and Deletion)\n",
    "\n",
    "padded_docs = pad_sequences(user_access_edit_deletion.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_access_edit_deletion.User_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER CHOICE/CONTROL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2658 samples, validate on 1140 samples\n",
      "Epoch 1/5\n",
      "2658/2658 [==============================] - 4s 1ms/step - loss: 1.3507 - acc: 0.3999 - val_loss: 0.8251 - val_acc: 0.4132\n",
      "Epoch 2/5\n",
      "2658/2658 [==============================] - 2s 679us/step - loss: 1.0435 - acc: 0.5903 - val_loss: 0.8725 - val_acc: 0.5096\n",
      "Epoch 3/5\n",
      "2658/2658 [==============================] - 2s 691us/step - loss: 0.9216 - acc: 0.7017 - val_loss: 0.8873 - val_acc: 0.5281\n",
      "Epoch 4/5\n",
      "2658/2658 [==============================] - 2s 734us/step - loss: 0.7998 - acc: 0.7468 - val_loss: 0.8664 - val_acc: 0.5658\n",
      "Epoch 5/5\n",
      "2658/2658 [==============================] - 2s 687us/step - loss: 0.7032 - acc: 0.7912 - val_loss: 0.7536 - val_acc: 0.6702\n",
      "[[561 235]\n",
      " [141 203]]\n",
      "0.5191815856777495\n"
     ]
    }
   ],
   "source": [
    "# Choice Scope Model (User Choice/Control)\n",
    "\n",
    "padded_docs = pad_sequences(user_choice_control.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_choice_control.Choice_Scope, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2658 samples, validate on 1140 samples\n",
      "Epoch 1/5\n",
      "2658/2658 [==============================] - 4s 1ms/step - loss: 1.1045 - acc: 0.6245 - val_loss: 0.6876 - val_acc: 0.6939\n",
      "Epoch 2/5\n",
      "2658/2658 [==============================] - 2s 686us/step - loss: 0.7084 - acc: 0.7923 - val_loss: 0.6575 - val_acc: 0.7228\n",
      "Epoch 3/5\n",
      "2658/2658 [==============================] - 2s 680us/step - loss: 0.5668 - acc: 0.8292 - val_loss: 0.6023 - val_acc: 0.7570\n",
      "Epoch 4/5\n",
      "2658/2658 [==============================] - 2s 697us/step - loss: 0.4746 - acc: 0.8755 - val_loss: 0.6666 - val_acc: 0.7500\n",
      "Epoch 5/5\n",
      "2658/2658 [==============================] - 2s 681us/step - loss: 0.4091 - acc: 0.8924 - val_loss: 0.6290 - val_acc: 0.7632\n",
      "[[581 192]\n",
      " [ 78 289]]\n",
      "0.6816037735849056\n"
     ]
    }
   ],
   "source": [
    "# Choice Type Model (User Choice/Control)\n",
    "\n",
    "padded_docs = pad_sequences(user_choice_control.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_choice_control.Choice_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2658 samples, validate on 1140 samples\n",
      "Epoch 1/5\n",
      "2658/2658 [==============================] - 4s 1ms/step - loss: 0.8483 - acc: 0.7931 - val_loss: 0.5465 - val_acc: 0.7342\n",
      "Epoch 2/5\n",
      "2658/2658 [==============================] - 2s 665us/step - loss: 0.5833 - acc: 0.8559 - val_loss: 0.3671 - val_acc: 0.8675\n",
      "Epoch 3/5\n",
      "2658/2658 [==============================] - 2s 823us/step - loss: 0.4701 - acc: 0.8864 - val_loss: 0.4213 - val_acc: 0.8386\n",
      "Epoch 4/5\n",
      "2658/2658 [==============================] - 2s 739us/step - loss: 0.3960 - acc: 0.9071 - val_loss: 0.4135 - val_acc: 0.8395\n",
      "Epoch 5/5\n",
      "2658/2658 [==============================] - 2s 673us/step - loss: 0.3482 - acc: 0.9169 - val_loss: 0.4151 - val_acc: 0.8553\n",
      "[[892  98]\n",
      " [ 67  83]]\n",
      "0.5015105740181269\n"
     ]
    }
   ],
   "source": [
    "# Personal Information Type Model (User Choice/Control)\n",
    "\n",
    "padded_docs = pad_sequences(user_choice_control.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_choice_control.Personal_Information_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2658 samples, validate on 1140 samples\n",
      "Epoch 1/5\n",
      "2658/2658 [==============================] - 4s 2ms/step - loss: 1.1167 - acc: 0.5944 - val_loss: 0.6460 - val_acc: 0.6982\n",
      "Epoch 2/5\n",
      "2658/2658 [==============================] - 2s 677us/step - loss: 0.7597 - acc: 0.8029 - val_loss: 0.6608 - val_acc: 0.6991\n",
      "Epoch 3/5\n",
      "2658/2658 [==============================] - 2s 723us/step - loss: 0.6282 - acc: 0.8266 - val_loss: 0.6207 - val_acc: 0.7395\n",
      "Epoch 4/5\n",
      "2658/2658 [==============================] - 2s 692us/step - loss: 0.5525 - acc: 0.8574 - val_loss: 0.6684 - val_acc: 0.7254\n",
      "Epoch 5/5\n",
      "2658/2658 [==============================] - 2s 696us/step - loss: 0.4901 - acc: 0.8841 - val_loss: 0.6650 - val_acc: 0.7465\n",
      "[[699 214]\n",
      " [ 75 152]]\n",
      "0.5126475548060709\n"
     ]
    }
   ],
   "source": [
    "# Purpose Model (User Choice/Control)\n",
    "\n",
    "padded_docs = pad_sequences(user_choice_control.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_choice_control.Purpose, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2658 samples, validate on 1140 samples\n",
      "Epoch 1/5\n",
      "2658/2658 [==============================] - 4s 1ms/step - loss: 0.4557 - acc: 0.9424 - val_loss: 0.2227 - val_acc: 0.9535\n",
      "Epoch 2/5\n",
      "2658/2658 [==============================] - 2s 637us/step - loss: 0.2597 - acc: 0.9631 - val_loss: 0.1422 - val_acc: 0.9579\n",
      "Epoch 3/5\n",
      "2658/2658 [==============================] - 2s 645us/step - loss: 0.1973 - acc: 0.9579 - val_loss: 0.1573 - val_acc: 0.9553\n",
      "Epoch 4/5\n",
      "2658/2658 [==============================] - 2s 661us/step - loss: 0.1414 - acc: 0.9714 - val_loss: 0.1418 - val_acc: 0.9623\n",
      "Epoch 5/5\n",
      "2658/2658 [==============================] - 2s 651us/step - loss: 0.1109 - acc: 0.9774 - val_loss: 0.1672 - val_acc: 0.9491\n",
      "[[1048   40]\n",
      " [  18   34]]\n",
      "0.5396825396825398\n"
     ]
    }
   ],
   "source": [
    "# User Type Model (User Choice/Control)\n",
    "\n",
    "padded_docs = pad_sequences(user_choice_control.enumerated_text, maxlen=max_length, padding='post', value=(len(embedding_matrix)-1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, user_choice_control.User_Type, test_size=0.3, random_state = 0)\n",
    "\n",
    "vocab_length = len(embedding_matrix)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 5.}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length,\n",
    "                   output_dim=100,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=padded_docs.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=100,\\\n",
    "                  validation_data=(x_test, y_test),class_weight=class_weights)\n",
    "\n",
    "print(confusion_matrix(y_test,model.predict_classes(x_test)))\n",
    "print(f1_score(y_test, model.predict_classes(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
